Title,Receive_date,Abstract,Link
Adversarial normalization for multi domain image segmentation,2019-12-02,"Abstract:  Image normalization is a critical step in medical imaging. This step is oftendone on a per-dataset basis, preventing current segmentation algorithms fromthe full potential of exploiting jointly normalized information across multipledatasets. To solve this problem, we propose an adversarial normalizationapproach for image segmentation which learns common normalizing functionsacross multiple datasets while retaining image realism. The adversarialtraining provides an optimal normalizer that improves both the segmentationaccuracy and the discrimination of unrealistic normalizing functions. Ourcontribution therefore leverages common imaging information from multipledomains. The optimality of our common normalizer is evaluated by combiningbrain images from both infants and adults. Results on the challenging iSEG andMRBrainS datasets reveal the potential of our adversarial normalizationapproach for segmentation, with Dice improvements of up to 59.6% over thebaseline.",https://arxiv.org/abs/1912.00993
X-Ray: Quantifying and Explaining Model-Knowledge Transfer in (Un-)Supervised NLP,2019-12-02,"Abstract:  While state-of-the-art NLP explainability (XAI) methods focus on supervised,per-instance end or diagnostic probing task evaluation[4, 2, 10], this isinsufficient to interpret and quantify model knowledge transfer during (un-)supervised training. By instead expressing each neuron as an interpretabletoken-activation distribution collected over many instances, one can quantifyand guide visual exploration of neuron-knowledge change between model trainingstages to analyze transfer beyond probing tasks and the per-instance level.This allows one to analyze: (RQ1) how neurons abstract knowledge duringunsupervised pretraining; (RQ2) how pretrained neurons zero-shot transferknowledge to new domain data; and (RQ3) how supervised tasks reorder pretrainedneuron knowledge abstractions. Since the meaningfulness of XAI methods is hardto quantify [11, 4], we analyze three example learning setups (RQ1-3) toempirically verify that our method (TX-Ray): identifies transfer (ir-)relevantneurons for pruning (RQ3), and that its transfer metrics coincide withtraditional measures like perplexity (RQ1). We also find, that TX-Ray guidedpruning of supervision (ir-)relevant neuron-knowledge (RQ3) can identify`lottery ticket'-like [9, 40] neurons that drive model performance androbustness. Upon inspecting pruned neurons, we find that task-relevantneuron-knowledge (`tickets'), appear (over-)fit, while task-irrelevant neuronslower overfitting, i.e. TX-Ray identifies neurons that generalize, transfer orspecialize model-knowledge [25]. Finally, through RQ1-3, we find that TX-Rayhelps to explore and quantify dynamics of (continual) knowledge transfer andthat it can shed light on neuron-knowledge specialization and generalization,to complement (costly) supervised probing task procurement and established`summary' statistics like perplexity, ROC or F scores.",https://arxiv.org/abs/1912.00982
KernelNet: A Data-Dependent Kernel Parameterization for Deep Generative Modeling,2019-12-02,"Abstract:  Learning with kernels is an often resorted tool in modern machine learning.Standard approaches for this type of learning use a predefined kernel thatrequires careful selection of hyperparameters. To mitigate this burden, wepropose in this paper a framework to construct and learn a data-dependentkernel based on random features and implicit spectral distributions (Fouriertransform of the kernel) parameterized by deep neural networks. We call theconstructed network {\em KernelNet}, and apply it for deep generative modelingin various scenarios, including variants of the MMD-GAN and an implicitVariational Autoencoder (VAE), the two popular learning paradigms in deepgenerative models. Extensive experiments show the advantages of the proposedKernelNet, consistently achieving better performance compared to relatedmethods.",https://arxiv.org/abs/1912.00979
Continuous Graph Neural Networks,2019-12-02,"Abstract:  This paper builds the connection between graph neural networks andtraditional dynamical systems. Existing graph neural networks essentiallydefine a discrete dynamic on node representations with multiple graphconvolution layers. We propose continuous graph neural networks (CGNN), whichgeneralise existing graph neural networks into the continuous-time dynamicsetting. The key idea is how to characterise the continuous dynamics of noderepresentations, i.e. the derivatives of node representations w.r.t. time.Inspired by existing diffusion-based methods on graphs (e.g. PageRank andepidemic models on social networks), we define the derivatives as a combinationof the current node representations, the representations of neighbors, and theinitial values of the nodes. We propose and analyse different possible dynamicson graphs---including each dimension of node representations (a.k.a. thefeature channel) change independently or interact with each other---both withtheoretical justification. The proposed continuous graph neural networks arerobust to over-smoothing and hence capture the long-range dependencies betweennodes. Experimental results on the task of node classification prove theeffectiveness of our proposed approach over competitive baselines.",https://arxiv.org/abs/1912.00967
AP-Perf: Incorporating Generic Performance Metrics in Differentiable Learning,2019-12-02,"Abstract:  We propose a method that enables practitioners to conveniently incorporatecustom non-decomposable performance metrics into differentiable learningpipelines, notably those based upon deep learning architectures. Our approachis based on the recently-developed adversarial prediction framework, adistributionally robust approach that optimizes a metric in the worst casegiven the statistical summary of the empirical distribution. We formulate amarginal distribution technique to reduce the complexity of optimizing theadversarial prediction formulation over a vast range of non-decomposablemetrics. We demonstrate how easy it is to write and incorporate complex custommetrics using our provided tool. Finally, we show the effectiveness of ourapproach for image classification tasks using MNIST and Fashion-MNIST datasetsas well as classification task on tabular data using UCI repository andbenchmark datasets.",https://arxiv.org/abs/1912.00965
LOGAN: Latent Optimisation for Generative Adversarial Networks,2019-12-02,"Abstract:  Training generative adversarial networks requires balancing of delicateadversarial dynamics. Even with careful tuning, training may diverge or end upin a bad equilibrium with dropped modes. In this work, we introduce a new formof latent optimisation inspired by the CS-GAN and show that it improvesadversarial dynamics by enhancing interactions between the discriminator andthe generator. We develop supporting theoretical analysis from the perspectivesof differentiable games and stochastic approximation. Our experimentsdemonstrate that latent optimisation can significantly improve GAN training,obtaining state-of-the-art performance for the ImageNet (128 x 128) dataset.Our model achieves an Inception Score (IS) of 148 and an Fr√©chet InceptionDistance (FID) of 3.4, an improvement of 17% and 32% in IS and FIDrespectively, compared with the baseline BigGAN-deep model with the samearchitecture and number of parameters.",https://arxiv.org/abs/1912.00953
FT-ClipAct: Resilience Analysis of Deep Neural Networks and Improving their Fault Tolerance using Clipped Activation,2019-12-02,"Abstract:  Deep Neural Networks (DNNs) are widely being adopted for safety-criticalapplications, e.g., healthcare and autonomous driving. Inherently, they areconsidered to be highly error-tolerant. However, recent studies have shown thathardware faults that impact the parameters of a DNN (e.g., weights) can havedrastic impacts on its classification accuracy. In this paper, we perform acomprehensive error resilience analysis of DNNs subjected to hardware faults(e.g., permanent faults) in the weight memory. The outcome of this analysis isleveraged to propose a novel error mitigation technique which squashes thehigh-intensity faulty activation values to alleviate their impact. We achievethis by replacing the unbounded activation functions with their clippedversions. We also present a method to systematically define the clipping valuesof the activation functions that result in increased resilience of the networksagainst faults. We evaluate our technique on the AlexNet and the VGG-16 DNNstrained for the CIFAR-10 dataset. The experimental results show that ourmitigation technique significantly improves the resilience of the DNNs tofaults. For example, the proposed technique offers on average 68.92%improvement in the classification accuracy of resilience-optimized VGG-16 modelat 1e-5 fault rate, when compared to the base network without any faultmitigation.",https://arxiv.org/abs/1912.00941
Learning to smell for wellness,2019-12-02,"Abstract:  Learning to automatically perceive smell is becoming increasingly importantwith applications in monitoring the quality of food and drinks for healthyliving. In todays age of proliferation of internet of things devices, thedeployment of electronic nose otherwise known as smell sensors is on theincrease for a variety of olfaction applications with the aid of machinelearning models. These models are trained to classify food and drink qualityinto several categories depending on the granularity of interest. However,models trained to smell in one domain rarely perform adequately when used inanother domain. In this work, we consider a problem where only few samples areavailable in the target domain and we are faced with the task of leveragingknowledge from another domain with relatively abundant data to make reliableinference in the target domain. We propose a weakly supervised domainadaptation framework where we demonstrate that by building multiple models in amixture of supervised and unsupervised framework, we can generalise effectivelyfrom one domain to another. We evaluate our approach on several datasets ofbeef cuts and quality collected across different conditions and environments.We empirically show via several experiments that our approach performcompetitively compared to a variety of baselines.",https://arxiv.org/abs/1912.00895
Deep Neural Network Fingerprinting by Conferrable Adversarial Examples,2019-12-02,"Abstract:  In Machine Learning as a Service, a provider trains a deep neural network andprovides many users access to it. However, the hosted (source) model issusceptible to model stealing attacks where an adversary derives a\emph{surrogate model} from API access to the source model. For post hocdetection of such attacks, the provider needs a robust method to determinewhether a suspect model is a surrogate of their model or not. We propose afingerprinting method for deep neural networks that extracts a set of inputsfrom the source model so that only surrogates agree with the source model onthe classification of such inputs. These inputs are a specifically craftedsubclass of targeted transferable adversarial examples which we callconferrable adversarial examples that transfer exclusively from a source modelto its surrogates. We propose new methods to generate these conferrableadversarial examples and use them as our fingerprint. Our fingerprint is thefirst to be successfully tested as robust against distillation attacks, and ourexperiments show that this robustness extends to robustness against weakerremoval attacks such as fine-tuning, ensemble attacks, and adversarialretraining. We even protect against a powerful adversary with white-box accessto the source model, whereas the defender only needs black-box access to thesurrogate model. We conduct our experiments on the CINIC dataset and a subsetof ImageNet32 with 100 classes.",https://arxiv.org/abs/1912.00888
Just Ask:An Interactive Learning Framework for Vision and Language Navigation,2019-12-02,"Abstract:  In the vision and language navigation task, the agent may encounter ambiguoussituations that are hard to interpret by just relying on visual information andnatural language instructions. We propose an interactive learning framework toendow the agent with the ability to ask for users' help in such situations. Aspart of this framework, we investigate multiple learning approaches for theagent with different levels of complexity. The simplest model-confusion-basedmethod lets the agent ask questions based on its confusion, relying on thepredefined confidence threshold of a next action prediction model. To build onthis confusion-based method, the agent is expected to demonstrate moresophisticated reasoning such that it discovers the timing and locations tointeract with a human. We achieve this goal using reinforcement learning (RL)with a proposed reward shaping term, which enables the agent to ask questionsonly when necessary. The success rate can be boosted by at least 15% with onlyone question asked on average during the navigation. Furthermore, we show thatthe RL agent is capable of adjusting dynamically to noisy human responses.Finally, we design a continual learning strategy, which can be viewed as a dataaugmentation method, for the agent to improve further utilizing its interactionhistory with a human. We demonstrate the proposed strategy is substantiallymore realistic and data-efficient compared to previously proposedpre-exploration techniques.",https://arxiv.org/abs/1912.00915
Learning Bayesian networks from demographic and health survey data,2019-12-02,"Abstract:  Child mortality from preventable diseases such as pneumonia and diarrhoea inlow and middle-income countries remains a serious global challenge. We combineknowledge with available Demographic and Health Survey (DHS) data from India,to construct Bayesian Networks (BNs) and investigate the factors associatedwith childhood diarrhoea. We make use of freeware tools to learn the graphicalstructure of the DHS data with score-based, constraint-based, and hybridstructure learning algorithms. We investigate the effect of missing values,sample size, and knowledge-based constraints on each of the structure learningalgorithms and assess their accuracy with multiple scoring functions.Weaknesses in the survey methodology and data available, as well as thevariability in the BNs generated, mean that is not possible to learn adefinitive causal BN from data. However, knowledge-based constraints are foundto be useful in reducing the variation in the graphs produced by the differentalgorithms, and produce graphs which are more reflective of the likelyinfluential relationships in the data. Furthermore, valuable insights aregained into the performance and characteristics of the structure learningalgorithms. Two score-based algorithms in particular, TABU and FGES,demonstrate many desirable qualities; a) with sufficient data, they produce agraph which is similar to the reference graph, b) they are relativelyinsensitive to missing values, and c) behave well with knowledge-basedconstraints. The results provide a basis for further investigation of the DHSdata and for a deeper understanding of the behaviour of the structure learningalgorithms when applied to real-world settings.",https://arxiv.org/abs/1912.00715
Optimality and limitations of audio-visual integration for cognitive systems,2019-12-02,"Abstract:  Multimodal integration is an important process in perceptual decision-making.In humans, this process has often been shown to be statistically optimal, ornear optimal: sensory information is combined in a fashion that minimises theaverage error in perceptual representation of stimuli. However, sometimes thereare costs that come with the optimization, manifesting as illusory percepts. Wereview audio-visual facilitations and illusions that are products ofmultisensory integration, and the computational models that account for thesephenomena. In particular, the same optimal computational model can lead toillusory percepts, and we suggest that more studies should be needed to detectand mitigate these illusions, as artefacts in artificial cognitive systems. Weprovide cautionary considerations when designing artificial cognitive systemswith the view of avoiding such artefacts. Finally, we suggest avenues ofresearch towards solutions to potential pitfalls in system design. We concludethat detailed understanding of multisensory integration and the mechanismsbehind audio-visual illusions can benefit the design of artificial cognitivesystems.",https://arxiv.org/abs/1912.00581
Abstract Reasoning with Distracting Features,2019-12-02,"Abstract:  Abstraction reasoning is a long-standing challenge in artificialintelligence. Recent studies suggest that many of the deep architectures thathave triumphed over other domains failed to work well in abstract reasoning. Inthis paper, we first illustrate that one of the main challenges in such areasoning task is the presence of distracting features, which requires thelearning algorithm to leverage counterevidence and to reject any of the falsehypotheses in order to learn the true patterns. We later show that carefullydesigned learning trajectory over different categories of training data caneffectively boost learning performance by mitigating the impacts of distractingfeatures. Inspired by this fact, we propose feature robust abstract reasoning(FRAR) model, which consists of a reinforcement learning based teacher networkto determine the sequence of training and a student network for predictions.Experimental results demonstrated strong improvements over baseline algorithmsand we are able to beat the state-of-the-art models by 18.7% in the RAVENdataset and 13.3% in the PGM dataset.",https://arxiv.org/abs/1912.00569
Knowledge Infused Learning (K-IL): Towards Deep Incorporation of Knowledge in Deep Learning,2019-12-01,"Abstract:  Learning the underlying patterns in the data goes beyond instance-basedgeneralization to some external knowledge represented in structured graphs ornetworks. Deep Learning (DL) has shown significant advances inprobabilistically learning latent patterns in the data using a multi-layerednetwork of computational nodes (i.e. neurons/hidden units). However, with thetremendous amount of training data, uncertainty in generalization ondomain-specific tasks, and delta improvement with an increase in complexity ofmodels seem to raise a concern on the features learned by the model. Asincorporation of domain specific knowledge will aid in supervising the learningof features for the model, infusion of knowledge from knowledge graphs withinhidden layers will further enhance the learning process. Although much workremains, we believe that KGs will play an increasing role in developing hybridneuro-symbolic intelligent systems (that is bottom up deep learning with topdown symbolic computing) as well as in building explainable AI systems forwhich KGs will provide a scaffolding for punctuating neural computing. In thisposition paper, we describe our motivation for such hybrid approach and aframework that combines knowledge graph and neural networks.",https://arxiv.org/abs/1912.00512
