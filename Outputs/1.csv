Title,Receive_date,Abstract,Link
Learning a faceted customer segmentation for discovering new business opportunities at Intel,2019-11-27,"For sales and marketing organizations within large enterprises, identifyingand understanding new markets, customers and partners is a key challenge.Intel's Sales and Marketing Group (SMG) faces similar challenges while growingin new markets and domains and evolving its existing business. In today'scomplex technological and commercial landscape, there is need for intelligentautomation supporting a fine-grained understanding of businesses in order tohelp SMG sift through millions of companies across many geographies andlanguages and identify relevant directions. We present a system developed inour company that mines millions of public business web pages, and extracts afaceted customer representation. We focus on two key customer aspects that areessential for finding relevant opportunities: industry segments (ranging frombroad verticals such as healthcare, to more specific fields such as 'videoanalytics') and functional roles (e.g., 'manufacturer' or 'retail'). To addressthe challenge of labeled data collection, we enrich our data with externalinformation gleaned from Wikipedia, and develop a semi-supervised multi-label,multi-lingual deep learning model that parses customer website texts andclassifies them into their respective facets. Our system scans and indexescompanies as part of a large-scale knowledge graph that currently holds tens ofmillions of connected entities with thousands being fetched, enriched andconnected to the graph by the hour in real time, and also supports knowledgeand insight discovery. In experiments conducted in our company, we are able tosignificantly boost the performance of sales personnel in the task ofdiscovering new customers and commercial partnership opportunities.",https://arxiv.org/abs/1912.00778
Latent Semantic Search and Information Extraction Architecture,2019-11-30,"The motivation, concept, design and implementation of latent semantic searchfor search engines have limited semantic search, entity extraction and propertyattribution features, have insufficient accuracy and response time of latentsearch, may impose privacy concerns and the search results are unavailable inoffline mode for robotic search operations. The alternative suggestion involvesautonomous search engine with adaptive storage consumption, configurable searchscope and latent search response time with built-in options for entityextraction and property attribution available as open source platform formobile, desktop and server solutions. The suggested architecture attempts toimplement artificial general intelligence (AGI) principles as long asautonomous behaviour constrained by limited resources is concerned, and it isapplied for specific task of enabling Web search for artificial agentsimplementing the AGI.",https://arxiv.org/abs/1912.00180
Method and Dataset Mining in Scientific Papers,2019-11-29,"Literature analysis facilitates researchers better understanding thedevelopment of science and technology. The conventional literature analysisfocuses on the topics, authors, abstracts, keywords, references, etc., andrarely pays attention to the content of papers. In the field of machinelearning, the involved methods (M) and datasets (D) are key information inpapers. The extraction and mining of M and D are useful for discipline analysisand algorithm recommendation. In this paper, we propose a novel entityrecognition model, called MDER, and constructe datasets from the papers of thePAKDD conferences (2009-2019). Some preliminary experiments are conducted toassess the extraction performance and the mining results are visualized.",https://arxiv.org/abs/1911.13096
Meteoroid Stream Formation Due to the Extraction of Space Resources from Asteroids,2019-11-28,"[Abridged] Asteroid mining is not necessarily a distant prospect. Hayabusa2and OSIRIS-REx have recently rendezvoused with near-Earth asteroids and willreturn samples to Earth. While there is significant science motivation forthese missions, there are also resource interests. Space agencies andcommercial entities are particularly interested in ices and water-bearingminerals that could be used to produce rocket fuel in space. Theinternationally coordinated roadmaps of major space agencies depend onutilizing the natural resources of such celestial bodies. Several companieshave already created plans for intercepting and extracting water and mineralsfrom near-Earth objects, as even a small asteroid could have high economicworth. However, the low surface gravity of asteroids could make the release ofmining waste and the subsequent formation of debris streams a consequence ofasteroid mining. Strategies to contain material during extraction could stilleventually require the purposeful jettison of waste to avoid managing unwantedmass. Using simulations, we explore the formation of mining debris streams byintegrating particles released from four select asteroids. Radiation effectsare included, and a range of debris sizes are explored. The simulation resultsare used to investigate the timescales for debris stream formation, the sizesof the streams, and the meteoroid fluxes compared with sporadic meteoroids. Wefind that for prodigious mining activities resulting in the loss of a fewpercent of the asteroid's mass or more, it is possible to produce streams thatexceed the sporadic flux during stream crossing for some meteoroid sizes. Theresult of these simulations are intended to highlight potential unintendedconsequences that could result from NewSpace activity, which could help toinform efforts to develop international space resource guidelines.",https://arxiv.org/abs/1911.12840
Self-Attention Enhanced Selective Gate with Entity-Aware Embedding for Distantly Supervised Relation Extraction,2019-11-27,"Distantly supervised relation extraction intrinsically suffers from noisylabels due to the strong assumption of distant supervision. Most prior worksadopt a selective attention mechanism over sentences in a bag to denoise fromwrongly labeled data, which however could be incompetent when there is only onesentence in a bag. In this paper, we propose a brand-new light-weight neuralframework to address the distantly supervised relation extraction problem andalleviate the defects in previous selective attention framework. Specifically,in the proposed framework, 1) we use an entity-aware word embedding method tointegrate both relative position information and head/tail entity embeddings,aiming to highlight the essence of entities for this task; 2) we develop aself-attention mechanism to capture the rich contextual dependencies as acomplement for local dependencies captured by piecewise CNN; and 3) instead ofusing selective attention, we design a pooling-equipped gate, which is based onrich contextual representations, as an aggregator to generate bag-levelrepresentation for final relation classification. Compared to selectiveattention, one major advantage of the proposed gating mechanism is that, itperforms stably and promisingly even if only one sentence appears in a bag andthus keeps the consistency across all training examples. The experiments on NYTdataset demonstrate that our approach achieves a new state-of-the-artperformance in terms of both AUC and top-n precision metrics.",https://arxiv.org/abs/1911.11899
Integrating Relation Constraints with Neural Relation Extractors,2019-11-26,"Recent years have seen rapid progress in identifying predefined relationshipbetween entity pairs using neural networks NNs. However, such models often makepredictions for each entity pair individually, thus often fail to solve theinconsistency among different predictions, which can be characterized bydiscrete relation constraints. These constraints are often defined overcombinations of entity-relation-entity triples, since there often lack ofexplicitly well-defined type and cardinality requirements for the relations. Inthis paper, we propose a unified framework to integrate relation constraintswith NNs by introducing a new loss term, ConstraintLoss. Particularly, wedevelop two efficient methods to capture how well the local predictions frommultiple instance pairs satisfy the relation constraints. Experiments on bothEnglish and Chinese datasets show that our approach can help NNs learn fromdiscrete relation constraints to reduce inconsistency among local predictions,and outperform popular neural relation extraction NRE models even enhanced withextra post-processing. Our source code and datasets will be released atthis https URL.",https://arxiv.org/abs/1911.11493
CopyMTL: Copy Mechanism for Joint Extraction of Entities and Relations with Multi-Task Learning,2019-11-24,"Joint extraction of entities and relations has received significant attentiondue to its potential of providing higher performance for both tasks. Amongexisting methods, CopyRE is effective and novel, which uses asequence-to-sequence framework and copy mechanism to directly generate therelation triplets. However, it suffers from two fatal problems. The model isextremely weak at differing the head and tail entity, resulting in inaccurateentity extraction. It also cannot predict multi-token entities (e.g.\textit{Steven Jobs}). To address these problems, we give a detailed analysisof the reasons behind the inaccurate entity extraction problem, and thenpropose a simple but extremely effective model structure to solve this problem.In addition, we propose a multi-task learning framework equipped with copymechanism, called CopyMTL, to allow the model to predict multi-token entities.Experiments reveal the problems of CopyRE and show that our model achievessignificant improvement over the current state-of-the-art method by 9% in NYTand 16% in WebNLG (F1 score). Our code is available atthis https URL",https://arxiv.org/abs/1911.10438
Effective Modeling of Encoder-Decoder Architecture for Joint Entity and Relation Extraction,2019-11-22,"elation tuple consists of two entities and the relation between them, andoften such tuples are found in unstructured text. There may be multiplerelation tuples present in a text and they may share one or both entities amongthem. Extracting such relation tuples from a sentence is a difficult task andsharing of entities or overlapping entities among the tuples makes it morechallenging. Most prior work adopted a pipeline approach where entities wereidentified first followed by finding the relations among them, thus missing theinteraction among the relation tuples in a sentence. In this paper, we proposetwo approaches to use encoder-decoder architecture for jointly extractingentities and relations. In the first approach, we propose a representationscheme for relation tuples which enables the decoder to generate one word at atime like machine translation models and still finds all the tuples present ina sentence with full entity names of different length and with overlappingentities. Next, we propose a pointer network-based decoding approach where anentire tuple is generated at every time step. Experiments on the publiclyavailable New York Times corpus show that our proposed approaches outperformprevious work and achieve significantly higher F1 scores.",https://arxiv.org/abs/1911.09886
Entity Extraction with Knowledge from Web Scale Corpora,2019-11-21,"Entity extraction is an important task in text mining and natural languageprocessing. A popular method for entity extraction is by comparing substringsfrom free text against a dictionary of entities. In this paper, we presentseveral techniques as a post-processing step for improving the effectiveness ofthe existing entity extraction technique. These techniques utilise modelstrained with the web-scale corpora which makes our techniques robust andversatile. Experiments show that our techniques bring a notable improvement onefficiency and effectiveness.",https://arxiv.org/abs/1911.09373
Drug Repurposing for Cancer: An NLP Approach to Identify Low-Cost Therapies,2019-11-18,"More than 200 generic drugs approved by the U.S. Food and Drug Administrationfor non-cancer indications have shown promise for treating cancer. Due to theirlong history of safe patient use, low cost, and widespread availability,repurposing of generic drugs represents a major opportunity to rapidly improveoutcomes for cancer patients and reduce healthcare costs worldwide. Evidence onthe efficacy of non-cancer generic drugs being tested for cancer exists inscientific publications, but trying to manually identify and extract suchevidence is intractable. In this paper, we introduce a system to automate thisevidence extraction from PubMed abstracts. Our primary contribution is todefine the natural language processing pipeline required to obtain suchevidence, comprising the following modules: querying, filtering, cancer typeentity extraction, therapeutic association classification, and study typeclassification. Using the subject matter expertise on our team, we create ourown datasets for these specialized domain-specific tasks. We obtain promisingperformance in each of the modules by utilizing modern language modelingtechniques and plan to treat them as baseline approaches for future improvementof individual components.",https://arxiv.org/abs/1911.07819
DNNRE: A Dynamic Neural Network for Distant Supervised Relation Extraction,2019-11-15,"Distant Supervised Relation Extraction (DSRE) is usually formulated as aproblem of classifying a bag of sentences that contain two query entities, intothe predefined relation classes. Most existing methods consider those relationclasses as distinct semantic categories while ignoring their potentialconnections to each other and query entities. In this paper, we propose toleverage those connections to improve the relation extraction accuracy. Our keyideas are twofold: (1) For sentences belonging to the same relation class, theexpression style, i.e. words choice, can vary according to the query entities.To account for this style shift, the model should adjust its parameters inaccordance with entity types. (2) Some relation classes are semanticallysimilar, and the mutual relationship of classes can be adopted to enhance therelation predictor. This is especially beneficial for those classes with fewsamples, i.e., long-tail classes. To unify these two ideas, we developed anovel Dynamic Neural Network for Relation Extraction (DNNRE). The networkadopts a novel dynamic parameter generator which dynamically generates thenetwork parameters according to the query entity types, relation classes, andthe class similarity matrix. By using this mechanism, the network cansimultaneously handle the style shift problem and leverage mutual classrelationships to enhance the prediction accuracy for long-tail classes. Throughour experimental study, we demonstrate the effectiveness of the proposed methodand show that it can achieve superior performance over the state-of-the-artmethods.",https://arxiv.org/abs/1911.06489
KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation,2019-11-13,"Pre-trained language representation models (PLMs) learn effective languagerepresentations from large-scale unlabeled corpora. Knowledge embedding (KE)algorithms encode the entities and relations in knowledge graphs intoinformative embeddings to do knowledge graph completion and provide externalknowledge for various NLP applications. In this paper, we propose a unifiedmodel for Knowledge Embedding and Pre-trained LanguagE Representation (KEPLER),which not only better integrates factual knowledge into PLMs but alsoeffectively learns knowledge graph embeddings. Our KEPLER utilizes a PLM toencode textual descriptions of entities as their entity embeddings, and thenjointly learn the knowledge embeddings and language representations.Experimental results on various NLP tasks such as the relation extraction andthe entity typing show that our KEPLER can achieve comparable results to thestate-of-the-art knowledge-enhanced PLMs without any additional inferenceoverhead. Furthermore, we construct Wikidata5m, a new large-scale knowledgegraph dataset with aligned text descriptions, to evaluate KE embedding methodsin both the traditional transductive setting and the challenging inductivesetting, which needs the models to predict entity embeddings for unseenentities. Experiments demonstrate our KEPLER can achieve good results in bothsettings.",https://arxiv.org/abs/1911.06136
Image-Based Feature Representation for Insider Threat Classification,2019-11-13,"Insiders are the trusted entities in the organization, but poses threat tothe with access to sensitive information network and resources. The insiderthreat detection is a well studied problem in security analytics. Identifyingthe features from data sources and using them with the right data analyticsalgorithms makes various kinds of threat analysis possible. The insider threatanalysis is mainly done using the frequency based attributes extracted from theraw data available from data sources. In this paper, we propose an image-basedfeature representation of the daily resource usage pattern of users in theorganization. The features extracted from the audit files of the organizationare represented as gray scale images. Hence, these images are used to representthe resource access patterns and thereby the behavior of users. Classificationmodels are applied to the representative images to detect anomalous behavior ofinsiders. The images are classified to malicious and non-malicious. Theeffectiveness of the proposed representation is evaluated using the CMU CERTdata V4.2, and state-of-art image classification models like Mobilenet, VGG andResNet. The experimental results showed improved accuracy. The comparison withexisting works show a performance improvement in terms of high recall andprecision values.",https://arxiv.org/abs/1911.05879
Leveraging Dependency Forest for Neural Medical Relation Extraction,2019-11-11,"Medical relation extraction discovers relations between entity mentions intext, such as research articles. For this task, dependency syntax has beenrecognized as a crucial source of features. Yet in the medical domain, 1-bestparse trees suffer from relatively low accuracies, diminishing theirusefulness. We investigate a method to alleviate this problem by utilizingdependency forests. Forests contain many possible decisions and therefore havehigher recall but more noise compared with 1-best outputs. A graph neuralnetwork is used to represent the forests, automatically distinguishing theuseful syntactic information from parsing noise. Results on two biomedicalbenchmarks show that our method outperforms the standard tree-based methods,giving the state-of-the-art results in the literature.",https://arxiv.org/abs/1911.04123
Decompressing Knowledge Graph Representations for Link Prediction,2019-11-11,"This paper studies the problem of predicting missing relationships betweenentities in knowledge graphs through learning their representations. Currently,the majority of existing link prediction models employ simple but intuitivescoring functions and relatively small embedding size so that they could beapplied to large-scale knowledge graphs. However, these properties alsorestrict the ability to learn more expressive and robust features. Therefore,diverging from most of the prior works which focus on designing new objectivefunctions, we propose, DeCom, a simple but effective mechanism to boost theperformance of existing link predictors such as DistMult, ComplEx, etc, throughextracting more expressive features while preventing overfitting by adding justa few extra parameters. Specifically, embeddings of entities and relationshipsare first decompressed to a more expressive and robust space by decompressingfunctions, then knowledge graph embedding models are trained in this newfeature space. Experimental results on several benchmark knowledge graphs andadvanced link prediction systems demonstrate the generalization andeffectiveness of our method. Especially, RESCAL + DeCom achievesstate-of-the-art performance on the FB15k-237 benchmark across all evaluationmetrics. In addition, we also show that compared with DeCom, explicitlyincreasing the embedding size significantly increase the number of parametersbut could not achieve promising performance improvement.",https://arxiv.org/abs/1911.04053
Learning to Few-Shot Learn Across Diverse Natural Language Classification Tasks,2019-11-10,"Self-supervised pre-training of transformer models has shown enormous successin improving performance on a number of downstream tasks. However, fine-tuningon a new task still requires large amounts of task-specific labelled data toachieve good performance. We consider this problem of learning to generalize tonew tasks with few examples as a meta-learning problem. While meta-learning hasshown tremendous progress in recent years, its application is still limited tosimulated problems or problems with limited diversity across tasks. We developa novel method, LEOPARD, which enables optimization-based meta-learning acrosstasks with different number of classes, and evaluate existing methods ongeneralization to diverse NLP classification tasks. LEOPARD is trained with thestate-of-the-art transformer architecture and shows strong generalization totasks not seen at all during training, with as few as 8 examples per label. On16 NLP datasets, across a diverse task-set such as entity typing, relationextraction, natural language inference, sentiment analysis, and several othertext categorization tasks, we show that LEOPARD learns better initialparameters for few-shot learning than self-supervised pre-training ormulti-task training, outperforming many strong baselines, for example,increasing F1 from 49% to 72%.",https://arxiv.org/abs/1911.03863
Hierarchical Graph Network for Multi-hop Question Answering,2019-11-09,"In this paper, we present Hierarchical Graph Network (HGN) for multi-hopquestion answering. To aggregate clues from scattered texts across multipleparagraphs, a hierarchical graph is created by constructing nodes fromdifferent levels of granularity (i.e., questions, paragraphs, sentences, andentities), the representations of which are initialized with BERT-based contextencoders. By weaving heterogeneous nodes in an integral unified graph, thischaracteristic hierarchical differentiation of node granularity enables HGN tosupport different question answering sub-tasks simultaneously (e.g., paragraphselection, supporting facts extraction, and answer prediction). Given aconstructed hierarchical graph for each question, the initial noderepresentations are updated through graph propagation; and for each sub-task,multi-hop reasoning is performed by traversing through graph edges. Extensiveexperiments on the HotpotQA benchmark demonstrate that the proposed HGNapproach significantly outperforms prior state-of-the-art methods by a largemargin in both Distractor and Fullwiki settings.",https://arxiv.org/abs/1911.03631
Relation Adversarial Network for Low Resource KnowledgeGraph Completion,2019-11-08,"Knowledge Graph Completion (KGC) has been proposed to improve KnowledgeGraphs by filling in missing connections via link prediction or relationextraction. One of the main difficulties for KGC is the low resource problem.Previous approaches assume sufficient training triples to learn versatilevectors for entities and relations, or a satisfactory number of labeledsentences to train a competent relation extraction model. However, low resourcerelations are very common in KGs, and those newly added relations often do nothave many known samples for training. In this work, we aim at predicting newfacts under a challenging setting where only limited training instances areavailable. We propose a general framework called Weighted Relation AdversarialNetwork, which utilizes an adversarial procedure to help adaptknowledge/features learned from high resource relations to different butrelated low resource relations. Specifically, the framework takes advantage ofa relation discriminator to distinguish between samples from differentrelations, and help learn relation-invariant features more transferable fromsource relations to target relations. Experimental results show that theproposed approach outperforms previous methods regarding low resource settingsfor both link prediction and relation extraction.",https://arxiv.org/abs/1911.03091
Neural Graph Embedding Methods for Natural Language Processing,2019-11-08,"Knowledge graphs are structured representations of facts in a graph, wherenodes represent entities and edges represent relationships between them. Recentresearch has resulted in the development of several large KGs. However, all ofthem tend to be sparse with very few facts per entity. In the first part of thethesis, we propose three solutions to alleviate this problem: (1) KGCanonicalization, i.e., identifying and merging duplicate entities in a KG, (2)Relation Extraction which involves automating the process of extractingsemantic relationships between entities from unstructured text, and (3) Linkprediction which includes inferring missing facts based on the known facts in aKG. Traditional Neural Networks like CNNs and RNNs are constrained to handleEuclidean data. However, graphs in Natural Language Processing (NLP) areprominent. Recently, Graph Convolutional Networks (GCNs) have been proposed toaddress this shortcoming and have been successfully applied for severalproblems. In the second part of the thesis, we utilize GCNs for DocumentTimestamping problem and for learning word embeddings using dependency contextof a word instead of sequential context. In this third part of the thesis, weaddress two limitations of existing GCN models, i.e., (1) The standardneighborhood aggregation scheme puts no constraints on the number of nodes thatcan influence the representation of a target node. This leads to a noisyrepresentation of hub-nodes which coves almost the entire graph in a few hops.(2) Most of the existing GCN models are limited to handle undirected graphs.However, a more general and pervasive class of graphs are relational graphswhere each edge has a label and direction associated with it. Existingapproaches to handle such graphs suffer from over-parameterization and arerestricted to learning representation of nodes only.",https://arxiv.org/abs/1911.03042
Click Maximization in Online Social Networks Using Optimal Choice of Targeted Interests,2019-11-05,"Click-through rate (CTR) prediction of advertisements on online socialnetwork platforms to optimize advertising is of much interest. Prior worksbuild machine learning models that take a user-centric approach in terms oftraining -- using predominantly user data to classify whether a user will clickon an advertisement or not. While this approach has proven effective, it isinaccessible to most entities and relies heavily on user data. To accommodatefor this, we first consider a large set of advertisement data on Facebook anduse natural language processing (NLP) to extract key concepts that we callconceptual nodes. To predict the value of CTR for a combination of conceptualnodes, we use the advertisement data to train four machine learning (ML)models. We then cast the problem of finding the optimal combination ofconceptual nodes as an optimization problem. Given a certain budget $k$, we areinterested in finding the optimal combination of conceptual nodes that maximizethe CTR. A discussion of the hardness and possible NP-hardness of theoptimization problem is provided. Then, we propose a greedy algorithm and agenetic algorithm to find near-optimal combinations of conceptual nodes inpolynomial time, with the genetic algorithm nearly matching the optimalsolution. We observe that Decision Tree Regressor and Random Forest Regressorexhibit the highest Pearson correlation coefficients w.r.t. click predictionsand real click values. Additionally, we find that the conceptual nodes of""politics"", ""celebrity"", and ""organization"" are notably more influential thanother considered conceptual nodes.",https://arxiv.org/abs/1911.02061
Integrating Dictionary Feature into A Deep Learning Model for Disease Named Entity Recognition,2019-11-05,"In recent years, Deep Learning (DL) models are becoming important due totheir demonstrated success at overcoming complex learning problems. DL modelshave been applied effectively for different Natural Language Processing (NLP)tasks such as part-of-Speech (PoS) tagging and Machine Translation (MT).Disease Named Entity Recognition (Disease-NER) is a crucial task which aims atextracting disease Named Entities (NEs) from text. In this paper, a DL modelfor Disease-NER using dictionary information is proposed and evaluated onNational Center for Biotechnology Information (NCBI) disease corpus and BC5CDRdataset. Word embeddings trained over general domain texts as well asbiomedical texts have been used to represent input to the proposed model. Thisstudy also compares two different Segment Representation (SR) schemes, namelyIOB2 and IOBES for Disease-NER. The results illustrate that using dictionaryinformation, pre-trained word embeddings, character embeddings and CRF withglobal score improves the performance of Disease-NER system.",https://arxiv.org/abs/1911.01600
Reasoning Over Paths via Knowledge Base Completion,2019-11-01,Reasoning over paths in large scale knowledge graphs is an important problemfor many applications. In this paper we discuss a simple approach toautomatically build and rank paths between a source and target entity pair withlearned embeddings using a knowledge base completion model (KBC). We assembleda knowledge graph by mining the available biomedical scientific literature andextracted a set of high frequency paths to use for validation. We demonstratethat our method is able to effectively rank a list of known paths between apair of entities and also come up with plausible paths that are not present inthe knowledge graph. For a given entity pair we are able to reconstruct thehighest ranking path 60% of the time within the the top 10 ranked paths andachieve 49% mean average precision. Our approach is compositional since any KBCmodel that can produce vector representations of entities can be used.,https://arxiv.org/abs/1911.00492
Neural Cross-Lingual Relation Extraction Based on Bilingual Word Embedding Mapping,2019-10-31,"Relation extraction (RE) seeks to detect and classify semantic relationshipsbetween entities, which provides useful information for many NLP applications.Since the state-of-the-art RE models require large amounts of manuallyannotated data and language-specific resources to achieve high accuracy, it isvery challenging to transfer an RE model of a resource-rich language to aresource-poor language. In this paper, we propose a new approach forcross-lingual RE model transfer based on bilingual word embedding mapping. Itprojects word embeddings from a target language to a source language, so that awell-trained source-language neural network RE model can be directly applied tothe target language. Experiment results show that the proposed approachachieves very good performance for a number of target languages on bothin-house and open datasets, using a small bilingual dictionary with only 1Kword pairs.",https://arxiv.org/abs/1911.00069
"A Semi-Automated Approach for Information Extraction, Classification and Analysis of Unstructured Data",2019-10-20,"In this paper, we show how Quantitative Narrative Analysis and simple NaturalLanguage Processing techniques apply to the extraction and categorization ofdata in a sample case study of the Diary of the former President of the ItalianRepublic (PoR), Giorgio Napolitano. The Diary contains a record of all hisinstitutional meetings. This information, if properly handled, allows for ananalysis of how the PoR used his so-called soft-powers to influence the Italianpolitical system during his first mandate. In this paper, we propose a way touse simple, yet very effective, Natural Language Processing techniques - suchas Regular Expressions and Named Entity Recognition - to extract informationfrom the Diary. Then, we propose an innovative way to organize the extracteddata relying on the methodological framework of Quantitative NarrativeAnalysis. Finally, we show how to analyze the structured data under differentlevels of detail using PC-ACE (Program for Computer-Assisted Coding of Events),a software developed specifically for this task and for data visualization.",https://arxiv.org/abs/1910.12734
Multi-Module System for Open Domain Chinese Question Answering over Knowledge Base,2019-10-28,"For the task of open domain Knowledge Based Question Answering in CCKS2019,we propose a method combining information retrieval and semantic parsing. Thismulti-module system extracts the topic entity and the most related relationpredicate from a question and transforms it into a Sparql query statement. Ourmethod obtained the F1 score of 70.45% on the test data.",https://arxiv.org/abs/1910.12477
