Title,Field,Receive_date,Abstract,Link
['Title:Establishing an Evaluation Metric to Quantify Climate Change Image Realism'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  With success on controlled tasks, generative models are being increasinglyapplied to humanitarian applications [1,2]. In this paper, we focus on theevaluation of a conditional generative model that illustrates the consequencesof climate change-induced flooding to encourage public interest and awarenesson the issue. Because metrics for comparing the realism of different modes in aconditional generative model do not exist, we propose several automated andhuman-based methods for evaluation. To do this, we adapt several existingmetrics, and assess the automated metrics against gold standard humanevaluation. We find that using Fr√©chet Inception Distance (FID) withembeddings from an intermediary Inception-V3 layer that precedes the auxiliaryclassifier produces results most correlated with human realism. Whileinsufficient alone to establish a human-correlated automatic evaluation metric,we believe this work begins to bridge the gap between human and automatedgenerative evaluation procedures.",https://arxiv.org/abs/1910.10143
"['Title:Class Mean Vectors, Self Monitoring and Self Learning for Neural Classifiers']",Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  In this paper we explore the role of sample mean in building a neural networkfor classification. This role is surprisingly extensive and includes: directcomputation of weights without training, performance monitoring for sampleswithout known classification, and self-training for unlabeled data.Experimental computation on a CIFAR-10 data set provides promising empiricalevidence on the efficacy of a simple and widely applicable approach to somedifficult problems.",https://arxiv.org/abs/1910.10122
['Title:Mask Combination of Multi-layer Graphs for Global Structure Inference'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  Structure inference is an important task for network data processing andanalysis in data science. In recent years, quite a few approaches have beendeveloped to learn the graph structure underlying a set of observationscaptured in a data space. Although real world data is often acquired insettings where relationships are influenced by a priori known rules, thisdomain knowledge is still not well exploited in structure inference problems.In this paper, we identify the structure of signals defined in a data spacewhose inner relationships are encoded by multi-layer graphs. We aim at properlyexploiting the information originating from each layer to infer the globalstructure underlying the signals. We thus present a novel method for combiningthe multiple graphs into a global graph using mask matrices, which areestimated through an optimization problem that accommodates the multi-layergraph information and a signal representation model. The proposed maskcombination method also estimates the contribution of each graph layer in thestructure of signals. The experiments conducted both on synthetic and realworld data suggest that integrating the multi-layer graph representation of thedata in the structure inference framework enhances the learning procedureconsiderably by adapting to the quality and the quantity of the input data",https://arxiv.org/abs/1910.10114
['Title:Improving singing voice separation with the Wave-U-Net using Minimum Hyperspherical Energy'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  In recent years, deep learning has surpassed traditional approaches to theproblem of singing voice separation. The Wave-U-Net is a recent deep networkarchitecture that operates directly on the time domain. The standard Wave-U-Netis trained with data augmentation and early stopping to prevent overfitting.Minimum hyperspherical energy (MHE) regularization has recently proven toincrease generalization in image classification problems by encouraging adiversified filter configuration. In this work, we apply MHE regularization tothe 1D filters of the Wave-U-Net. We evaluated this approach for separating thevocal part from mixed music audio recordings on the MUSDB18 dataset. We foundthat adding MHE regularization to the loss function consistently improvessinging voice separation, as measured in the Signal to Distortion Ratio on testrecordings, leading to the current best time-domain system for singing voiceextraction.",https://arxiv.org/abs/1910.10071
['Title:Deep Learning Regression of VLSI Plasma Etch Metrology'],Machine Learning ,(Submitted on 10 Sep 2019),"Abstract:  In computer chip manufacturing, the study of etch patterns on silicon wafers,or metrology, occurs on the nano-scale and is therefore subject to largevariation from small, yet significant, perturbations in the manufacturingenvironment. An enormous amount of information can be gathered from a singleetch process, a sequence of actions taken to produce an etched wafer from ablank piece of silicon. Each final wafer, however, is costly to takemeasurements from, which limits the number of examples available to train apredictive model. Part of the significance of this work is the success we sawfrom the models despite the limited number of examples. In order to accommodatethe high dimensional process signatures, we isolated important sensor variablesand applied domain-specific summarization on the data using multiple featureengineering techniques. We used a neural network architecture consisting of thesummarized inputs, a single hidden layer of 4032 units, and an output layer ofone unit. Two different models were learned, corresponding to the metrologymeasurements in the dataset, Recess and Remaining Mask. The outputs are relatedabstractly and do not form a two dimensional space, thus two separate modelswere learned. Our results approach the error tolerance of the microscopicimaging system. The model can make predictions for a class of etch recipes thatinclude the correct number of etch steps and plasma reactors with theappropriate sensors, which are chambers containing an ionized gas thatdetermine the manufacture environment. Notably, this method is not restrictedto some maximum process length due to the summarization techniques used. Thisallows the method to be adapted to new processes that satisfy theaforementioned requirements. In order to automate semiconductor manufacturing,models like these will be needed throughout the process to evaluate productionquality.",https://arxiv.org/abs/1910.10067
['Title:Adversarial Example Detection by Classification for Deep Speech Recognition'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  Machine Learning systems are vulnerable to adversarial attacks and willhighly likely produce incorrect outputs under these attacks. There arewhite-box and black-box attacks regarding to adversary's access level to thevictim learning algorithm. To defend the learning systems from these attacks,existing methods in the speech domain focus on modifying input signals andtesting the behaviours of speech recognizers. We, however, formulate thedefense as a classification problem and present a strategy for systematicallygenerating adversarial example datasets: one for white-box attacks and one forblack-box attacks, containing both adversarial and normal examples. Thewhite-box attack is a gradient-based method on Baidu DeepSpeech with theMozilla Common Voice database while the black-box attack is a gradient-freemethod on a deep model-based keyword spotting system with the Google SpeechCommand dataset. The generated datasets are used to train a proposedConvolutional Neural Network (CNN), together with cepstral features, to detectadversarial examples. Experimental results show that, it is possible toaccurately distinct between adversarial and normal examples for known attacks,in both single-condition and multi-condition training settings, while theperformance degrades dramatically for unknown attacks. The adversarial datasetsand the source code are made publicly available.",https://arxiv.org/abs/1910.10013
['Title:Toward Automated Website Classification by Deep Learning'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  In recent years, the interest in Big Data sources has been steadily growingwithin the Official Statistic community. The Italian National Institute ofStatistics (Istat) is currently carrying out several Big Data pilot studies.One of these studies, the ICT Big Data pilot, aims at exploiting massiveamounts of textual data automatically scraped from the websites of Italianenterprises in order to predict a set of target variables (e.g. e-commerce)that are routinely observed by the traditional ICT Survey. In this paper, weshow that Deep Learning techniques can successfully address this problem.Essentially, we tackle a text classification task: an algorithm must learn toinfer whether an Italian enterprise performs e-commerce from the textualcontent of its website. To reach this goal, we developed a sophisticatedprocessing pipeline and evaluated its performance through extensiveexperiments. Our pipeline uses Convolutional Neural Networks and relies on WordEmbeddings to encode raw texts into grayscale images (i.e. normalized numericmatrices). Web-scraped texts are huge and have very low signal to noise ratio:to overcome these issues, we adopted a framework known as False PositiveReduction, which has seldom (if ever) been applied before to textclassification tasks. Several original contributions enable our processingpipeline to reach good classification results. Empirical evidence shows thatour proposal outperforms all the alternative Machine Learning solutions alreadytested in Istat for the same task.",https://arxiv.org/abs/1910.09991
['Title:Abnormal Client Behavior Detection in Federated Learning'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  In federated learning systems, clients are autonomous in that their behaviorsare not fully governed by the server. Consequently, a client may intentionallyor unintentionally deviate from the prescribed course of federated modeltraining, resulting in abnormal behaviors, such as turning into a maliciousattacker or a malfunctioning client. Timely detecting those anomalous clientsis therefore critical to minimize their adverse impacts. In this work, wepropose to detect anomalous clients at the server side. In particular, wegenerate low-dimensional surrogates of model weight vectors and use them toperform anomaly detection. We evaluate our solution through experiments onimage classification model training over the FEMNIST dataset. Experimentalresults show that the proposed detection-based approach significantlyoutperforms the conventional defense-based methods.",https://arxiv.org/abs/1910.09933
['Title:Neural Network Training with Approximate Logarithmic Computations'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  The high computational complexity associated with training deep neuralnetworks limits online and real-time training on edge devices. This paperproposed an end-to-end training and inference scheme that eliminatesmultiplications by approximate operations in the log-domain which has thepotential to significantly reduce implementation complexity. We implement theentire training procedure in the log-domain, with fixed-point datarepresentations. This training procedure is inspired by hardware-friendlyapproximations of log-domain addition which are based on look-up tables andbit-shifts. We show that our 16-bit log-based training can achieveclassification accuracy within approximately 1% of the equivalentfloating-point baselines for a number of commonly used datasets.",https://arxiv.org/abs/1910.09876
['Title:A Prototypical Triplet Loss for Cover Detection'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  Automatic cover detection -- the task of finding in a audio dataset allcovers of a query track -- has long been a challenging theoretical problem inMIR community. It also became a practical need for music composers societiesrequiring to detect automatically if an audio excerpt embeds musical contentbelonging to their catalog.In a recent work, we addressed this problem with a convolutional neuralnetwork mapping each track's dominant melody to an embedding vector, andtrained to minimize cover pairs distance in the embeddings space, whilemaximizing it for non-covers. We showed in particular that training this modelwith enough works having five or more covers yields state-of-the-art results.This however does not reflect the realistic use case, where music catalogstypically contain works with zero or at most one or two covers. We thusintroduce here a new test set incorporating these constraints, and propose twocontributions to improve our model's accuracy under these stricter conditions:we replace dominant melody with multi-pitch representation as input data, anddescribe a novel prototypical triplet loss designed to improve coversclustering. We show that these changes improve results significantly for twoconcrete use cases, large dataset lookup and live songs identification.",https://arxiv.org/abs/1910.09862
['Title:An Efficient EKF Based Algorithm For LSTM-Based Online Learning'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  We investigate online nonlinear regression with long short term memory (LSTM)based networks, which we refer to as LSTM-based online learning. For LSTM-basedonline learning, we introduce a highly efficient extended Kalman filter (EKF)based training algorithm with a theoretical convergence guarantee. Throughsimulations, we illustrate significant performance improvements achieved by ouralgorithm with respect to the conventional LSTM training methods. Weparticularly show that our algorithm provides very similar error performancewith the EKF learning algorithm in 25-40 times shorter training time dependingon the parameter size of the network.",https://arxiv.org/abs/1910.09857
['Title:Orthogonal variance decomposition based feature selection'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  Existing feature selection methods fail to properly account for interactionsbetween features when evaluating feature subsets. In this paper, we attempt toremedy this issue by using orthogonal variance decomposition to evaluatefeatures. The orthogonality of the decomposition allows us to directlycalculate the total contribution of a feature to the output variance. Thus weobtain an efficient algorithm for feature evaluation which takes into accountinteractions among features. Numerical experiments demonstrate that our methodaccurately identifies relevant features and improves the accuracy of numericalmodels.",https://arxiv.org/abs/1910.09851
['Title:Towards best practice in explaining neural network decisions with LRP'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  Within the last decade, neural network based predictors have demonstratedimpressive - and at times super-human - capabilities. This performance is oftenpaid for with an intransparent prediction process and thus has sparked numerouscontributions in the novel field of explainable artificial intelligence (XAI).In this paper, we focus on a popular and widely used method of XAI, theLayer-wise Relevance Propagation (LRP). Since its initial proposition LRP hasevolved as a method, and a best practice for applying the method has tacitlyemerged, based on humanly observed evidence. We investigate - and for the firsttime quantify - the effect of this current best practice on feedforward neuralnetworks in a visual object detection setting. The results verify that thecurrent, layer-dependent approach to LRP applied in recent literature betterrepresents the model's reasoning, and at the same time increases the objectlocalization and class discriminativity of LRP.",https://arxiv.org/abs/1910.09840
['Title:A Scalable Predictive Maintenance Model for Detecting Wind Turbine Component Failures Based on SCADA Data'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  In this work, a novel predictive maintenance system is presented and appliedto the main components of wind turbines. The proposed model is based on machinelearning and statistical process control tools applied to SCADA (SupervisoryControl And Data Acquisition) data of critical components. The test campaignwas divided into two stages: a first two years long offline test, and a secondone year long real-time test. The offline test used historical faults from sixwind farms located in Italy and Romania, corresponding to a total of 150 windturbines and an overall installed nominal power of 283 MW. The resultsdemonstrate outstanding capabilities of anomaly prediction up to 2 monthsbefore device unscheduled downtime. Furthermore, the real-time 12-months testconfirms the ability of the proposed system to detect several anomalies,therefore allowing the operators to identify the root causes, and to schedulemaintenance actions before reaching a catastrophic stage.",https://arxiv.org/abs/1910.09808
['Title:Two-Step Sound Source Separation: Training on Learned Latent Targets'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  In this paper, we propose a two-step training procedure for source separationvia a deep neural network. In the first step we learn a transform (and it'sinverse) to a latent space where masking-based separation performance usingoracles is optimal. For the second step, we train a separation module thatoperates on the previously learned space. In order to do so, we also make useof a scale-invariant signal to distortion ratio (SI-SDR) loss function thatworks in the latent space, and we prove that it lower-bounds the SI-SDR in thetime domain. We run various sound separation experiments that show how thisapproach can obtain better performance as compared to systems that learn thetransform and the separation module jointly. The proposed methodology isgeneral enough to be applicable to a large class of neural network end-to-endseparation systems.",https://arxiv.org/abs/1910.09804
['Title:Improving Siamese Networks for One Shot Learning using Kernel Based Activation functions'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  The lack of a large amount of training data has always been the constrainingfactor in solving a lot of problems in machine learning, making One ShotLearning one of the most intriguing ideas in machine learning. It aims to learninformation about object categories from one, or only a few training examples.This process of learning in deep learning is usually accomplished by properobjective function, i.e; loss function and embeddings extraction i.e;architecture. In this paper, we discussed about metrics based deep learningarchitectures for one shot learning such as Siamese neural networks and presenta method to improve on their accuracy using Kafnets (kernel-basednon-parametric activation functions for neural networks) by learning properembeddings with relatively less number of epochs. Using kernel activationfunctions, we are able to achieve strong results which exceed those of ReLUbased deep learning models in terms of embeddings structure, loss convergence,and accuracy.",https://arxiv.org/abs/1910.09798
['Title:Robust Training with Ensemble Consensus'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  Since deep neural networks are over-parametrized, they may memorize noisyexamples. We address such memorizing issue under the existence of annotationnoise. From the fact that deep neural networks cannot generalize neighborhoodsof the features acquired via memorization, we find that noisy examples do notconsistently incur small losses on the network in the presence of perturbation.Based on this, we propose a novel training method called Learning with EnsembleConsensus (LEC) whose goal is to prevent overfitting noisy examples byeliminating them identified via consensus of an ensemble of perturbed networks.One of the proposed LECs, LTEC outperforms the current state-of-the-art methodson MNIST, CIFAR-10, and CIFAR-100 despite its efficient memory.",https://arxiv.org/abs/1910.09792
['Title:Bridging the Gap Between $f$-GANs and Wasserstein GANs'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  Generative adversarial networks (GANs) have enjoyed much success in learninghigh-dimensional distributions. Learning objectives approximately minimize an$f$-divergence ($f$-GANs) or an integral probability metric (Wasserstein GANs)between the model and the data distribution using a discriminator. WassersteinGANs enjoy superior empirical performance, but in $f$-GANs the discriminatorcan be interpreted as a density ratio estimator which is necessary in some GANapplications. In this paper, we bridge the gap between $f$-GANs and WassersteinGANs (WGANs). First, we list two constraints over variational $f$-divergenceestimation objectives that preserves the optimal solution. Next, we minimizeover a Lagrangian relaxation of the constrained objective, and show that itgeneralizes critic objectives of both $f$-GAN and WGAN. Based on thisgeneralization, we propose a novel practical objective, named KL-WassersteinGAN (KL-WGAN). We demonstrate empirical success of KL-WGAN on syntheticdatasets and real-world image generation benchmarks, and achievestate-of-the-art FID scores on CIFAR10 image generation.",https://arxiv.org/abs/1910.09779
['Title:Self-supervised pre-training with acoustic configurations for replay spoofing detection'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  Large datasets are well-known as a key to the recent advances in deeplearning. However, dataset construction, especially for replay spoofingdetection, requires the physical process of playing an utterance andre-recording it, which hinders the construction of large-scale datasets. Tocompensate for the limited availability of replay spoofing datasets, in thisstudy, we propose a method for pre-training acoustic configurations usingexternal data unrelated to replay attacks. Here, acoustic configurations referto variables present in the process of a voice being uttered by a speaker andrecorded through a microphone. Specifically, we select pairs of audio segmentsand train the network to determine whether the acoustic configurations of twosegments are identical. We conducted experiments using the ASVspoof 2019physical access dataset, and the results revealed that our proposed methodreduced the relative error rate by over 37% compared to the baseline.",https://arxiv.org/abs/1910.09778
['Title:Weakly Supervised Disentanglement with Guarantees'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  Learning disentangled representations that correspond to factors of variationin real-world data is critical to interpretable and human-controllable machinelearning. Recently, concerns about the viability of learning disentangledrepresentations in a purely unsupervised manner has spurred a shift toward theincorporation of weak supervision. However, there is currently no formalismthat identifies when and how weak supervision will guarantee disentanglement.To address this issue, we provide a theoretical framework to assist inanalyzing the disentanglement guarantees (or lack thereof) conferred by weaksupervision when coupled with learning algorithms based on distributionmatching. We empirically verify the guarantees and limitations of several weaksupervision methods (restricted labeling, match-pairing, and rank-pairing),demonstrating the predictive power and usefulness of our theoretical framework.",https://arxiv.org/abs/1910.09772
['Title:Face representation by deep learning: a linear encoding in a parameter space?'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  Recently, Convolutional Neural Networks (CNNs) have achieved tremendousperformances on face recognition, and one popular perspective regarding CNNs'success is that CNNs could learn discriminative face representations from faceimages with complex image feature encoding. However, it is still unclear whatis the intrinsic mechanism of face representation in CNNs. In this work, weinvestigate this problem by formulating face images as points in ashape-appearance parameter space, and our results demonstrate that: (i) Theencoding and decoding of the neuron responses (representations) to face imagesin CNNs could be achieved under a linear model in the parameter space, inagreement with the recent discovery in primate IT face neurons, but differentfrom the aforementioned perspective on CNNs' face representation with compleximage feature encoding; (ii) The linear model for face encoding and decoding inthe parameter space could achieve close or even better performances on facerecognition and verification than state-of-the-art CNNs, which might providenew lights on the design strategies for face recognition systems; (iii) Theneuron responses to face images in CNNs could not be adequately modelled by theaxis model, a model recently proposed on face modelling in primate IT cortex.All these results might shed some lights on the often complained blackboxnature behind CNNs' tremendous performances on face recognition.",https://arxiv.org/abs/1910.09768
['Title:Stochastic Feedforward Neural Networks: Universal Approximation'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  In this chapter we take a look at the universal approximation question forstochastic feedforward neural networks. In contrast to deterministic networks,which represent mappings from a set of inputs to a set of outputs, stochasticnetworks represent mappings from a set of inputs to a set of probabilitydistributions over the set of outputs. In particular, even if the sets ofinputs and outputs are finite, the class of stochastic mappings in question isnot finite. Moreover, while for a deterministic function the values of alloutput variables can be computed independently of each other given the valuesof the inputs, in the stochastic setting the values of the output variables mayneed to be correlated, which requires that their values are computed jointly. Aprominent class of stochastic feedforward networks which has played a key rolein the resurgence of deep learning are deep belief networks. Therepresentational power of these networks has been studied mainly in thegenerative setting, as models of probability distributions without an input, orin the discriminative setting for the special case of deterministic mappings.We study the representational power of deep sigmoid belief networks in terms ofcompositions of linear transformations of probability distributions, Markovkernels, that can be expressed by the layers of the network. We investigatedifferent types of shallow and deep architectures, and the minimal number oflayers and units per layer that are sufficient and necessary in order for thenetwork to be able to approximate any given stochastic mapping from the set ofinputs to the set of outputs arbitrarily well.",https://arxiv.org/abs/1910.09763
['Title:Unsupervised Boosting-based Autoencoder Ensembles for Outlier Detection'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  Autoencoders, as a dimensionality reduction technique, have been recentlyapplied to outlier detection. However, neural networks are known to bevulnerable to overfitting, and therefore have limited potential in theunsupervised outlier detection setting. Current approaches to ensemble-basedautoencoders do not generate a sufficient level of diversity to avoid theoverfitting issue. To overcome the aforementioned limitations we develop aBoosting-based Autoencoder Ensemble approach (in short, BAE). BAE is anunsupervised ensemble method that, similarly to the boosting approach, buildsan adaptive cascade of autoencoders to achieve improved and robust results. BAEtrains the autoencoder components sequentially by performing a weightedsampling of the data, aimed at reducing the amount of outliers used duringtraining, and at injecting diversity in the ensemble. We perform extensiveexperiments and show that the proposed methodology outperforms state-of-the-artapproaches under a variety of conditions.",https://arxiv.org/abs/1910.09754
['Title:Vanishing Nodes: Another Phenomenon That Makes Training Deep Neural Networks Difficult'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  It is well known that the problem of vanishing/exploding gradients is achallenge when training deep networks. In this paper, we describe anotherphenomenon, called vanishing nodes, that also increases the difficulty oftraining deep neural networks. As the depth of a neural network increases, thenetwork's hidden nodes have more highly correlated behavior. This results ingreat similarities between these nodes. The redundancy of hidden nodes thusincreases as the network becomes deeper. We call this problem vanishing nodes,and we propose the metric vanishing node indicator (VNI) for quantitativelymeasuring the degree of vanishing nodes. The VNI can be characterized by thenetwork parameters, which is shown analytically to be proportional to the depthof the network and inversely proportional to the network width. The theoreticalresults show that the effective number of nodes vanishes to one when the VNIincreases to one (its maximal value), and that vanishing/exploding gradientsand vanishing nodes are two different challenges that increase the difficultyof training deep neural networks. The numerical results from the experimentssuggest that the degree of vanishing nodes will become more evident duringback-propagation training, and that when the VNI is equal to 1, the networkcannot learn simple tasks (e.g. the XOR problem) even when the gradients areneither vanishing nor exploding. We refer to this kind of gradients as thewalking dead gradients, which cannot help the network converge when having arelatively large enough scale. Finally, the experiments show that thelikelihood of failed training increases as the depth of the network increases.The training will become much more difficult due to the lack of networkrepresentation capability.",https://arxiv.org/abs/1910.09745
['Title:Composite Neural Network: Theory and Application to PM2.5 Prediction'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  This work investigates the framework and performance issues of the compositeneural network, which is composed of a collection of pre-trained andnon-instantiated neural network models connected as a rooted directed acyclicgraph for solving complicated applications. A pre-trained neural network modelis generally well trained, targeted to approximate a specific function. Despitea general belief that a composite neural network may perform better than asingle component, the overall performance characteristics are not clear. Inthis work, we construct the framework of a composite network, and prove that acomposite neural network performs better than any of its pre-trained componentswith a high probability bound. In addition, if an extra pre-trained componentis added to a composite network, with high probability, the overall performancewill not be degraded. In the study, we explore a complicatedapplication---PM2.5 prediction---to illustrate the correctness of the proposedcomposite network theory. In the empirical evaluations of PM2.5 prediction, theconstructed composite neural network models support the proposed theory andperform better than other machine learning models, demonstrate the advantagesof the proposed framework.",https://arxiv.org/abs/1910.09739
['Title:Single Versus Union: Non-parallel Support Vector Machine Frameworks'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  Considering the classification problem, we summarize the nonparallel supportvector machines with the nonparallel hyperplanes to two types of frameworks.The first type constructs the hyperplanes separately. It solves a series ofsmall optimization problems to obtain a series of hyperplanes, but is hard tomeasure the loss of each sample. The other type constructs all the hyperplanessimultaneously, and it solves one big optimization problem with the ascertainedloss of each sample. We give the characteristics of each framework and comparethem carefully. In addition, based on the second framework, we construct amax-min distance-based nonparallel support vector machine for multiclassclassification problem, called NSVM. It constructs hyperplanes with largedistance margin by solving an optimization problem. Experimental results onbenchmark data sets and human face databases show the advantages of our NSVM.",https://arxiv.org/abs/1910.09734
['Title:Explicitly Bayesian Regularizations in Deep Learning'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  Generalization is essential for deep learning. In contrast to previous worksclaiming that Deep Neural Networks (DNNs) have an implicit regularizationimplemented by the stochastic gradient descent, we demonstrate explicitlyBayesian regularizations in a specific category of DNNs, i.e., ConvolutionalNeural Networks (CNNs). First, we introduce a novel probabilisticrepresentation for the hidden layers of CNNs and demonstrate that CNNscorrespond to Bayesian networks with the serial connection. Furthermore, weshow that the hidden layers close to the input formulate prior distributions,thus CNNs have explicitly Bayesian regularizations based on the Bayesianregularization theory. In addition, we clarify two recently observed empiricalphenomena that are inconsistent with traditional theories of generalization.Finally, we validate the proposed theory on a synthetic dataset",https://arxiv.org/abs/1910.09732
['Title:Multiple Sample Clustering'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  The clustering algorithms that view each object data as a single sample drawnfrom a certain distribution, Gaussian distribution, for example, have been ahot topic for decades. Many clustering algorithms: such ask-means and spectralclustering, are proposed based on the single sample assumption. However, inreal life, each input object can usually be the multiple samples drawn from acertain hidden distribution. The traditional clustering algorithms cannothandle such a situation. This calls for the multiple sample clusteringalgorithm. But the traditional multiple sample clustering algorithms can onlyhandle scalar samples or samples from Gaussian distribution. This constrainsthe application field of multiple sample clustering algorithms. In this paper,we purpose a general framework for multiple sample clustering. Variousalgorithms can be generated by this framework. We apply two specific cases ofthis framework: Wasserstein distance version and Bhattacharyyadistance versionon both synthetic data and stock price data. The simulation results show thatthe sufficient statistic can greatly improve the clustering accuracy andstability",https://arxiv.org/abs/1910.09731
['Title:Convolutional Prototype Learning for Zero-Shot Recognition'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  Zero-shot learning (ZSL) has received increasing attention in recent yearsespecially in areas of fine-grained object recognition, retrieval, and imagecaptioning. The key to ZSL is to transfer knowledge from the seen to the unseenclasses via auxiliary class attribute vectors. However, the popularly learnedprojection functions in previous works cannot generalize well since they assumethe distribution consistency between seen and unseen domains atsample-level.Besides, the provided non-visual and unique class attributes cansignificantly degrade the recognition performance in semantic space. In thispaper, we propose a simple yet effective convolutional prototype learning (CPL)framework for zero-shot recognition. By assuming distribution consistency attask-level, our CPL is capable of transferring knowledge smoothly to recognizeunseen samples.Furthermore, inside each task, discriminative visual prototypesare learned via a distance based training mechanism. Consequently, we canperform recognition in visual space, instead of semantic space. An extensivegroup of experiments are then carefully designed and presented, demonstratingthat CPL obtains more favorable effectiveness, over currently availablealternatives under various settings.",https://arxiv.org/abs/1910.09728
['Title:Spatiotemporal Emotion Recognition using Deep CNN Based on EEG during Music Listening'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  Emotion recognition based on EEG has become an active research area. As oneof the machine learning models, CNN has been utilized to solve diverse problemsincluding issues in this domain. In this work, a study of CNN and itsspatiotemporal feature extraction has been conducted in order to explorecapabilities of the model in varied window sizes and electrode orders. Ourinvestigation was conducted in subject-independent fashion. Results have shownthat temporal information in distinct window sizes significantly affectsrecognition performance in both 10-fold and leave-one-subject-out crossvalidation. Spatial information from varying electrode order has modicum effecton classification. SVM classifier depending on spatiotemporal knowledge on thesame dataset was previously employed and compared to these empirical results.Even though CNN and SVM have a homologous trend in window size effect, CNNoutperformed SVM using leave-one-subject-out cross validation. This could becaused by different extracted features in the elicitation process.",https://arxiv.org/abs/1910.09719
['Title:A deep active learning system for species identification and counting in camera trap images'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  Biodiversity conservation depends on accurate, up-to-date information aboutwildlife population distributions. Motion-activated cameras, also known ascamera traps, are a critical tool for population surveys, as they are cheap andnon-intrusive. However, extracting useful information from camera trap imagesis a cumbersome process: a typical camera trap survey may produce millions ofimages that require slow, expensive manual review. Consequently, criticalinformation is often lost due to resource limitations, and criticalconservation questions may be answered too slowly to support decision-making.Computer vision is poised to dramatically increase efficiency in image-basedbiodiversity surveys, and recent studies have harnessed deep learningtechniques for automatic information extraction from camera trap images.However, the accuracy of results depends on the amount, quality, and diversityof the data available to train models, and the literature has focused onprojects with millions of relevant, labeled training images. Many camera trapprojects do not have a large set of labeled images and hence cannot benefitfrom existing machine learning techniques. Furthermore, even projects that dohave labeled data from similar ecosystems have struggled to adopt deep learningmethods because image classification models overfit to specific imagebackgrounds (i.e., camera locations). In this paper, we focus not on automatingthe labeling of camera trap images, but on accelerating this process. Wecombine the power of machine intelligence and human intelligence to build ascalable, fast, and accurate active learning system to minimize the manual workrequired to identify and count animals in camera trap images. Our proposedscheme can match the state of the art accuracy on a 3.2 million image datasetwith as few as 14,100 manual labels, which means decreasing manual labelingeffort by over 99.5%.",https://arxiv.org/abs/1910.09716
['Title:Smoothness-Adaptive Stochastic Bandits'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  We consider the problem of non-parametric multi-armed bandits with stochasticcovariates, where a key factor in determining the complexity of the problem andin the design of effective policies is the smoothness of payoff functions.Previous work treats this problem when the smoothness of payoff functions are apriori known. In practical settings, however, the smoothness that characterizesthe class of functions to which payoff functions belong is not known inadvance, and misspecification of this smoothness may cause the performance ofexisting methods to severely deteriorate. In this work, we address thechallenge of adapting to a priori unknown smoothness in the payoff functions.Our approach is based on the notion of \textit{self-similarity} that appears inthe literature on adaptive non-parametric confidence intervals. We develop aprocedure that infers a global smoothness parameter of the payoff functionsbased on collected observations, and establish that this procedure achievesrate-optimal performance up to logarithmic factors. We further extend thismethod in order to account for local complexity of the problem which depends onhow smooth payoff functions are in different regions of the covariate space. Weshow that under reasonable assumptions on the way this smoothness changes overthe covariate space, our method achieves significantly improved performancethat is characterized by the local complexity of the problem as opposed to itsglobal complexity.",https://arxiv.org/abs/1910.09714
['Title:Collaborative Graph Walk for Semi-supervised Multi-Label Node Classification'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  In this work, we study semi-supervised multi-label node classificationproblem in attributed graphs. Classic solutions to multi-label nodeclassification follow two steps, first learn node embedding and then build anode classifier on the learned embedding. To improve the discriminating powerof the node embedding, we propose a novel collaborative graph walk, namedMulti-Label-Graph-Walk, to finely tune node representations with the availablelabel assignments in attributed graphs via reinforcement learning. The proposedmethod formulates the multi-label node classification task as simultaneousgraph walks conducted by multiple label-specific agents. Furthermore, policiesof the label-wise graph walks are learned in a cooperative way to capture firstthe predictive relation between node labels and structural attributes ofgraphs; and second, the correlation among the multiple label-specificclassification tasks. A comprehensive experimental study demonstrates that theproposed method can achieve significantly better multi-label classificationperformance than the state-of-the-art approaches and conduct more efficientgraph exploration.",https://arxiv.org/abs/1910.09706
['Title:Learning to Make Generalizable and Diverse Predictions for Retrosynthesis'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  We propose a new model for making generalizable and diverse retrosyntheticreaction predictions. Given a target compound, the task is to predict thelikely chemical reactants to produce the target. This generative task can beframed as a sequence-to-sequence problem by using the SMILES representations ofthe molecules. Building on top of the popular Transformer architecture, wepropose two novel pre-training methods that construct relevant auxiliary tasks(plausible reactions) for our problem. Furthermore, we incorporate a discretelatent variable model into the architecture to encourage the model to produce adiverse set of alternative predictions. On the 50k subset of reaction examplesfrom the United States patent literature (USPTO-50k) benchmark dataset, ourmodel greatly improves performance over the baseline, while also generatingpredictions that are more diverse.",https://arxiv.org/abs/1910.09688
['Title:Signal Combination for Language Identification'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Google's multilingual speech recognition system combines low-level acousticsignals with language-specific recognizer signals to better predict thelanguage of an utterance. This paper presents our experience with differentsignal combination methods to improve overall language identification accuracy.We compare the performance of a lattice-based ensemble model and a deep neuralnetwork model to combine signals from recognizers with that of a baseline thatonly uses low-level acoustic signals. Experimental results show that the deepneural network model outperforms the lattice-based ensemble model, and itreduced the error rate from 5.5% in the baseline to 4.3%, which is a 21.8%relative reduction.",https://arxiv.org/abs/1910.09687
['Title:Multiphase flow prediction with deep neural networks'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  This paper proposes a deep neural network approach for predicting multiphaseflow in heterogeneous domains with high computational efficiency. The deepneural network model is able to handle permeability heterogeneity in highdimensional systems, and can learn the interplay of viscous, gravity, andcapillary forces from small data sets. Using the example of carbon dioxide(CO2) storage, we demonstrate that the model can generate highly accuratepredictions of a CO2 saturation distribution given a permeability field,injection duration, injection rate, and injection location. The trained neuralnetwork model has an excellent ability to interpolate and to a limited extent,the ability to extrapolate beyond the training data ranges. To improve theprediction accuracy when the neural network model needs to extrapolate, wepropose a transfer learning (fine-tuning) procedure that can quickly teach theneural network model new information without going through massive datacollection and retraining. Based on this trained neural network model, aweb-based tool is provided that allows users to perform CO2-water multiphaseflow calculations online. With the tools provided in this paper, the deepneural network approach can provide a computationally efficient substitute forrepetitive forward multiphase flow simulations, which can be adopted to thecontext of history matching and uncertainty quantification.",https://arxiv.org/abs/1910.09657
['Title:Stability of Graph Neural Networks to Relative Perturbations'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Graph neural networks (GNNs), consisting of a cascade of layers applying agraph convolution followed by a pointwise nonlinearity, have become a powerfularchitecture to process signals supported on graphs. Graph convolutions (andthus, GNNs), rely heavily on knowledge of the graph for operation. However, inmany practical cases the GSO is not known and needs to be estimated, or mightchange from training time to testing time. In this paper, we are set to studythe effect that a change in the underlying graph topology that supports thesignal has on the output of a GNN. We prove that graph convolutions withintegral Lipschitz filters lead to GNNs whose output change is bounded by thesize of the relative change in the topology. Furthermore, we leverage thisresult to show that the main reason for the success of GNNs is that they arestable architectures capable of discriminating features on high eigenvalues,which is a feat that cannot be achieved by linear graph filters (which areeither stable or discriminative, but cannot be both). Finally, we comment onthe use of this result to train GNNs with increased stability and runexperiments on movie recommendation systems.",https://arxiv.org/abs/1910.09655
['Title:Causal bootstrapping'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  To draw scientifically meaningful conclusions and build reliable models ofquantitative phenomena, cause and effect must be taken into consideration(either implicitly or explicitly). This is particularly challenging when themeasurements are not from controlled experimental (interventional) settings,since cause and effect can be obscured by spurious, indirect influences. Modernpredictive techniques from machine learning are capable of capturinghigh-dimensional, nonlinear relationships between variables while relying onfew parametric or probabilistic model assumptions. However, since thesetechniques are associational, applied to observational data they are prone topicking up spurious influences from non-experimental (observational) data,making their predictions unreliable. Techniques from causal inference, such asprobabilistic causal diagrams and do-calculus, provide powerful (nonparametric)tools for drawing causal inferences from such observational data. However,these techniques are often incompatible with modern, nonparametric machinelearning algorithms since they typically require explicit probabilistic models.Here, we develop causal bootstrapping for augmenting classical nonparametricbootstrap resampling with information on the causal relationship betweenvariables. This makes it possible to resample observational data such that, ifit is possible to identify an interventional relationship from that data, newdata representing that relationship can be simulated from the originalobservational data. In this way, we can use modern machine learning algorithmsunaltered to make statistically powerful, yet causally-robust, predictions. Wedevelop several causal bootstrapping algorithms for drawing interventionalinferences from observational data, for classification and regression problems,and demonstrate, using synthetic and real-world examples, the value of thisapproach.",https://arxiv.org/abs/1910.09648
['Title:GANspection'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Generative Adversarial Networks (GANs) have been used extensively and quitesuccessfully for unsupervised learning. As GANs don't approximate an explicitprobability distribution, it's an interesting study to inspect the latent spacerepresentations learned by GANs. The current work seeks to push the boundariesof such inspection methods to further understand in more detail the manifoldbeing learned by GANs. Various interpolation and extrapolation techniques alongwith vector arithmetic is used to understand the learned manifold. We showthrough experiments that GANs indeed learn a data probability distributionrather than memorize images/data. Further, we prove that GANs encodesemantically relevant information in the learned probability distribution. Theexperiments have been performed on two publicly available datasets - LargeScale Scene Understanding (LSUN) and CelebA.",https://arxiv.org/abs/1910.09638
['Title:Non-Gaussianity of Stochastic Gradient Noise'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  What enables Stochastic Gradient Descent (SGD) to achieve bettergeneralization than Gradient Descent (GD) in Neural Network training? Thisquestion has attracted much attention. In this paper, we study the distributionof the Stochastic Gradient Noise (SGN) vectors during the training. We observethat for batch sizes 256 and above, the distribution is best described asGaussian at-least in the early phases of training. This holds across data-sets,architectures, and other choices.",https://arxiv.org/abs/1910.09626
['Title:You May Not Need Order in Time Series Forecasting'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Time series forecasting with limited data is a challenging yet critical task.While transformers have achieved outstanding performances in time seriesforecasting, they often require many training samples due to the large numberof trainable parameters. In this paper, we propose a training technique fortransformers that prepares the training windows through random sampling. Asinput time steps need not be consecutive, the number of distinct samplesincreases from linearly to combinatorially many. By breaking the temporalorder, this technique also helps transformers to capture dependencies amongtime steps in finer granularity. We achieve competitive results compared to thestate-of-the-art on real-world datasets.",https://arxiv.org/abs/1910.09620
['Title:IPO: Interior-point Policy Optimization under Constraints'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  In this paper, we study reinforcement learning (RL) algorithms to solvereal-world decision problems with the objective of maximizing the long-termreward as well as satisfying cumulative constraints. We propose a novelfirst-order policy optimization method, Interior-point Policy Optimization(IPO), which augments the objective with logarithmic barrier functions,inspired by the interior-point method. Our proposed method is easy to implementwith performance guarantees and can handle general types of cumulativemulticonstraint settings. We conduct extensive evaluations to compare ourapproach with state-of-the-art baselines. Our algorithm outperforms thebaseline algorithms, in terms of reward maximization and constraintsatisfaction.",https://arxiv.org/abs/1910.09615
['Title:Universal flow approximation with deep residual networks'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Residual networks (ResNets) are a deep learning architecture with therecursive structure \[x_{k+1} = x_k + R_k(x_k)\] where $R_k$ is a neuralnetwork and the copying of the input $x_k$ is called a skip connection. Thisstructure can be seen as the explicit Euler discretisation of an associatedordinary differential equation. We use this interpretation to show that bysimultaneously increasing the number of skip connection as well as theexpressivity of the networks $R_k$ the flow of an arbitrary right hand side\[f\in L^1\left( I; \mathcal C_b^{0, 1}(\mathbb R^d; \mathbb R^d)\right)\] canbe approximated uniformly by deep ReLU ResNets on compact sets. Further, wederive estimates on the number of parameters needed to do this up to aprescribed accuracy under temporal regularity assumptions. Finally, we discussthe possibility of using ResNets for diffeomorphic matching problems andpropose some next steps in the theoretical foundation of this approach.",https://arxiv.org/abs/1910.09599
['Title:Federated Neuromorphic Learning of Spiking Neural Networks for Low-Power Edge Intelligence'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Spiking Neural Networks (SNNs) offer a promising alternative to conventionalArtificial Neural Networks (ANNs) for the implementation of on-device low-poweronline learning and inference. On-device training is, however, constrained bythe limited amount of data available at each device. In this paper, we proposeto mitigate this problem via cooperative training through Federated Learning(FL). To this end, we introduce an online FL-based learning rule for networkedon-device SNNs, which we refer to as FL-SNN. FL-SNN leverages local feedbacksignals within each SNN, in lieu of backpropagation, and global feedbackthrough communication via a base station. The scheme demonstrates significantadvantages over separate training and features a flexible trade-off betweencommunication load and accuracy via the selective exchange of synaptic weights.",https://arxiv.org/abs/1910.09594
['Title:Edge Dithering for Robust Adaptive Graph Convolutional Networks'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Graph convolutional networks (GCNs) are vulnerable to perturbations of thegraph structure that are either random, or, adversarially designed. Theperturbed links modify the graph neighborhoods, which critically affects theperformance of GCNs in semi-supervised learning (SSL) tasks. Aiming atrobustifying GCNs conditioned on the perturbed graph, the present papergenerates multiple auxiliary graphs, each having its binary 0-1 edge weightsflip values with probabilities designed to enhance robustness. The resultantedge-dithered auxiliary graphs are leveraged by an adaptive (A)GCN thatperforms SSL. Robustness is enabled through learnable graph-combining weightsalong with suitable regularizers. Relative to GCN, the novel AGCN achievesmarkedly improved performance in tests with noisy inputs, graph perturbations,and state-of-the-art adversarial attacks. Further experiments with proteininteraction networks showcase the competitive performance of AGCN for SSL overmultiple graphs.",https://arxiv.org/abs/1910.09590
['Title:GraphSAC: Detecting anomalies in large-scale graphs'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  A graph-based sampling and consensus (GraphSAC) approach is introduced toeffectively detect anomalous nodes in large-scale graphs. Existing approachesrely on connectivity and attributes of all nodes to assign an anomaly score pernode. However, nodal attributes and network links might be compromised byadversaries, rendering these holistic approaches vulnerable. Alleviating thislimitation, GraphSAC randomly draws subsets of nodes, and relies on graph-awarecriteria to judiciously filter out sets contaminated by anomalous nodes, beforeemploying a semi-supervised learning (SSL) module to estimate nominal labeldistributions per node. These learned nominal distributions are minimallyaffected by the anomalous nodes, and hence can be directly adopted for anomalydetection. Rigorous analysis provides performance guarantees for GraphSAC, bybounding the required number of draws. The per-draw complexity grows linearlywith the number of edges, which implies efficient SSL, while draws can be runin parallel, thereby ensuring scalability to large graphs. GraphSAC is testedunder different anomaly generation models based on random walks, clusteredanomalies, as well as contemporary adversarial attacks for graph data.Experiments with real-world graphs showcase the advantage of GraphSAC relativeto state-of-the-art alternatives.",https://arxiv.org/abs/1910.09589
['Title:Collapsed Amortized Variational Inference for Switching Nonlinear Dynamical Systems'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  We propose an efficient inference method for switching nonlinear dynamicalsystems. The key idea is to learn an inference network which can be used as aproposal distribution for the continuous latent variables, while performingexact marginalization of the discrete latent variables. This allows us to usethe reparameterization trick, and apply end-to-end training with stochasticgradient descent. We show that the proposed method can successfully segmenttime series data (including videos) into meaningful ""regimes"", by using thepiece-wise nonlinear dynamics.",https://arxiv.org/abs/1910.09588
['Title:On Predictive Information Sub-optimality of RNNs'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Certain biological neurons demonstrate a remarkable capability to optimallycompress the history of sensory inputs while being maximally informative aboutthe future. In this work, we investigate if the same can be said of artificialneurons in recurrent neural networks (RNNs) trained with maximum likelihood. Inexperiments on two datasets, restorative Brownian motion and a hand-drawnsketch dataset, we find that RNNs are sub-optimal in the information plane.Instead of optimally compressing past information, they extract additionalinformation that is not relevant for predicting the future. Overcoming thislimitation may require alternative training procedures and architectures, orobjectives beyond maximum likelihood estimation.",https://arxiv.org/abs/1910.09578
['Title:Detecting Extrapolation with Local Ensembles'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  We present local ensembles, a method for detecting extrapolation at test timein a pre-trained model. We focus on underdetermination as a key component ofextrapolation: we aim to detect when many possible predictions are consistentwith the training data and model class. Our method uses local second-orderinformation to approximate the variance of predictions across an ensemble ofmodels from the same class. We compute this approximation by estimating thenorm of the component of a test point's gradient that aligns with thelow-curvature directions of the Hessian, and provide a tractable method forestimating this quantity. Experimentally, we show that our method is capable ofdetecting when a pre-trained model is extrapolating on test data, withapplications to out-of-distribution detection, detecting spurious correlates,and active learning.",https://arxiv.org/abs/1910.09573
['Title:Distributed interference cancellation in multi-agent scenarios'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  This paper considers the problem of detecting impaired and noisy nodes overnetwork. In a distributed algorithm, lots of processing units are incorporatingand communicating with each other to reach a global goal. Due to each one'sstate in the shared environment, they can help the other nodes or mislead them(due to noise or a deliberate attempt). Previous works mainly focused on properlocating agents and weight assignment based on initial environment state tominimize malfunctioning of noisy nodes. We propose an algorithm to be able toadapt sharing weights according to behavior of the agents. Applying theintroduced algorithm to a multi-agent RL scenario and the well-known diffusionLMS demonstrates its capability and generality.",https://arxiv.org/abs/1910.10109
['Title:Cross-Representation Transferability of Adversarial Perturbations: From Spectrograms to Audio Waveforms'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  This paper shows the susceptibility of spectrogram-based audio classifiers toadversarial attacks and the transferability of such attacks to audio waveforms.Some commonly adversarial attacks to images have been applied to Mel-frequencyand short-time Fourier transform spectrograms and such perturbed spectrogramsare able to fool a 2D convolutional neural network (CNN) for music genreclassification with a high fooling rate and high confidence. Such attacksproduce perturbed spectrograms that are visually imperceptible by humans.Experimental results on a dataset of western music have shown that the 2D CNNachieves up to 81.87% of mean accuracy on legitimate examples and such aperformance drops to 12.09% on adversarial examples. Furthermore, the audiosignals reconstructed from the adversarial spectrograms produce audio waveformsthat perceptually resemble the legitimate audio.",https://arxiv.org/abs/1910.10106
['Title:Modeling plate and spring reverberation using a DSP-informed deep neural network'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  Plate and spring reverberators are electromechanical systems first used andresearched as means to substitute real room reverberation. Nowadays they areoften used in music production for aesthetic reasons due to their particularsonic characteristics. The modeling of these audio processors and theirperceptual qualities is difficult since they use mechanical elements togetherwith analog electronics resulting in an extremely complex response. Based ondigital reverberators that use sparse FIR filters, we propose a signalprocessing-informed deep learning architecture for the modeling of artificialreverberators. We explore the capabilities of deep neural networks to learnsuch highly nonlinear electromechanical responses and we perform modeling ofplate and spring reverberators. In order to measure the performance of themodel, we conduct a perceptual evaluation experiment and we also analyze howthe given task is accomplished and what the model is actually learning.",https://arxiv.org/abs/1910.10105
['Title:Image processing in DNA'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  The main obstacles for the practical deployment of DNA-based data storageplatforms are the prohibitively high cost of synthetic DNA and the large numberof errors introduced during synthesis. In particular, synthetic DNA productscontain both individual oligo (fragment) symbol errors as well as missing DNAoligo errors, with rates that exceed those of modern storage systems by ordersof magnitude. These errors can be corrected either through the use of a largenumber of redundant oligos or through cycles of writing, reading, and rewritingof information that eliminate the errors. Both approaches add to the overallstorage cost and are hence undesirable. Here we propose the first method forstoring quantized images in DNA that uses signal processing and machinelearning techniques to deal with error and cost issues without resorting to theuse of redundant oligos or rewriting. Our methods rely on decoupling the RGBchannels of images, performing specialized quantization and compression on theindividual color channels, and using new discoloration detection and imageinpainting techniques. We demonstrate the performance of our approachexperimentally on a collection of movie posters stored in DNA.",https://arxiv.org/abs/1910.10095
['Title:Continual Learning for Infinite Hierarchical Change-Point Detection'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  Change-point detection (CPD) aims to locate abrupt transitions in thegenerative model of a sequence of observations. When Bayesian methods areconsidered, the standard practice is to infer the posterior distribution of thechange-point locations. However, for complex models (high-dimensional orheterogeneous), it is not possible to perform reliable detection. To circumventthis problem, we propose to use a hierarchical model, which yields observationsthat belong to a lower-dimensional manifold. Concretely, we consider alatent-class model with an unbounded number of categories, which is based onthe chinese-restaurant process (CRP). For this model we derive a continuallearning mechanism that is based on the sequential construction of the CRP andthe expectation-maximization (EM) algorithm with a stochastic maximizationstep. Our results show that the proposed method is able to recursively inferthe number of underlying latent classes and perform CPD in a reliable manner.",https://arxiv.org/abs/1910.10087
['Title:From Personalization to Privatization: Meta Matrix Factorization for Private Rating Predictions'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  Matrix factorization (MF) techniques have been shown to be effective forrating predictions (RPs) in personalized recommender systems. Existing MFmethods use the same item embeddings and the same RP model for all users, whileignoring the possibility that different users may have different views aboutthe same item and may favor different RP models. We introduce a novel MFframework, named meta matrix factorization (MetaMF), that generates privateitem embeddings and RP models. Given a vector representing a user, we firstobtain a collaborative vector by collecting useful information from all userswith a collaborative memory (CM) module. Then, we employ a meta recommender(MR) module to generate private item embeddings and a RP model based on thecollaborative vector. To address the challenge of generating a large number ofhigh-dimensional item embeddings, we devise a rise-dimensional generation (RG)strategy that first generates a low-dimensional item embedding matrix and arise-dimensional matrix, and then multiply them to obtain high-dimensionalembeddings. Finally, we use the generated model to produce private RPs for agiven user. Experiments on two benchmark datasets show that MetaMF outperformsstate-of-the-art MF methods. MetaMF generates similar/dissimilar itemembeddings and models for different users to flexibly exploit collaborativefiltering (CF), demonstrating the benefits of MetaMF.",https://arxiv.org/abs/1910.10086
['Title:Separation of Chaotic Signals by Reservoir Computing'],Machine Learning ,(Submitted on 18 Oct 2019),"Abstract:  We demonstrate the utility of machine learning in the separation ofsuperimposed chaotic signals using a technique called Reservoir Computing. Weassume no knowledge of the dynamical equations that produce the signals, andrequire only training data consisting of finite time samples of the componentsignals. We test our method on signals that are formed as linear combinationsof signals from two Lorenz systems with different parameters. Comparing ournonlinear method with the optimal linear solution to the separation problem,the Wiener filter, we find that our method significantly outperforms the Wienerfilter in all the scenarios we study. Furthermore, this difference isparticularly striking when the component signals have similar frequencyspectra. Indeed, our method works well when the component frequency spectra areindistinguishable - a case where a Wiener filter performs essentially noseparation.",https://arxiv.org/abs/1910.10080
['Title:Optimizing electrode positions for 2D Electrical Impedance Tomography sensors using deep learning'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Electrical Impedance Tomography (EIT) is a powerful tool for non-destructiveevaluation, state estimation, process tomography, and much more. For theseapplications, and in order to reliably reconstruct images of a given processusing EIT, we must obtain high-quality voltage measurements from the EIT sensor(or structure) of interest. Inasmuch, it is no surprise that the locations ofelectrodes used for measuring plays a key role in this task. Yet, to date,methods for optimally placing electrodes either require knowledge on the EITtarget (which is, in practice, never fully known), are computationallydifficult to implement numerically, or require electrode segmentation. In thispaper, we circumvent these challenges and present a straightforward deeplearning based approach for optimizing electrodes positions. It is found thatthe optimized electrode positions outperformed ""standard"" uniformly-distributedelectrode layouts in all test cases using a metric independent from theoptimization parameters.",https://arxiv.org/abs/1910.10077
['Title:Prediction of Reaction Time and Vigilance Variability from Spatiospectral Features of Resting-State EEG in a Long Sustained Attention Task'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Resting-state brain networks represent the intrinsic state of the brainduring the majority of cognitive and sensorimotor tasks. However, no study hasyet presented concise predictors of task-induced vigilance variability fromspectrospatial features of the pre-task, resting-state electroencephalograms(EEG). We asked ten healthy volunteers (6 females, 4 males) to participate in105-minute fixed-sequence-varying-duration sessions of sustained attention toresponse task (SART). A novel and adaptive vigilance scoring scheme wasdesigned based on the performance and response time in consecutive trials, anddemonstrated large inter-participant variability in terms of maintainingconsistent tonic performance. Multiple linear regression using featurerelevance analysis obtained significant predictors of the mean cumulativevigilance score (CVS), mean response time, and variabilities of these scoresfrom the resting-state, band-power ratios of EEG signals, p<0.05. Single-layerneural networks trained with cross-validation also captured differentassociations for the beta sub-bands. Increase in the gamma (28-48 Hz) and upperbeta ratios from the left central and temporal regions predicted slowerreactions and more inconsistent vigilance as explained by the increasedactivation of default mode network (DMN) and differences between the high- andlow-attention networks at temporal regions. Higher ratios of parietal alphafrom the Brodmann's areas 18, 19, and 37 during the eyes-open states predictedslower responses but more consistent CVS and reactions associated with thesuperior ability in vigilance maintenance. The proposed framework and thesefindings on the most stable and significant attention predictors from theintrinsic EEG power ratios can be used to model attention variations during thecalibration sessions of BCI applications and vigilance monitoring systems.",https://arxiv.org/abs/1910.10076
['Title:Automatic Generation of Multi-precision Multi-arithmetic CNN Accelerators for FPGAs'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Modern deep Convolutional Neural Networks (CNNs) are computationallydemanding, yet real applications often require high throughput and low latency.To help tackle these problems, we propose Tomato, a framework designed toautomate the process of generating efficient CNN accelerators. The generateddesign is pipelined and each convolution layer uses different arithmetics atvarious precisions. Using Tomato, we showcase state-of-the-art multi-precisionmulti-arithmetic networks, including MobileNet-V1, running on FPGAs. To ourknowledge, this is the first multi-precision multi-arithmetic auto-generationframework for CNNs. In software, Tomato fine-tunes pretrained networks to use amixture of short powers-of-2 and fixed-point weights with a minimal loss inclassification accuracy. The fine-tuned parameters are combined with thetemplated hardware designs to automatically produce efficient inferencecircuits in FPGAs. We demonstrate how our approach significantly reduces modelsizes and computation complexities, and permits us to pack a complete ImageNetnetwork onto a single FPGA without accessing off-chip memories for the firsttime. Furthermore, we show how Tomato produces implementations of networks withvarious sizes running on single or multiple FPGAs. To the best of ourknowledge, our automatically generated accelerators outperform closestFPGA-based competitors by at least 2-4x for lantency and throughput; thegenerated accelerator runs ImageNet classification at a rate of more than 3000frames per second.",https://arxiv.org/abs/1910.10075
['Title:Depth-Adaptive Transformer'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  State of the art sequence-to-sequence models perform a fixed number ofcomputations for each input sequence regardless of whether it is easy or hardto process. In this paper, we train Transformer models which can make outputpredictions at different stages of the network and we investigate differentways to predict how much computation is required for a particular sequence.Unlike dynamic computation in Universal Transformers, which applies the sameset of layers iteratively, we apply different layers at every step to adjustboth the amount of computation as well as the model capacity. Experiments onmachine translation benchmarks show that this approach can match the accuracyof a baseline Transformer while using only half the number of decoder layers.",https://arxiv.org/abs/1910.10073
['Title:Attacking Optical Flow'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  Deep neural nets achieve state-of-the-art performance on the problem ofoptical flow estimation. Since optical flow is used in several safety-criticalapplications like self-driving cars, it is important to gain insights into therobustness of those techniques. Recently, it has been shown that adversarialattacks easily fool deep neural networks to misclassify objects. The robustnessof optical flow networks to adversarial attacks, however, has not been studiedso far. In this paper, we extend adversarial patch attacks to optical flownetworks and show that such attacks can compromise their performance. We showthat corrupting a small patch of less than 1% of the image size cansignificantly affect optical flow estimates. Our attacks lead to noisy flowestimates that extend significantly beyond the region of the attack, in manycases even completely erasing the motion of objects in the scene. Whilenetworks using an encoder-decoder architecture are very sensitive to theseattacks, we found that networks using a spatial pyramid architecture are lessaffected. We analyse the success and failure of attacking both architectures byvisualizing their feature maps and comparing them to classical optical flowtechniques which are robust to these attacks. We also demonstrate that suchattacks are practical by placing a printed pattern into real scenes.",https://arxiv.org/abs/1910.10053
['Title:Uncertainty Quantification with Generative Models'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  We develop a generative model-based approach to Bayesian inverse problems,such as image reconstruction from noisy and incomplete images. Our frameworkaddresses two common challenges of Bayesian reconstructions: 1) It makes use ofcomplex, data-driven priors that comprise all available information about theuncorrupted data distribution. 2) It enables computationally tractableuncertainty quantification in the form of posterior analysis in latent and dataspace. The method is very efficient in that the generative model only has to betrained once on an uncorrupted data set, after that, the procedure can be usedfor arbitrary corruption types.",https://arxiv.org/abs/1910.10046
"['Title:Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI']",Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  In the last years, Artificial Intelligence (AI) has achieved a notablemomentum that may deliver the best of expectations over many applicationsectors across the field. For this to occur, the entire community stands infront of the barrier of explainability, an inherent problem of AI techniquesbrought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were notpresent in the last hype of AI. Paradigms underlying this problem fall withinthe so-called eXplainable AI (XAI) field, which is acknowledged as a crucialfeature for the practical deployment of AI models. This overview examines theexisting literature in the field of XAI, including a prospect toward what isyet to be reached. We summarize previous efforts to define explainability inMachine Learning, establishing a novel definition that covers prior conceptualpropositions with a major focus on the audience for which explainability issought. We then propose and discuss about a taxonomy of recent contributionsrelated to the explainability of different Machine Learning models, includingthose aimed at Deep Learning methods for which a second taxonomy is built. Thisliterature analysis serves as the background for a series of challenges facedby XAI, such as the crossroads between data fusion and explainability. Ourprospects lead toward the concept of Responsible Artificial Intelligence,namely, a methodology for the large-scale implementation of AI methods in realorganizations with fairness, model explainability and accountability at itscore. Our ultimate goal is to provide newcomers to XAI with a referencematerial in order to stimulate future research advances, but also to encourageexperts and professionals from other disciplines to embrace the benefits of AIin their activity sectors, without any prior bias for its lack ofinterpretability.",https://arxiv.org/abs/1910.10045
['Title:Human Action Recognition in Drone Videos using a Few Aerial Training Examples'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  Drones are enabling new forms of human actions surveillance due to their lowcost and fast mobility. However, using deep neural networks for automaticaerial action recognition is difficult due to the need for the humongous numberof aerial human action videos needed for training. Collecting a largecollection of human action aerial videos is costly, time-consuming anddifficult. In this paper, we explore two alternative data sources to improveaerial action classification when only a few training aerial examples areavailable. As a first data source, we resort to video games. We collect plentyof ground and aerial videos pairs of human actions from video games. For thesecond data source, we generate discriminative fake aerial examples usingconditional Wasserstein Generative Adversarial Networks. We integrate featuresfrom both game action videos and fake aerial examples with a few availableaerial training examples using disjoint multitask learning. We validate theproposed approach on several aerial action datasets and demonstrate that aerialgames and generated fake aerial examples can be extremely useful for animproved action recognition in real aerial videos when only a few aerialtraining examples are available.",https://arxiv.org/abs/1910.10027
['Title:Compressive Learning for Semi-Parametric Models'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  In the compressive learning theory, instead of solving a statistical learningproblem from the input data, a so-called sketch is computed from the data priorto learning. The sketch has to capture enough information to solve the problemdirectly from it, allowing to discard the dataset from the memory. This isuseful when dealing with large datasets as the size of the sketch does notscale with the size of the database. In this paper, we reformulate the originalcompressive learning framework to explicitly cater for the class ofsemi-parametric models. The reformulation takes account of the inherenttopology and structure of semi-parametric models, creating an intuitive pathwayto the development of compressive learning algorithms. We apply our developedframework to both the semi-parametric models of independent component analysisand subspace clustering, demonstrating the robustness of the framework toexplicitly show when a compression in complexity can be achieved.",https://arxiv.org/abs/1910.10024
['Title:Learning Resilient Behaviors for Navigation Under Uncertainty Environments'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  Deep reinforcement learning has great potential to acquire complex, adaptivebehaviors for autonomous agents automatically. However, the underlying neuralnetwork polices have not been widely deployed in real-world applications,especially in these safety-critical tasks (e.g., autonomous driving). One ofthe reasons is that the learned policy cannot perform flexible and resilientbehaviors as traditional methods to adapt to diverse environments. In thispaper, we consider the problem that a mobile robot learns adaptive andresilient behaviors for navigating in unseen uncertain environments whileavoiding collisions. We present a novel approach for uncertainty-awarenavigation by introducing an uncertainty-aware predictor to model theenvironmental uncertainty, and we propose a novel uncertainty-aware navigationnetwork to learn resilient behaviors in the prior unknown environments. Totrain the proposed uncertainty-aware network more stably and efficiently, wepresent the temperature decay training paradigm, which balances exploration andexploitation during the training process. Our experimental evaluationdemonstrates that our approach can learn resilient behaviors in diverseenvironments and generate adaptive trajectories according to environmentaluncertainties.",https://arxiv.org/abs/1910.09998
['Title:Sequence-to-sequence Singing Synthesis Using the Feed-forward Transformer'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  We propose a sequence-to-sequence singing synthesizer, which avoids the needfor training data with pre-aligned phonetic and acoustic features. Rather thanthe more common approach of a content-based attention mechanism combined withan autoregressive decoder, we use a different mechanism suitable forfeed-forward synthesis. Given that phonetic timings in singing are highlyconstrained by the musical score, we derive an approximate initial alignmentwith the help of a simple duration model. Then, using a decoder based on afeed-forward variant of the Transformer model, a series of self-attention andconvolutional layers refines the result of the initial alignment to reach thetarget acoustic features. Advantages of this approach include faster inferenceand avoiding the exposure bias issues that affect autoregressive models trainedby teacher forcing. We evaluate the effectiveness of this model compared to anautoregressive baseline, the importance of self-attention, and the importanceof the accuracy of the duration model.",https://arxiv.org/abs/1910.09989
['Title:Orthogonal Nonnegative Tucker Decomposition'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  In this paper, we study the nonnegative tensor data and propose an orthogonalnonnegative Tucker decomposition (ONTD). We discuss some properties of ONTD anddevelop a convex relaxation algorithm of the augmented Lagrangian function tosolve the optimization problem. The convergence of the algorithm is given. Weemploy ONTD on the image data sets from the real world applications includingface recognition, image representation, hyperspectral unmixing. Numericalresults are shown to illustrate the effectiveness of the proposed algorithm.",https://arxiv.org/abs/1910.09979
['Title:Deep Set-to-Set Matching and Learning'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  Matching two sets of items, called set-to-set matching problem, is beingrecently raised. The difficulties of set-to-set matching over ordinary datamatching lie in the exchangeability in 1) set-feature extraction and 2)set-matching score; the pair of sets and the items in each set should beexchangeable. In this paper, we propose a deep learning architecture for theset-to-set matching that overcomes the above difficulties, including two novelmodules: 1) a cross-set transformation and 2) cross-similarity function. Theformer provides the exchangeable set-feature through interactions between twosets in intermediate layers, and the latter provides the exchangeable setmatching through calculating the cross-feature similarity of items between twosets. We evaluate the methods through experiments with two industrialapplications, fashion set recommendation, and group re-identification. Throughthese experiments, we show that the proposed methods perform better than abaseline given by an extension of the Set Transformer, the state-of-the-artset-input function.",https://arxiv.org/abs/1910.09972
['Title:Convolutional Neural Networks for Space-Time Block Coding Recognition'],Machine Learning ,(Submitted on 19 Oct 2019),"Abstract:  We find that the latest advances in machine learning with deep neural networkby applying them to the task of radio modulation recognition, channel codingrecognition, and spectrum monitor. This paper first proposes a novelidentification algorithm for Space-Time Block coding(STBC) signal. The featurebetween Spatial Multiplexing (SM) and Alamouti (AL) signals is extracted viaadapting convolutional neural networks after preprocessing the receivedsequence. Unlike other algorithms, this method does not require any priorinformation of channel coefficient, and noise power and, consequently, iswell-suited for non-cooperative context. Results show that the proposedalgorithm performs well even at a low signal to noise ratio (SNR).",https://arxiv.org/abs/1910.09952
['Title:Meta-Learning to Communicate: Fast End-to-End Training for Fading Channels'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  When a channel model is available, learning how to communicate on fadingnoisy channels can be formulated as the (unsupervised) training of anautoencoder consisting of the cascade of encoder, channel, and decoder. Animportant limitation of the approach is that training should be generallycarried out from scratch for each new channel. To cope with this problem, priorworks considered joint training over multiple channels with the aim of findinga single pair of encoder and decoder that works well on a class of channels. Asa result, joint training ideally mimics the operation of non-coherenttransmission schemes. In this paper, we propose to obviate the limitations ofjoint training via meta-learning: Rather than training a common model for allchannels, meta-learning finds a common initialization vector that enables fasttraining on any channel. The approach is validated via numerical results,demonstrating significant training speed-ups, with effective encoders anddecoders obtained with as little as one iteration of Stochastic GradientDescent.",https://arxiv.org/abs/1910.09945
['Title:Hypergraph clustering with categorical edge labels'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  Graphs and networks are a standard model for describing data or systems basedon pairwise interactions. Oftentimes, the underlying relationships involve morethan two entities at a time, and hypergraphs are a more faithful model.However, we have fewer rigorous methods that can provide insight from suchrepresentations. Here, we develop a computational framework for the problem ofclustering hypergraphs with categorical edge labels --- or differentinteraction types --- where clusters corresponds to groups of nodes thatfrequently participate in the same type of interaction.Our methodology is based on a combinatorial objective function that isrelated to correlation clustering but enables the design of much more efficientalgorithms. When there are only two label types, our objective can be optimizedin polynomial time, using an algorithm based on minimum cuts. Minimizing ourobjective becomes NP-hard with more than two label types, but we develop fastapproximation algorithms based on linear programming relaxations that havetheoretical cluster quality guarantees. We demonstrate the efficacy of ouralgorithms and the scope of the model through problems in edge-label communitydetection, clustering with temporal data, and exploratory data analysis.",https://arxiv.org/abs/1910.09943
['Title:Cross-task pre-training for acoustic scene classification'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  Acoustic scene classification(ASC) and acoustic event detection(AED) aredifferent but related tasks. Acoustic scenes can be shaped by occurred acousticevents which can provide useful information in training ASC tasks. However,most of the datasets are provided without either the acoustic event or scenelabels. Therefore, We explored cross-task pre-training mechanism to utilizeacoustic event information extracted from the pre-trained model to optimize theASC task. We present three cross-task pre-training architectures and evaluatedthem in feature-based and fine-tuning strategies on two datasets respectively:TAU Urban Acoustic Scenes 2019 dataset and TUT Acoustic Scenes 2017 dataset.Results have shown that cross-task pre-training mechanism can significantlyimprove the performance of ASC tasks and the performance of our best modelimproved relatively 9.5% in the TAU Urban Acoustic Scenes 2019 dataset, andalso improved 10% in the TUT Acoustic Scenes 2017 dataset compared with theofficial baseline.",https://arxiv.org/abs/1910.09935
['Title:Improving the Gating Mechanism of Recurrent Neural Networks'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  Gating mechanisms are widely used in neural network models, where they allowgradients to backpropagate more easily through depth or time. However, theirsaturation property introduces problems of its own. For example, in recurrentmodels these gates need to have outputs near 1 to propagate information overlong time-delays, which requires them to operate in their saturation regime andhinders gradient-based learning of the gate mechanism. We address this problemby deriving two synergistic modifications to the standard gating mechanism thatare easy to implement, introduce no additional hyperparameters, and improvelearnability of the gates when they are close to saturation. We show how thesechanges are related to and improve on alternative recently proposed gatingmechanisms such as chrono-initialization and Ordered Neurons. Empirically, oursimple gating mechanisms robustly improve the performance of recurrent modelson a range of applications, including synthetic memorization tasks, sequentialimage classification, language modeling, and reinforcement learning,particularly when long-term dependencies are involved.",https://arxiv.org/abs/1910.09890
['Title:Kernel computations from large-scale random features obtained by Optical Processing Units'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  Approximating kernel functions with random features (RFs)has been asuccessful application of random projections for nonparametric estimation.However, performing random projections presents computational challenges forlarge-scale problems. Recently, a new optical hardware called OpticalProcessing Unit (OPU) has been developed for fast and energy-efficientcomputation of large-scale RFs in the analog domain. More specifically, the OPUperforms the multiplication of input vectors by a large random matrix withcomplex-valued i.i.d. Gaussian entries, followed by the application of anelement-wise squared absolute value operation - this last nonlinearity beingintrinsic to the sensing process. In this paper, we show that this operationresults in a dot-product kernel that has connections to the polynomial kernel,and we extend this computation to arbitrary powers of the feature map.Experiments demonstrate that the OPU kernel and its RF approximation achievecompetitive performance in applications using kernel ridge regression andtransfer learning for image classification. Crucially, thanks to the use of theOPU, these results are obtained with time and energy savings.",https://arxiv.org/abs/1910.09880
['Title:Structure Matters: Towards Generating Transferable Adversarial Images'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  Recent works on adversarial examples for image classification focus ondirectly modifying pixels with minor perturbations. The small perturbationrequirement is imposed to ensure the generated adversarial examples beingnatural and realistic to humans, which, however, puts a curb on the attackspace thus limiting the attack ability and transferability especially forsystems protected by a defense mechanism. In this paper, we propose the novelconcepts of structure patterns and structure-aware perturbations that relax thesmall perturbation constraint while still keeping images natural. The key ideaof our approach is to allow perceptible deviation in adversarial examples whilekeeping structure patterns that are central to a human classifier. Built uponthese concepts, we propose a \emph{structure-preserving attack (SPA)} forgenerating natural adversarial examples with extremely high transferability.Empirical results on the MNIST and the CIFAR10 datasets show that SPAadversarial images can easily bypass strong PGD-based adversarial training andare still effective against SPA-based adversarial training. Further, theytransfer well to other target models with little or no loss of successfulattack rate, thus exhibiting competitive black-box attack performance. Our codeis available at \url{this https URL}.",https://arxiv.org/abs/1910.09821
['Title:Self-Correction for Human Parsing'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  Labeling pixel-level masks for fine-grained semantic segmentation tasks, e.g.human parsing, remains a challenging task. The ambiguous boundary betweendifferent semantic parts and those categories with similar appearance usuallyare confusing, leading to unexpected noises in ground truth masks. To tacklethe problem of learning with label noises, this work introduces a purificationstrategy, called Self-Correction for Human Parsing (SCHP), to progressivelypromote the reliability of the supervised labels as well as the learned models.In particular, starting from a model trained with inaccurate annotations asinitialization, we design a cyclically learning scheduler to infer morereliable pseudo-masks by iteratively aggregating the current learned model withthe former optimal one in an online manner. Besides, those correspondinglycorrected labels can in turn to further boost the model performance. In thisway, the models and the labels will reciprocally become more robust andaccurate during the self-correction learning cycles. Benefiting from thesuperiority of SCHP, we achieve the best performance on two popularsingle-person human parsing benchmarks, including LIP and Pascal-Person-Partdatasets. Our overall system ranks 1st in CVPR2019 LIP Challenge. Code isavailable at this https URL.",https://arxiv.org/abs/1910.09777
['Title:ProDyn0: Inferring calponin homology domain stretching behavior using graph neural networks'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  Graph neural networks are a quickly emerging field for non-Euclidean datathat leverage the inherent graphical structure to predict node, edge, andglobal-level properties of a system. Protein properties can not easily beunderstood as a simple sum of their parts (i.e. amino acids), therefore,understanding their dynamical properties in the context of graphs is attractivefor revealing how perturbations to their structure can affect their globalfunction. To tackle this problem, we generate a database of 2020 mutatedcalponin homology (CH) domains undergoing large-scale separation in moleculardynamics. To predict the mechanosensitive force response, we develop neuralmessage passing networks and residual gated graph convnets which predict theprotein dependent force separation at 86.63 percent, 81.59 kJ/mol/nm MAE, 76.99psec MAE for force mode classification, max force magnitude, max force timerespectively-- significantly better than non-graph-based deep learningtechniques. Towards uniting geometric learning techniques and biophysicalobservables, we premiere our simulation database as a benchmark dataset forfurther development/evaluation of graph neural network architectures.",https://arxiv.org/abs/1910.09738
['Title:Drivers Drowsiness Detection using Condition-Adaptive Representation Learning Framework'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  We propose a condition-adaptive representation learning framework for thedriver drowsiness detection based on 3D-deep convolutional neural network. Theproposed framework consists of four models: spatio-temporal representationlearning, scene condition understanding, feature fusion, and drowsinessdetection. The spatio-temporal representation learning extracts features thatcan describe motions and appearances in video simultaneously. The scenecondition understanding classifies the scene conditions related to variousconditions about the drivers and driving situations such as statuses of wearingglasses, illumination condition of driving, and motion of facial elements suchas head, eye, and mouth. The feature fusion generates a condition-adaptiverepresentation using two features extracted from above models. The detectionmodel recognizes drivers drowsiness status using the condition-adaptiverepresentation. The condition-adaptive representation learning framework canextract more discriminative features focusing on each scene condition than thegeneral representation so that the drowsiness detection method can provide moreaccurate results for the various driving situations. The proposed framework isevaluated with the NTHU Drowsy Driver Detection video dataset. The experimentalresults show that our framework outperforms the existing drowsiness detectionmethods based on visual analysis.",https://arxiv.org/abs/1910.09722
['Title:Embedded Bayesian Network Classifiers'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  Low-dimensional probability models for local distribution functions in aBayesian network include decision trees, decision graphs, and causalindependence models. We describe a new probability model for discrete Bayesiannetworks, which we call an embedded Bayesian network classifier or EBNC. Themodel for a node $Y$ given parents $\bf X$ is obtained from a (usuallydifferent) Bayesian network for $Y$ and $\bf X$ in which $\bf X$ need not bethe parents of $Y$. We show that an EBNC is a special case of a softmaxpolynomial regression model. Also, we show how to identify a non-redundant setof parameters for an EBNC, and describe an asymptotic approximation forlearning the structure of Bayesian networks that contain EBNCs. Unlike thedecision tree, decision graph, and causal independence models, we are unawareof a semantic justification for the use of these models. Experiments are neededto determine whether the models presented in this paper are useful in practice.",https://arxiv.org/abs/1910.09715
['Title:Discriminative Neural Clustering for Speaker Diarisation'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  This paper proposes a novel method for supervised data clustering. Theclustering procedure is modelled by a discriminative sequence-to-sequenceneural network that learns from examples. The effectiveness of theTransformer-based Discriminative Neural Clustering (DNC) model is validated ona speaker diarisation task using the challenging AMI data set, where audiosegments need to be clustered into an unknown number of speakers. The AMIcorpus contains only 147 meetings as training examples for the DNC model, whichis very limited for training an encoder-decoder neural network. Data scarcityis mitigated through three data augmentation schemes proposed in this paper,including Diaconis Augmentation, a novel technique proposed for discriminativeembeddings trained using cosine similarities. Comparing between DNC and thecommonly used spectral clustering algorithm for speaker diarisation shows thatthe DNC approach outperforms its unsupervised counterpart by 29.4% relative.Furthermore, DNC requires no explicit definition of a similarity measurebetween samples, which is a significant advantage considering that such ameasure might be difficult to specify.",https://arxiv.org/abs/1910.09703
['Title:Direct Estimation of Differential Functional Graphical Models'],Machine Learning ,(Submitted on 22 Oct 2019),"Abstract:  We consider the problem of estimating the difference between two functionalundirected graphical models with shared structures. In many applications, dataare naturally regarded as high-dimensional random function vectors rather thanmultivariate scalars. For example, electroencephalography (EEG) data are moreappropriately treated as functions of time. In these problems, not only can thenumber of functions measured per sample be large, but each function is itselfan infinite dimensional object, making estimation of model parameterschallenging. We develop a method that directly estimates the difference ofgraphs, avoiding separate estimation of each graph, and show it is consistentin certain high-dimensional settings. We illustrate finite sample properties ofour method through simulation studies. Finally, we apply our method to EEG datato uncover differences in functional brain connectivity between alcoholics andcontrol subjects.",https://arxiv.org/abs/1910.09701
['Title:Quantifying the Carbon Emissions of Machine Learning'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  From an environmental standpoint, there are a few crucial aspects of traininga neural network that have a major impact on the quantity of carbon that itemits. These factors include: the location of the server used for training andthe energy grid that it uses, the length of the training procedure, and eventhe make and model of hardware on which the training takes place. In order toapproximate these emissions, we present our Machine Learning EmissionsCalculator, a tool for our community to better understand the environmentalimpact of training ML models. We accompany this tool with an explanation of thefactors cited above, as well as concrete actions that individual practitionersand organizations can take to mitigate their carbon emissions.",https://arxiv.org/abs/1910.09700
['Title:Self-Attentive Document Interaction Networks for Permutation Equivariant Ranking'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  How to leverage cross-document interactions to improve ranking performance isan important topic in information retrieval (IR) research. However, this topichas not been well-studied in the learning-to-rank setting and most of theexisting work still treats each document independently while scoring. Therecent development of deep learning shows strength in modeling complexrelationships across sequences and sets. It thus motivates us to study how toleverage cross-document interactions for learning-to-rank in the deep learningframework. In this paper, we formally define the permutation-equivariancerequirement for a scoring function that captures cross-document interactions.We then propose a self-attention based document interaction network and showthat it satisfies the permutation-equivariant requirement, and can generatescores for document sets of varying sizes. Our proposed methods canautomatically learn to capture document interactions without any auxiliaryinformation, and can scale across large document sets. We conduct experimentson three ranking datasets: the benchmark Web30k, a Gmail search, and a GoogleDrive Quick Access dataset. Experimental results show that our proposed methodsare both more effective and efficient than baselines.",https://arxiv.org/abs/1910.09676
['Title:Coercing Machine Learning to Output Physically Accurate Results'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Many machine/deep learning artificial neural networks are trained to simplybe interpolation functions that map input variables to output valuesinterpolated from the training data in a linear/nonlinear fashion. Even whenthe input/output pairs of the training data are physically accurate (e.g. theresults of an experiment or numerical simulation), interpolated quantities candeviate quite far from being physically accurate. Although one could projectthe output of a network into a physically feasible region, such a postprocessis not captured by the energy function minimized when training the network;thus, the final projected result could incorrectly deviate quite far from thetraining data. We propose folding any such projection or postprocess directlyinto the network so that the final result is correctly compared to the trainingdata by the energy function. Although we propose a general approach, weillustrate its efficacy on a specific convolutional neural network that takesin human pose parameters (joint rotations) and outputs a prediction of vertexpositions representing a triangulated cloth mesh. While the original networkoutputs vertex positions with erroneously high stretching and compressionenergies, the new network trained with our physics prior remedies these issuesproducing highly improved results.",https://arxiv.org/abs/1910.09671
['Title:Faster Stochastic Algorithms via History-Gradient Aided Batch Size Adaptation'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Various schemes for adapting batch size have been recently proposed toaccelerate stochastic algorithms. However, existing schemes either applyprescribed batch size adaption or require additional backtracking and conditionverification steps to exploit the information along optimization path. In thispaper, we propose an easy-to-implement scheme for adapting batch size byexploiting history stochastic gradients, based on which we propose the Adaptivebatch size SGD (AbaSGD), AbaSVRG, and AbaSPIDER algorithms. To handle thedependence of the batch size on history stochastic gradients, we develop a newconvergence analysis technique, and show that these algorithms achieve improvedoverall complexity over their vanilla counterparts. Moreover, their convergencerates are adaptive to the optimization landscape that the iterate experiences.Extensive experiments demonstrate that our algorithms substantially outperformexisting competitive algorithms.",https://arxiv.org/abs/1910.09670
['Title:Combining Benefits from Trajectory Optimization and Deep Reinforcement Learning'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Recent breakthroughs both in reinforcement learning and trajectoryoptimization have made significant advances towards real world robotic systemdeployment. Reinforcement learning (RL) can be applied to many problems withoutneeding any modeling or intuition about the system, at the cost of high samplecomplexity and the inability to prove any metrics about the learned policies.Trajectory optimization (TO) on the other hand allows for stability androbustness analyses on generated motions and trajectories, but is only as goodas the often over-simplified derived model, and may have prohibitivelyexpensive computation times for real-time control. This paper seeks to combinethe benefits from these two areas while mitigating their drawbacks by (1)decreasing RL sample complexity by using existing knowledge of the problem withoptimal control, and (2) providing an upper bound estimate on thetime-to-arrival of the combined learned-optimized policy, allowing onlinepolicy deployment at any point in the training process by using the TO as aworst-case scenario action. This method is evaluated for a car model, withapplicability to any mobile robotic system. A video showing policy executioncomparisons can be found at this https URL .",https://arxiv.org/abs/1910.09667
['Title:Learning to Map Natural Language Instructions to Physical Quadcopter Control using Simulated Flight'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  We propose a joint simulation and real-world learning framework for mappingnavigation instructions and raw first-person observations to continuouscontrol. Our model estimates the need for environment exploration, predicts thelikelihood of visiting environment positions during execution, and controls theagent to both explore and visit high-likelihood positions. We introduceSupervised Reinforcement Asynchronous Learning (SuReAL). Learning uses bothsimulation and real environments without requiring autonomous flight in thephysical environment during training, and combines supervised learning forpredicting positions to visit and reinforcement learning for continuouscontrol. We evaluate our approach on a natural language instruction-followingtask with a physical quadcopter, and demonstrate effective execution andexploration behavior.",https://arxiv.org/abs/1910.09664
['Title:Kernelized Wasserstein Natural Gradient'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Many machine learning problems can be expressed as the optimization of somecost functional over a parametric family of probability distributions. It isoften beneficial to solve such optimization problems using natural gradientmethods. These methods are invariant to the parametrization of the family, andthus can yield more effective optimization. Unfortunately, computing thenatural gradient is challenging as it requires inverting a high dimensionalmatrix at each iteration. We propose a general framework to approximate thenatural gradient for the Wasserstein metric, by leveraging a dual formulationof the metric restricted to a Reproducing Kernel Hilbert Space. Our approachleads to an estimator for gradient direction that can trade-off accuracy andcomputational cost, with theoretical guarantees. We verify its accuracy onsimple examples, and show the advantage of using such an estimator inclassification tasks on Cifar10 and Cifar100 empirically.",https://arxiv.org/abs/1910.09652
['Title:Markov Random Fields for Collaborative Filtering'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  In this paper, we model the dependencies among the items that are recommendedto a user in a collaborative-filtering problem via a Gaussian Markov RandomField (MRF). We build upon Besag's auto-normal parameterization andpseudo-likelihood, which not only enables computationally efficient learning,but also connects the areas of MRFs and sparse inverse covariance estimationwith autoencoders and neighborhood models, two successful approaches incollaborative filtering. We propose a novel approximation for learning sparseMRFs, where the trade-off between recommendation-accuracy and training-time canbe controlled. At only a small fraction of the training-time compared tovarious baselines, including deep nonlinear models, the proposed approachachieved competitive ranking-accuracy on all three well-known data-sets used inour experiments, and notably a 20% gain in accuracy on the data-set with thelargest number of items.",https://arxiv.org/abs/1910.09645
['Title:ConEx: Efficient Exploration of Big-Data System Configurations for Better Performance'],Machine Learning ,(Submitted on 17 Oct 2019),"Abstract:  Configuration space complexity makes the big-data software systems hard toconfigure well. Consider Hadoop, with over nine hundred parameters, developersoften just use the default configurations provided with Hadoop distributions.The opportunity costs in lost performance are significant. Popularlearning-based approaches to auto-tune software does not scale well forbig-data systems because of the high cost of collecting training data. Wepresent a new method based on a combination of Evolutionary Markov Chain MonteCarlo (EMCMC) sampling and cost reduction techniques to cost-effectively findbetter-performing configurations for big data systems. For cost reduction, wedeveloped and experimentally tested and validated two approaches: usingscaled-up big data jobs as proxies for the objective function for larger jobsand using a dynamic job similarity measure to infer that results obtained forone kind of big data problem will work well for similar problems. Ourexperimental results suggest that our approach promises to significantlyimprove the performance of big data systems and that it outperforms competingapproaches based on random sampling, basic genetic algorithms (GA), andpredictive model learning. Our experimental results support the conclusion thatour approach has strongly demonstrated potential to significantly andcost-effectively improve the performance of big data systems.",https://arxiv.org/abs/1910.09644
['Title:CPWC: Contextual Point Wise Convolution for Object Recognition'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Convolutional layers are a major driving force behind the successes of deeplearning. Pointwise convolution (PWC) is a 1x1 convolutional filter that isprimarily used for parameter reduction. However, the PWC ignores the spatialinformation around the points it is processing. This design is by choice, inorder to reduce the overall parameters and computations. However, wehypothesize that this shortcoming of PWC has a significant impact on thenetwork performance. We propose an alternative design for pointwiseconvolution, which uses spatial information from the input efficiently. Ourdesign significantly improves the performance of the networks withoutsubstantially increasing the number of parameters and computations. Weexperimentally show that our design results in significant improvement in theperformance of the network for classification as well as detection.",https://arxiv.org/abs/1910.09643
['Title:Conquering the CNN Over-Parameterization Dilemma: A Volterra Filtering Approach for Action Recognition'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  The importance of inference in Machine Learning (ML) has led to an explosivenumber of different proposals in ML, and particularly in Deep Learning. In anattempt to reduce the complexity of Convolutional Neural Networks, we propose aVolterra filter-inspired Network architecture. This architecture introducescontrolled non-linearities in the form of interactions between the delayedinput samples of data. We propose a cascaded implementation of Volterra Filterso as to significantly reduce the number of parameters required to carry outthe same classification task as that of a conventional Neural Network. Wedemonstrate an efficient parallel implementation of this new Volterra network,along with its remarkable performance while retaining a relatively simpler andpotentially more tractable structure. Furthermore, we show a rathersophisticated adaptation of this network to nonlinearly fuse the RGB (spatial)information and the Optical Flow (temporal) information of a video sequence foraction recognition. The proposed approach is evaluated on UCF-101 and HMDB-51datasets for action recognition, and is shown to outperform state of the artwhen trained on the datasets from scratch (i.e. without pre-training on alarger dataset).",https://arxiv.org/abs/1910.09616
['Title:Is graph biased feature selection of genes better than random?'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Gene interaction graphs aim to capture various relationships between genesand can represent decades of biology research. When trying to make predictionsfrom genomic data, those graphs could be used to overcome the curse ofdimensionality by making machine learning models sparser and more biased withbiological common knowledge. In this work, we focus on assessing whether thosegraphs capture dependencies seen in gene expression data better than random. Weformulate a condition that graphs should satisfy to provide a good bias andpropose to test it using a 'Single Gene Inference' (SGI) task. We comparerandom graphs with seven major gene interaction graphs published by differentresearch groups, aiming to measure the true benefit of using biologicallyrelevant graphs in this context. Our analysis finds that dependencies can becaptured almost as well at random which suggests that, in terms of geneexpression levels, the relevant information about the state of the cell isspread across many genes.",https://arxiv.org/abs/1910.09600
['Title:A Single-MOSFET MAC for Confidence and Resolution (CORE) Driven Machine Learning Classification'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Mixed-signal machine-learning classification has recently been demonstratedas an efficient alternative for classification with power expensive digitalcircuits. In this paper, a high-COnfidence high-REsolution (CORE) mixed-signalclassifier is proposed for classifying high-dimensional input data intomulti-class output space with less power and area than state-of-the-artclassifiers. A high-resolution multiplication is facilitated within asingle-MOSFET by feeding the features and feature weights into, respectively,the body and gate inputs. High-resolution classifier that considers theconfidence of the individual predictors is designed at 45 nm technology nodeand operates at 100 MHz in subthreshold region. To evaluate the performance ofthe classifier, a reduced MNIST dataset is generated by downsampling the MNISTdigit images from 28 $\times$ 28 features to 9 $\times$ 9 features. The systemis simulated across a wide range of PVT variations, exhibiting nominal accuracyof 90%, energy consumption of 6.2 pJ per classification (over 45 times lowerthan state-of-the-art classifiers), area of 2,179 $\mu$$m^{2}$ (over 7.3 timeslower than state-of-the-art classifiers), and a stable response under PVTvariations.",https://arxiv.org/abs/1910.09597
['Title:A Mutual Information Maximization Perspective of Language Representation Learning'],Machine Learning ,(Submitted on 18 Oct 2019),"Abstract:  We show state-of-the-art word representation learning methods maximize anobjective function that is a lower bound on the mutual information betweendifferent parts of a word sequence (i.e., a sentence). Our formulation providesan alternative perspective that unifies classical word embedding models (e.g.,Skip-gram) and modern contextual embeddings (e.g., BERT, XLNet). In addition toenhancing our theoretical understanding of these methods, our derivation leadsto a principled framework that can be used to construct new self-supervisedtasks. We provide an example by drawing inspirations from related methods basedon mutual information maximization that have been successful in computervision, and introduce a simple self-supervised objective that maximizes themutual information between a global sentence representation and n-grams in thesentence. Our analysis offers a holistic view of representation learningmethods to transfer knowledge and translate progress across multiple domains(e.g., natural language processing, computer vision, audio processing).",https://arxiv.org/abs/1910.08350
['Title:Sparsification as a Remedy for Staleness in Distributed Asynchronous SGD'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Large scale machine learning is increasingly relying on distributedoptimization, whereby several machines contribute to the training process of astatistical model. While there exist a large literature on stochastic gradientdescent (SGD) and variants, the study of countermeasures to mitigate problemsarising in asynchronous distributed settings are still in their infancy. Thekey question of this work is whether sparsification, a technique predominantlyused to reduce communication overheads, can also mitigate the staleness problemthat affects asynchronous SGD. We study the role of sparsification boththeoretically and empirically. Our theory indicates that, in an asynchronous,non-convex setting, the ergodic convergence rate of sparsified SGD matches theknown result $\mathcal{O} \left( 1/\sqrt{T} \right)$ of non-convex SGD. We thencarry out an empirical study to complement our theory and show that, inpractice, sparsification consistently improves over vanilla SGD and currentalternatives to mitigate the effects of staleness.",https://arxiv.org/abs/1910.09466
['Title:Learning to Learn by Zeroth-Order Oracle'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  In the learning to learn (L2L) framework, we cast the design of optimizationalgorithms as a machine learning problem and use deep neural networks to learnthe update rules. In this paper, we extend the L2L framework to zeroth-order(ZO) optimization setting, where no explicit gradient information is available.Our learned optimizer, modeled as recurrent neural network (RNN), firstapproximates gradient by ZO gradient estimator and then produces parameterupdate utilizing the knowledge of previous iterations. To reduce high varianceeffect due to ZO gradient estimator, we further introduce another RNN to learnthe Gaussian sampling rule and dynamically guide the query direction sampling.Our learned optimizer outperforms hand-designed algorithms in terms ofconvergence rate and final solution on both synthetic and practical ZOoptimization tasks (in particular, the black-box adversarial attack task, whichis one of the most widely used tasks of ZO optimization). We finally conductextensive analytical experiments to demonstrate the effectiveness of ourproposed optimizer.",https://arxiv.org/abs/1910.09464
['Title:Aleatoric and Epistemic Uncertainty in Machine Learning: A Tutorial Introduction'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  The notion of uncertainty is of major importance in machine learning andconstitutes a key element of machine learning methodology. In line with thestatistical tradition, uncertainty has long been perceived as almost synonymouswith standard probability and probabilistic predictions. Yet, due to thesteadily increasing relevance of machine learning for practical applicationsand related issues such as safety requirements, new problems and challengeshave recently been identified by machine learning scholars, and these problemsmay call for new methodological developments. In particular, this includes theimportance of distinguishing between (at least) two different types ofuncertainty, often refereed to as aleatoric and epistemic. In this paper, weprovide an introduction to the topic of uncertainty in machine learning as wellas an overview of hitherto attempts at handling uncertainty in general andformalizing this distinction in particular.",https://arxiv.org/abs/1910.09457
['Title:Self-Educated Language Agent With Hindsight Experience Replay For Instruction Following'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Language creates a compact representation of the world and allows thedescription of unlimited situations and objectives through compositionality.These properties make it a natural fit to guide the training of interactiveagents as it could ease recurrent challenges in Reinforcement Learning such assample complexity, generalization, or multi-tasking. Yet, it remains anopen-problem to relate language and RL in even simple instruction followingscenarios. Current methods rely on expert demonstrations, auxiliary losses, orinductive biases in neural architectures. In this paper, we propose anorthogonal approach called Textual Hindsight Experience Replay (THER) thatextends the Hindsight Experience Replay approach to the language setting.Whenever the agent does not fulfill its instruction, THER learns to output anew directive that matches the agent trajectory, and it relabels the episodewith a positive reward. To do so, THER learns to map a state into aninstruction by using past successful trajectories, which removes the need tohave external expert interventions to relabel episodes as in vanilla HER. Weobserve that this simple idea also initiates a learning synergy betweenlanguage acquisition and policy learning on instruction following tasks in theBabyAI environment.",https://arxiv.org/abs/1910.09451
['Title:Zero-shot Learning via Simultaneous Generating and Learning'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  To overcome the absence of training data for unseen classes, conventionalzero-shot learning approaches mainly train their model on seen datapoints andleverage the semantic descriptions for both seen and unseen classes. Beyondexploiting relations between classes of seen and unseen, we present a deepgenerative model to provide the model with experience about both seen andunseen classes. Based on the variational auto-encoder with class-specificmulti-modal prior, the proposed method learns the conditional distribution ofseen and unseen classes. In order to circumvent the need for samples of unseenclasses, we treat the non-existing data as missing examples. That is, ournetwork aims to find optimal unseen datapoints and model parameters, byiteratively following the generating and learning strategy. Since we obtain theconditional generative model for both seen and unseen classes, classificationas well as generation can be performed directly without any off-the-shellclassifiers. In experimental results, we demonstrate that the proposedgenerating and learning strategy makes the model achieve the outperformingresults compared to that trained only on the seen classes, and also to theseveral state-of-the-art methods.",https://arxiv.org/abs/1910.09446
['Title:Maximum Probability Principle and Black-Box Priors'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  We present an axiomatic way of assigning probabilities to black box models.In particular, we quantify an upper bound for probability of a model or interms of information theory, a lower bound for amount of information that isstored in a model. In our setup, maximizing probabilities of models isequivalent to removing assumptions or information stored in the model.Furthermore, we represent the problem of learning from an alternative viewwhere the underlying probability space is considered directly. In thisperspective both the true underlying model and the model at hand are events.Consequently, the problem of learning is represented as minimizing theprobability of the symmetric difference of the model and the true underlyingmodel.",https://arxiv.org/abs/1910.09417
['Title:Interpreting Basis Path Set in Neural Networks'],Machine Learning ,(Submitted on 18 Oct 2019),"Abstract:  Based on basis path set, G-SGD algorithm significantly outperformsconventional SGD algorithm in optimizing neural networks. However, how theinner mechanism of basis paths work remains mysterious. From the aspect ofgraph theory, this paper defines basis path, investigates structure propertiesof basis paths in regular fully connected neural network and interprets thegraph representation of basis path set. Moreover, we propose hierarchicalalgorithm HBPS to find basis path set B in fully connected neural network bydecomposing the network into several independent and parallel substructures.Algorithm HBPS demands that there doesn't exist shared edges between any twoindependent substructure paths.",https://arxiv.org/abs/1910.09402
['Title:Towards User Empowerment'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Counterfactual explanations can be obtained by identifying the smallestchange made to a feature vector to qualitatively influence a prediction; forexample, from 'loan rejected' to 'awarded' or from 'high risk of cardiovasculardisease' to 'low risk'. Previous approaches often emphasized thatcounterfactuals should be easily interpretable to humans, motivating sparsesolutions with few changes to the feature vectors. However, these approacheswould not ensure that the produced counterfactuals be proximate (i.e., notlocal outliers) and connected to regions with substantial data density (i.e.,close to correctly classified observations), two requirements known ascounterfactual faithfulness. These requirements are fundamental when makingsuggestions to individuals that are indeed attainable. Our contribution istwofold. On one hand, we suggest to complement the catalogue of counterfactualquality measures [1] using a criterion to quantify the degree of difficulty fora certain counterfactual suggestion. On the other hand, drawing ideas from themanifold learning literature, we develop a framework that generates attainablecounterfactuals. We suggest the counterfactual conditional heterogeneousvariational autoencoder (C-CHVAE) to identify attainable counterfactuals thatlie within regions of high data density.",https://arxiv.org/abs/1910.09398
['Title:Stochastic Recursive Gradient-Based Methods for Projection-Free Online Learning'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  This paper focuses on projection-free methods for solving smooth OnlineConvex Optimization (OCO) problems. Existing projection-free methods eitherachieve suboptimal regret bounds or have high per-iteration computationalcosts. To fill this gap, two efficient projection-free online methods calledORGFW and MORGFW are proposed for solving stochastic and adversarial OCOproblems, respectively. By employing a recursive gradient estimator, ourmethods achieve optimal regret bounds (up to a logarithmic factor) whilepossessing low per-iteration computational costs. Experimental resultsdemonstrate the efficiency of the proposed methods compared tostate-of-the-arts.",https://arxiv.org/abs/1910.09396
['Title:An Unbiased Risk Estimator for Learning with Augmented Classes'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  In this paper, we study the problem of learning with augmented classes (LAC),where new classes that do not appear in the training dataset might emerge inthe testing phase. The mixture of known classes and new classes in the testingdistribution makes the LAC problem quite challenging. Our discovery is that byexploiting cheap and vast unlabeled data, the testing distribution can beestimated in the training stage, which paves us a way to develop algorithmswith nice statistical properties. Specifically, we propose an unbiased riskestimator over the testing distribution for the LAC problem, and furtherdevelop an efficient algorithm to perform the empirical risk minimization. Bothasymptotic and non-asymptotic analyses are provided as theoretical guarantees.The efficacy of the proposed algorithm is also confirmed by experiments.",https://arxiv.org/abs/1910.09388
['Title:Graph Construction from Data using Non Negative Kernel regression (NNK Graphs)'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Data driven graph constructions are often used in various applications,including several machine learning tasks, where the goal is to make predictionsand discover patterns. However, learning an optimal graph from data is still achallenging task. Weighted $K$-nearest neighbor and $\epsilon$-neighborhoodmethods are among the most common graph construction methods, due to theircomputational simplicity but the choice of parameters such as $K$ and$\epsilon$ associated with these methods is often ad hoc and lacks a clearinterpretation. We formulate graph construction as the problem of finding asparse signal approximation in kernel space, identifying key similaritiesbetween methods in signal approximation and existing graph learning methods. Wepropose non-negative kernel regression~(NNK), an improved approach for graphconstruction with interesting geometric and theoretical properties. We showexperimentally the efficiency of NNK graphs, its robustness to choice ofsparsity $K$ and better performance over state of the art graph methods in semisupervised learning tasks on real world data.",https://arxiv.org/abs/1910.09383
['Title:Movienet: A Movie Multilayer Network Model using Visual and Textual Semantic Cues'],Machine Learning ,(Submitted on 18 Oct 2019),"Abstract:  Discovering content and stories in movies is one of the most importantconcepts in multimedia content research studies. Network models have proven tobe an efficient choice for this purpose. When an audience watches a movie, theyusually compare the characters and the relationships between them. For thisreason, most of the models developed so far are based on social networksanalysis. They focus essentially on the characters at play. By analyzingcharacters' interactions, we can obtain a broad picture of the narration'scontent. Other works have proposed to exploit semantic elements such as scenes,dialogues, etc. However, they are always captured from a single facet.Motivated by these limitations, we introduce in this work a multilayer networkmodel to capture the narration of a movie based on its script, its subtitles,and the movie content. After introducing the model and the extraction processfrom the raw data, we perform a comparative analysis of the whole 6-movie cycleof the Star Wars saga. Results demonstrate the effectiveness of the proposedframework for video content representation and analysis.",https://arxiv.org/abs/1910.09368
['Title:Separable Convolutional Eigen-Filters (SCEF): Building Efficient CNNs Using Redundancy Analysis'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  The high model complexity of deep learning algorithms enables remarkablelearning capacity in many application domains. However, a large number oftrainable parameters comes with a high cost. For example, during both thetraining and inference phases, the numerous trainable parameters consume alarge amount of resources, such as CPU/GPU cores, memory and electric power. Inaddition, from a theoretical statistical learning perspective, the highcomplexity of the network can result in a high variance in its generalizationperformance. One way to reduce the complexity of a network without sacrificingits accuracy is to define and identify redundancies in order to remove them. Inthis work, we propose a method to observe and analyze redundancies in theweights of a 2D convolutional (Conv2D) network. Based on the proposed analysis,we construct a new layer called Separable Convolutional Eigen-Filters (SCEF) asan alternative parameterization to Conv2D layers. A SCEF layer can be easilyimplemented using depthwise separable convolution, which are known to becomputationally effective. To verify our hypothesis, experiments are conductedon the CIFAR-10 and ImageNet datasets by replacing the Conv2D layers with SCEFand the results have shown an increased accuracy using about 2/3 of theoriginal parameters and reduce the number of FLOPs to 2/3 of the original net.Implementation-wise, our method is highly modular, easy to use, fast to processand does not require any additional dependencies.",https://arxiv.org/abs/1910.09359
['Title:Making Bayesian Predictive Models Interpretable: A Decision Theoretic Approach'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  A salient approach to interpretable machine learning is to restrict modelingto simple and hence understandable models. In the Bayesian framework, this canbe pursued by restricting the model structure and prior to favor interpretablemodels. Fundamentally, however, interpretability is about users' preferences,not the data generation mechanism: it is more natural to formulateinterpretability as a utility function. In this work, we propose aninterpretability utility, which explicates the trade-off between explanationfidelity and interpretability in the Bayesian framework. The method consists oftwo steps. First, a reference model, possibly a black-box Bayesian predictivemodel compromising no accuracy, is constructed and fitted to the training data.Second, a proxy model from an interpretable model family that best mimics thepredictive behaviour of the reference model is found by optimizing theinterpretability utility function. The approach is model agnostic - neither theinterpretable model nor the reference model are restricted to be from a certainclass of models - and the optimization problem can be solved using standardtools in the chosen model family. Through experiments on real-word data setsusing decision trees as interpretable models and Bayesian additive regressionmodels as reference models, we show that for the same level ofinterpretability, our approach generates more accurate models than the earlieralternative of restricting the prior. We also propose a systematic way tomeasure stabilities of interpretabile models constructed by differentinterpretability approaches and show that our proposed approach generates morestable models.",https://arxiv.org/abs/1910.09358
['Title:Task-Based Learning via Task-Oriented Prediction Network'],Machine Learning ,(Submitted on 17 Oct 2019),"Abstract:  Real-world applications often involve domain-specific and task-basedperformance objectives that are not captured by the standard machine learningmetrics, such as mean squared error, mean absolute error, and cross-entropyloss, but are critical for decision making. A key challenge for directintegration of more meaningful domain and task-based evaluation criteria intoan end-to-end gradient-based training process is the fact that often suchperformance objectives are not necessarily differentiable and may even requireadditional decision-making optimization processing. We propose Task-OrientedPrediction Network (TOPNet), an end-to-end learning scheme that automaticallyintegrates task-based evaluation criteria into the learning process via atask-oriented estimator and directly learns a model with respect to thetask-based goal. A major benefit of the proposed TOPNet learning scheme lies inits capability of automatically integrating non-differentiable evaluationcriteria. This makes it particularly suitable for diversified and customizedtask-based evaluation criteria in real-world prediction tasks. We validate theperformance of TOPNet on two real-world financial prediction tasks, revenuesurprise forecasting and credit risk modeling. The experimental results onmultiple real-world data sets demonstrate that TOPNet significantly outperformsboth traditional modeling with standard losses and modeling with differentiable(relaxed) surrogate losses.",https://arxiv.org/abs/1910.09357
['Title:Supervised Machine Learning based Ensemble Model for Accurate Prediction of Type 2 Diabetes'],Machine Learning ,(Submitted on 18 Oct 2019),"Abstract:  According to the American Diabetes Association(ADA), 30.3 million people inthe United States have diabetes, but only 7.2 million may be undiagnosed andunaware of their condition. Type 2 diabetes is usually diagnosed for mostpatients later on in life whereas the less common Type 1 diabetes is diagnosedearly on in life. People can live healthy and happy lives while living withdiabetes, but early detection produces a better overall outcome on mostpatient's health. Thus, to test the accurate prediction of Type 2 diabetes, weuse the patients' information from an electronic health records company calledPractice Fusion, which has about 10,000 patient records from 2009 to 2012. Thisdata contains individual key biometrics, including age, diastolic and systolicblood pressure, gender, height, and weight. We use this data on popular machinelearning algorithms and for each algorithm, we evaluate the performance ofevery model based on their classification accuracy, precision, sensitivity,specificity/recall, negative predictive value, and F1 score. In our study, wefind that all algorithms other than Naive Bayes suffered from very lowprecision. Hence, we take a step further and incorporate all the algorithmsinto a weighted average or soft voting ensemble model where each algorithm willcount towards a majority vote towards the decision outcome of whether a patienthas diabetes or not. The accuracy of the Ensemble model on Practice Fusion is85\%, by far our ensemble approach is new in this space. We firmly believe thatthe weighted average ensemble model not only performed well in overall metricsbut also helped to recover wrong predictions and aid in accurate prediction ofType 2 diabetes. Our accurate novel model can be used as an alert for thepatients to seek medical evaluation in time.",https://arxiv.org/abs/1910.09356
['Title:Theoretical Investigation of Composite Neural Network'],Machine Learning ,(Submitted on 18 Oct 2019),"Abstract:  A composite neural network is a rooted directed acyclic graph combining a setof pre-trained and non-instantiated neural network models. A pre-trained neuralnetwork model is well-crafted for a specific task and with instantiatedweights. is generally well trained, targeted to approximate a specificfunction. Despite a general belief that a composite neural network may performbetter than a single component, the overall performance characteristics are notclear. In this work, we prove that there exist parameters such that a compositeneural network performs better than any of its pre-trained components with ahigh probability bound.",https://arxiv.org/abs/1910.09351
['Title:Approximate Sampling using an Accelerated Metropolis-Hastings based on Bayesian Optimization and Gaussian Processes'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Markov Chain Monte Carlo (MCMC) methods have a drawback when working with atarget distribution or likelihood function that is computationally expensive toevaluate, specially when working with big data. This paper focuses onMetropolis-Hastings (MH) algorithm for unimodal distributions. Here, anenhanced MH algorithm is proposed that requires less number of expensivefunction evaluations, has shorter burn-in period, and uses a better proposaldistribution. The main innovations include the use of Bayesian optimization toreach the high probability region quickly, emulating the target distributionusing Gaussian processes (GP), and using Laplace approximation of the GP tobuild a proposal distribution that captures the underlying correlation better.The experiments show significant improvement over the regular MH. Statisticalcomparison between the results from two algorithms is presented.",https://arxiv.org/abs/1910.09347
['Title:Gradient Boosted Decision Tree Neural Network'],Machine Learning ,(Submitted on 17 Oct 2019),"Abstract:  In this paper we propose a method to build a neural network that is similarto an ensemble of decision trees. We first illustrate how to convert a learnedensemble of decision trees to a single neural network with one hidden layer andan input transformation. We then relax some properties of this network such asthresholds and activation functions to train an approximately equivalentdecision tree ensemble. The final model, Hammock, is surprisingly simple: afully connected two layers neural network where the input is quantized andone-hot encoded. Experiments on large and small datasets show this simplemethod can achieve performance similar to that of Gradient Boosted DecisionTrees.",https://arxiv.org/abs/1910.09340
['Title:An Alternative Surrogate Loss for PGD-based Adversarial Testing'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Adversarial testing methods based on Projected Gradient Descent (PGD) arewidely used for searching norm-bounded perturbations that cause the inputs ofneural networks to be misclassified. This paper takes a deeper look at thesemethods and explains the effect of different hyperparameters (i.e., optimizer,step size and surrogate loss). We introduce the concept of MultiTargetedtesting, which makes clever use of alternative surrogate losses, and explainwhen and how MultiTargeted is guaranteed to find optimal perturbations.Finally, we demonstrate that MultiTargeted outperforms more sophisticatedmethods and often requires less iterative steps than other variants of PGDfound in the literature. Notably, MultiTargeted ranks first on MadryLab'swhite-box MNIST and CIFAR-10 leaderboards, reducing the accuracy of their MNISTmodel to 88.36% (with $\ell_\infty$ perturbations of $\epsilon = 0.3$) and theaccuracy of their CIFAR-10 model to 44.03% (at $\epsilon = 8/255$).MultiTargeted also ranks first on the TRADES leaderboard reducing the accuracyof their CIFAR-10 model to 53.07% (with $\ell_\infty$ perturbations of$\epsilon = 0.031$).",https://arxiv.org/abs/1910.09338
['Title:Integrals over Gaussians under Linear Domain Constraints'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Integrals of linearly constrained multivariate Gaussian densities are afrequent problem in machine learning and statistics, arising in tasks likegeneralized linear models and Bayesian optimization. Yet they are notoriouslyhard to compute, and to further complicate matters, the numerical values ofsuch integrals may be very small. We present an efficient black-box algorithmthat exploits geometry for the estimation of integrals over a small, truncatedGaussian volume, and to simulate therefrom. Our algorithm uses theHolmes-Diaconis-Ross (HDR) method combined with an analytic version ofelliptical slice sampling (ESS). Adapted to the linear setting, ESS allows forefficient, rejection-free sampling, because intersections of ellipses anddomain boundaries have closed-form solutions. The key idea of HDR is todecompose the integral into easier-to-compute conditional probabilities byusing a sequence of nested domains. Remarkably, it allows for directcomputation of the logarithm of the integral value and thus enables thecomputation of extremely small probability masses. We demonstrate theeffectiveness of our tailored combination of HDR and ESS on high-dimensionalintegrals and on entropy search for Bayesian optimization.",https://arxiv.org/abs/1910.09328
['Title:Recurrent Attentive Neural Process for Sequential Data'],Machine Learning ,(Submitted on 17 Oct 2019),"Abstract:  Neural processes (NPs) learn stochastic processes and predict thedistribution of target output adaptively conditioned on a context set ofobserved input-output pairs. Furthermore, Attentive Neural Process (ANP)improved the prediction accuracy of NPs by incorporating attention mechanismamong contexts and targets. In a number of real-world applications such asrobotics, finance, speech, and biology, it is critical to learn the temporalorder and recurrent structure from sequential data. However, the capability ofNPs capturing these properties is limited due to its permutation invarianceinstinct. In this paper, we proposed the Recurrent Attentive Neural Process(RANP), or alternatively, Attentive Neural Process-RecurrentNeuralNetwork(ANP-RNN), in which the ANP is incorporated into a recurrent neuralnetwork. The proposed model encapsulates both the inductive biases of recurrentneural networks and also the strength of NPs for modelling uncertainty. Wedemonstrate that RANP can effectively model sequential data and outperforms NPsand LSTMs remarkably in a 1D regression toy example as well asautonomous-driving applications.",https://arxiv.org/abs/1910.09323
['Title:Momentum in Reinforcement Learning'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  We adapt the optimization's concept of momentum to reinforcement learning.Seeing the state-action value functions as an analog to the gradients inoptimization, we interpret momentum as an average of consecutive $q$-functions.We derive Momentum Value Iteration (MoVI), a variation of Value Iteration thatincorporates this momentum idea. Our analysis shows that this allows MoVI toaverage errors over successive iterations. We show that the proposed approachcan be readily extended to deep learning. Specifically, we propose a simpleimprovement on DQN based on MoVI, and experiment it on Atari games.",https://arxiv.org/abs/1910.09322
['Title:Pricing Mechanism for Resource Sustainability in Competitive Online Learning Multi-Agent Systems'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  In this paper, we consider the problem of resource congestion control forcompeting online learning agents. On the basis of non-cooperative game as themodel for the interaction between the agents, and the noisy online mirrorascent as the model for rational behavior of the agents, we propose a novelpricing mechanism which gives the agents incentives for sustainable use of theresources. Our mechanism is distributed and resource-centric, in the sense thatit is done by the resources themselves and not by a centralized instance, andthat it is based rather on the congestion state of the resources than thepreferences of the agents. In case that the noise is persistent, and forseveral choices of the intrinsic parameter of the agents, such as theirlearning rate, and of the mechanism parameters, such as the learning rate of -,the progressivity of the price-setters, and the extrinsic price sensitivity ofthe agents, we show that the accumulative violation of the resource constraintsof the resulted iterates is sub-linear w.r.t. the time horizon. Moreover, weprovide numerical simulations to support our theoretical findings.",https://arxiv.org/abs/1910.09314
['Title:Learning Hierarchical Feature Space Using CLAss-specific Subspace Multiple Kernel -- Metric Learning for Classification'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Metric learning for classification has been intensively studied over the lastdecade. The idea is to learn a metric space induced from a normed vector spaceon which data from different classes are well separated. Different measures ofthe separation thus lead to various designs of the objective function in themetric learning model. One classical metric is the Mahalanobis distance, wherea linear transformation matrix is designed and applied on the original datasetto obtain a new subspace equipped with the Euclidean norm. The kernelizedversion has also been developed, followed by Multiple-Kernel learning models.In this paper, we consider metric learning to be the identification of the bestkernel function with respect to a high class separability in the correspondingmetric space. The contribution is twofold: 1) No pairwise computations arerequired as in most metric learning techniques; 2) Better flexibility and lowercomputational complexity is achieved using the CLAss-Specific (Multiple) Kernel- Metric Learning (CLAS(M)K-ML). The proposed techniques can be considered as apreprocessing step to any kernel method or kernel approximation technique. Anextension to a hierarchical learning structure is also proposed to furtherimprove the classification performance, where on each layer, the CLASMK iscomputed based on a selected ""marginal"" subset and feature vectors areconstructed by concatenating the features from all previous layers.",https://arxiv.org/abs/1910.09309
['Title:Approximation capabilities of neural networks on unbounded domains'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  We prove universal approximation theorems of neural networks in$L^{p}(\mathbb{R} \times [0, 1]^n)$, under the conditions that $p \in [2,\infty)$ and that the activiation function belongs to among others a monotonesigmoid, relu, elu, softplus or leaky relu. Our results partially generalizeclassical universal approximation theorems on $[0,1]^n.$",https://arxiv.org/abs/1910.09293
['Title:Dealing with Sparse Rewards in Reinforcement Learning'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Successfully navigating a complex environment to obtain a desired outcome isa difficult task, that up to recently was believed to be capable only byhumans. This perception has been broken down over time, especially with theintroduction of deep reinforcement learning, which has greatly increased thedifficulty of tasks that can be automated. However, for traditionalreinforcement learning agents this requires an environment to be able toprovide frequent extrinsic rewards, which are not known or accessible for manyreal-world environments. This project aims to explore and contrast existingreinforcement learning solutions that circumnavigate the difficulties of anenvironment that provide sparse rewards. Different reinforcement solutions willbe implemented over a several video game environments with varying difficultyand varying frequency of rewards, as to properly investigate the applicabilityof these solutions. This project introduces a novel reinforcement learningsolution, by combining aspects of two existing state of the art sparse rewardsolutions.",https://arxiv.org/abs/1910.09281
['Title:Constructing Artificial Data for Fine-tuning for Low-Resource Biomedical Text Tagging with Applications in PICO Annotation'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Biomedical text tagging systems are plagued by the dearth of labeled trainingdata. There have been recent attempts at using pre-trained encoders to dealwith this issue. Pre-trained encoder provides representation of the input textwhich is then fed to task-specific layers for classification. The entirenetwork is fine-tuned on the labeled data from the target task. Unfortunately,a low-resource biomedical task often has too few labeled instances forsatisfactory fine-tuning. Also, if the label space is large, it contains few orno labeled instances for majority of the labels. Most biomedical taggingsystems treat labels as indexes, ignoring the fact that these labels are oftenconcepts expressed in natural language e.g. `Appearance of lesion on brainimaging'. To address these issues, we propose constructing extra labeledinstances using label-text (i.e. label's name) as input for the correspondinglabel-index (i.e. label's index). In fact, we propose a number of strategiesfor manufacturing multiple artificial labeled instances from a single label.The network is then fine-tuned on a combination of real and these newlyconstructed artificial labeled instances. We evaluate the proposed approach onan important low-resource biomedical task called \textit{PICO annotation},which requires tagging raw text describing clinical trials with labelscorresponding to different aspects of the trial i.e. PICO (Population,Intervention/Control, Outcome) characteristics of the trial. Our empiricalresults show that the proposed method achieves a new state-of-the-artperformance for PICO annotation with very significant improvements overcompetitive baselines.",https://arxiv.org/abs/1910.09255
['Title:Who wants accurate models? Arguing for a different metrics to take classification models seriously'],Machine Learning ,"(Submitted on 21 Oct 2019 (v1), last revised 22 Oct 2019 (this version, v2))","Abstract:  With the increasing availability of AI-based decision support, there is anincreasing need for their certification by both AI manufacturers and notifiedbodies, as well as the pragmatic (real-world) validation of these systems.Therefore, there is the need for meaningful and informative ways to assess theperformance of AI systems in clinical practice. Common metrics (like accuracyscores and areas under the ROC curve) have known problems and they do not takeinto account important information about the preferences of clinicians and theneeds of their specialist practice, like the likelihood and impact of errorsand the complexity of cases. In this paper, we present a new accuracy measure,the H-accuracy (Ha), which we claim is more informative in the medical domain(and others of similar needs) for the elements it encompasses. We also provideproof that the H-accuracy is a generalization of the balanced accuracy andestablish a relation between the H-accuracy and the Net Benefit. Finally, weillustrate an experimentation in two user studies to show the descriptive powerof the Ha score and how complementary and differently informative measures canbe derived from its formulation (a Python script to compute Ha is also madeavailable).",https://arxiv.org/abs/1910.09246
['Title:Recovering Localized Adversarial Attacks'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Deep convolutional neural networks have achieved great successes over recentyears, particularly in the domain of computer vision. They are fast,convenient, and -- thanks to mature frameworks -- relatively easy to implementand deploy. However, their reasoning is hidden inside a black box, in spite ofa number of proposed approaches that try to provide human-understandableexplanations for the predictions of neural networks. It is still a matter ofdebate which of these explainers are best suited for which situations, and howto quantitatively evaluate and compare them. In this contribution, we focus onthe capabilities of explainers for convolutional deep neural networks in anextreme situation: a setting in which humans and networks fundamentallydisagree. Deep neural networks are susceptible to adversarial attacks thatdeliberately modify input samples to mislead a neural network's classification,without affecting how a human observer interprets the input. Our goal with thiscontribution is to evaluate explainers by investigating whether they canidentify adversarially attacked regions of an image. In particular, wequantitatively and qualitatively investigate the capability of three popularexplainers of classifications -- classic salience, guided backpropagation, andLIME -- with respect to their ability to identify regions of attack as theexplanatory regions for the (incorrect) prediction in representative examplesfrom image classification. We find that LIME outperforms the other explainers.",https://arxiv.org/abs/1910.09239
['Title:Aggregated Gradient Langevin Dynamics'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  In this paper, we explore a general Aggregated Gradient Langevin Dynamicsframework (AGLD) for the Markov Chain Monte Carlo (MCMC) sampling. Weinvestigate the nonasymptotic convergence of AGLD with a unified analysis fordifferent data accessing (e.g. random access, cyclic access and randomreshuffle) and snapshot updating strategies, under convex and nonconvexsettings respectively. It is the first time that bounds for I/O friendlystrategies such as cyclic access and random reshuffle have been established inthe MCMC literature. The theoretic results also indicate that methods in AGLDpossess the merits of both the low per-iteration computational complexity andthe short mixture time. Empirical studies demonstrate that our framework allowsto derive novel schemes to generate high-quality samples for large-scaleBayesian posterior learning tasks.",https://arxiv.org/abs/1910.09223
['Title:Regularization Matters in Policy Optimization'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Deep Reinforcement Learning (Deep RL) has been receiving increasingly moreattention thanks to its encouraging performance on a variety of control tasks.Yet, conventional regularization techniques in training neural networks (e.g.,$L_2$ regularization, dropout) have been largely ignored in RL methods,possibly because agents are typically trained and evaluated in the sameenvironment. In this work, we present the first comprehensive study ofregularization techniques with multiple policy optimization algorithms oncontinuous control tasks. Interestingly, we find conventional regularizationtechniques on the policy networks can often bring large improvement on the taskperformance, and the improvement is typically more significant when the task ismore difficult. We also compare with the widely used entropy regularization andfind $L_2$ regularization is generally better. Our findings are furtherconfirmed to be robust against the choice of training hyperparameters. We alsostudy the effects of regularizing different components and find that onlyregularizing the policy network is typically enough. We hope our study providesguidance for future practices in regularizing policy optimization algorithms.",https://arxiv.org/abs/1910.09191
['Title:Mining GOLD Samples for Conditional GANs'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Conditional generative adversarial networks (cGANs) have gained aconsiderable attention in recent years due to its class-wise controllabilityand superior quality for complex generation tasks. We introduce a simple yeteffective approach to improving cGANs by measuring the discrepancy between thedata distribution and the model distribution on given samples. The proposedmeasure, coined the gap of log-densities (GOLD), provides an effectiveself-diagnosis for cGANs while being efficienty computed from thediscriminator. We propose three applications of the GOLD: example re-weighting,rejection sampling, and active learning, which improve the training, inference,and data selection of cGANs, respectively. Our experimental results demonstratethat the proposed methods outperform corresponding baselines for all threeapplications on different image datasets.",https://arxiv.org/abs/1910.09170
['Title:A $ŒΩ$- support vector quantile regression model with automatic accuracy control'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  This paper proposes a novel '$\nu$-support vector quantile regression'($\nu$-SVQR) model for the quantile estimation. It can facilitate the automaticcontrol over accuracy by creating a suitable asymmetric $\epsilon$-insensitivezone according to the variance present in data. The proposed $\nu$-SVQR modeluses the $\nu$ fraction of training data points for the estimation of thequantiles. In the $\nu$-SVQR model, training points asymptotically appear aboveand below of the asymmetric $\epsilon$-insensitive tube in the ratio of$1-\tau$ and $\tau$. Further, there are other interesting properties of theproposed $\nu$-SVQR model, which we have briefly described in this paper. Theseproperties have been empirically verified using the artificial and real worlddataset also.",https://arxiv.org/abs/1910.09168
"[""Title:Implementation of a modified Nesterov's Accelerated quasi-Newton Method on Tensorflow""]",Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Recent studies incorporate Nesterov's accelerated gradient method for theacceleration of gradient based training. The Nesterov's AcceleratedQuasi-Newton (NAQ) method has shown to drastically improve the convergencespeed compared to the conventional quasi-Newton method. This paper implementsNAQ for non-convex optimization on Tensorflow. Two modifications have beenproposed to the original NAQ algorithm to ensure global convergence andeliminate linesearch. The performance of the proposed algorithm - mNAQ isevaluated on standard non-convex function approximation benchmark problems andmicrowave circuit modelling problems. The results show that the improvedalgorithm converges better and faster compared to first order optimizers suchas AdaGrad, RMSProp, Adam, and the second order methods such as thequasi-Newton method.",https://arxiv.org/abs/1910.09158
['Title:A New Framework for Multi-Agent Reinforcement Learning -- Centralized Training and Exploration with Decentralized Execution via Policy Distillation'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Deep reinforcement learning (DRL) is a booming area of artificialintelligence. Many practical applications of DRL naturally involve more thanone collaborative learners, making it important to study DRL in a multi-agentcontext. Previous research showed that effective learning in complexmulti-agent systems demands for highly coordinated environment explorationamong all the participating agents. Many researchers attempted to cope withthis challenge through learning centralized value functions. However, thecommon strategy for every agent to learn their local policies directly oftenfail to nurture strong inter-agent collaboration and can be sample inefficientwhenever agents alter their communication channels. To address these issues, wepropose a new framework known as centralized training and exploration withdecentralized execution via policy distillation. Guided by this framework andthe maximum-entropy learning technique, we will first train agents' policieswith shared global component to foster coordinated and effective learning.Locally executable policies will be derived subsequently from the trainedglobal policies via policy distillation. Experiments show that our newframework and algorithm can achieve significantly better performance and highersample efficiency than a cutting-edge baseline on several multi-agent DRLbenchmarks.",https://arxiv.org/abs/1910.09152
['Title:Perception-Distortion Trade-off with Restricted Boltzmann Machines'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  In this work, we introduce a new procedure for applying Restricted BoltzmannMachines (RBMs) to missing data inference tasks, based on linearization of theeffective energy function governing the distribution of observations. Wecompare the performance of our proposed procedure with those obtained usingexisting reconstruction procedures trained on incomplete data. We place theseperformance comparisons within the context of the perception-distortiontrade-off observed in other data reconstruction tasks, which has, until now,remained unexplored in tasks relying on incomplete training data.",https://arxiv.org/abs/1910.09122
"['Title:Generative Hierarchical Models for Parts, Objects, and Scenes']",Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Compositional structures between parts and objects are inherent in naturalscenes. Modeling such compositional hierarchies via unsupervised learning canbring various benefits such as interpretability and transferability, which areimportant in many downstream tasks. In this paper, we propose the first deeplatent variable model, called RICH, for learning Representation ofInterpretable Compositional Hierarchies. At the core of RICH is a latent scenegraph representation that organizes the entities of a scene into a treestructure according to their compositional relationships. During inference,taking top-down approach, RICH is able to use higher-level representation toguide lower-level decomposition. This avoids the difficult problem of routingbetween parts and objects that is faced by bottom-up approaches. In experimentson images containing multiple objects with different part compositions, wedemonstrate that RICH is able to learn the latent compositional hierarchy andgenerate imaginary scenes.",https://arxiv.org/abs/1910.09119
['Title:Unsupervised Out-of-Distribution Detection with Batch Normalization'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Likelihood from a generative model is a natural statistic for detectingout-of-distribution (OoD) samples. However, generative models have been shownto assign higher likelihood to OoD samples compared to ones from the trainingdistribution, preventing simple threshold-based detection rules. We demonstratethat OoD detection fails even when using more sophisticated statistics based onthe likelihoods of individual samples. To address these issues, we propose anew method that leverages batch normalization. We argue that batchnormalization for generative models challenges the traditional i.i.d. dataassumption and changes the corresponding maximum likelihood objective. Based onthis insight, we propose to exploit in-batch dependencies for OoD detection.Empirical results suggest that this leads to more robust detection forhigh-dimensional images.",https://arxiv.org/abs/1910.09115
['Title:Discovering the Compositional Structure of Vector Representations with Role Learning Networks'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Neural networks (NNs) are able to perform tasks that rely on compositionalstructure even though they lack obvious mechanisms for representing thisstructure. To analyze the internal representations that enable such success, wepropose ROLE, a technique that detects whether these representations implicitlyencode symbolic structure. ROLE learns to approximate the representations of atarget encoder E by learning a symbolic constituent structure and an embeddingof that structure into E's representational vector space. The constituents ofthe approximating symbol structure are defined by structural positions ---roles --- that can be filled by symbols. We show that when E is constructed toexplicitly embed a particular type of structure (string or tree), ROLEsuccessfully extracts the ground-truth roles defining that structure. We thenanalyze a GRU seq2seq network trained to perform a more complex compositionaltask (SCAN), where there is no ground truth role scheme available. For thismodel, ROLE successfully discovers an interpretable symbolic structure that themodel implicitly uses to perform the SCAN task, providing a comprehensiveaccount of the representations that drive the behavior of a frequently-used buthard-to-interpret type of model. We verify the causal importance of thediscovered symbolic structure by showing that, when we systematicallymanipulate hidden embeddings based on this symbolic structure, the model'sresulting output is changed in the way predicted by our analysis. Finally, weuse ROLE to explore whether popular sentence embedding models are capturingcompositional structure and find evidence that they are not; we conclude bydiscussing how insights from ROLE can be used to impart new inductive biases toimprove the compositional abilities of such models.",https://arxiv.org/abs/1910.09113
['Title:Boosting Mapping Functionality of Neural Networks via Latent Feature Generation based on Reversible Learning'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  This paper addresses a boosting method for mapping functionality of neuralnetworks in visual recognition such as image classification and facerecognition. We present reversible learning for generating and learning latentfeatures using the network itself. By generating latent features correspondingto hard samples and applying the generated features in a training stage,reversible learning can improve a mapping functionality without additional dataaugmentation or handling the bias of dataset. We demonstrate an efficiency ofthe proposed method on the MNIST,Cifar-10/100, and Extremely Biased and poorlycategorized dataset (EBPC dataset). The experimental results show that theproposed method can outperform existing state-of-the-art methods in visualrecognition. Extensive analysis shows that our method can efficiently improvethe mapping capability of a network.",https://arxiv.org/abs/1910.09108
['Title:Fast Exact Matrix Completion: A Unifying Optimization Framework'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  We consider the problem of matrix completion of rank $k$ on an $n\times m$matrix. We show that both the general case and the case with side informationcan be formulated as a combinatorical problem of selecting $k$ vectors from $p$column features. We demonstrate that it is equivalent to a separableoptimization problem that is amenable to stochastic gradient descent. We designfastImpute, based on projected stochastic gradient descent, to enable efficientscaling of the algorithm of sizes of $10^5 \times 10^5$. We report experimentson both synthetic and real-world datasets that show fastImpute is competitivein both the accuracy of the matrix recovered and the time needed across allcases. Furthermore, when a high number of entries are missing, fastImpute isover $75\%$ lower in MAPE and $10$x faster than current state-of-the-art matrixcompletion methods in both the case with side information and without.",https://arxiv.org/abs/1910.09092
['Title:A game method for improving the interpretability of convolution neural network'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Real artificial intelligence always has been focused on by many machinelearning researchers, especially in the area of deep learning. However deepneural network is hard to be understood and explained, and sometimes, evenmetaphysics. The reason is, we believe that: the network is essentially aperceptual model. Therefore, we believe that in order to complete complexintelligent activities from simple perception, it is necessary to con-structanother interpretable logical network to form accurate and reasonable responsesand explanations to external things. Researchers like Bolei Zhou and QuanshiZhang have found many explanatory rules for deep feature extraction aimed atthe feature extraction stage of convolution neural network. However, althoughresearchers like Marco Gori have also made great efforts to improve theinterpretability of the fully connected layers of the network, the problem isalso very difficult. This paper firstly analyzes its reason. Then a method ofconstructing logical network based on the fully connected layers and extractinglogical relation between input and output of the layers is proposed. The gameprocess between perceptual learning and logical abstract cognitive learning isimplemented to improve the interpretable performance of deep learning processand deep learning model. The benefits of our approach are illustrated onbenchmark data sets and in real-world experiments.",https://arxiv.org/abs/1910.09090
['Title:Multi-player Multi-Armed Bandits with non-zero rewards on collisions for uncoordinated spectrum access'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  In this paper, we study the uncoordinated spectrum access problem using themulti-player multi-armed bandits framework. We consider a model where there isno central control and the users cannot communicate with each other. Theenvironment may appear differently to different users, \textit{i.e.}, the meanrewards as seen by different users for a particular channel may be different.Additionally, in case of a collision, we allow for the colliding users toreceive non-zero rewards. With this setup, we present a policy that achievesexpected regret of order $O(\log^{2+\delta}{T})$ for some $\delta > 0$.",https://arxiv.org/abs/1910.09089
['Title:Contextual Prediction Difference Analysis'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  The interpretation of black-box models has been investigated in recent years.A number of model-aware saliency methods were proposed to explain individualclassification decisions by creating saliency maps. However, they are notapplicable when the parameters and the gradients of the underlying models areunavailable. Recently, model-agnostic methods have received increasedattention. As one of them, Prediction Difference Analysis (PDA), aprobabilistic sound methodology, was proposed. In this work, we first show thatPDA can suffer from saturated classifiers. The saturation phenomenon ofclassifiers exists widely in current neural network-based classifiers. Tounderstand the decisions of saturated classifiers better, we further proposeContextual PDA, which runs hundreds of times faster than PDA. The experimentsshow the superiority of our method by explaining image classifications of thestate-of-the-art deep convolutional neural networks. We also apply our methodto commercial general vision recognition systems.",https://arxiv.org/abs/1910.09086
['Title:From Importance Sampling to Doubly Robust Policy Gradient'],Machine Learning ,(Submitted on 20 Oct 2019),"Abstract:  We show that policy gradient (PG) and its variance reduction variants can bederived by taking finite difference of function evaluations supplied byestimators from the importance sampling (IS) family for off-policy evaluation(OPE). Starting from the doubly robust (DR) estimator [Jiang and Li, 2016], weprovide a simple derivation of a very general and flexible form of PG, whichsubsumes the state-of-the-art variance reduction technique [Cheng et al., 2019]as its special case and immediately hints at further variance reductionopportunities overlooked by existing literature.",https://arxiv.org/abs/1910.09066
['Title:An Optimal Transport Framework for Zero-Shot Learning'],Machine Learning ,(Submitted on 20 Oct 2019),"Abstract:  We present an optimal transport (OT) framework for generalized zero-shotlearning (GZSL) of imaging data, seeking to distinguish samples for both seenand unseen classes, with the help of auxiliary attributes. The discrepancybetween features and attributes is minimized by solving an optimal transportproblem. {Specifically, we build a conditional generative model to generatefeatures from seen-class attributes, and establish an optimal transport betweenthe distribution of the generated features and that of the real features.} Thegenerative model and the optimal transport are optimized iteratively with anattribute-based regularizer, that further enhances the discriminative power ofthe generated features. A classifier is learned based on the features generatedfor both the seen and unseen classes. In addition to generalized zero-shotlearning, our framework is also applicable to standard and transductive ZSLproblems. Experiments show that our optimal transport-based method outperformsstate-of-the-art methods on several benchmark datasets.",https://arxiv.org/abs/1910.09057
['Title:Amortized Rejection Sampling in Universal Probabilistic Programming'],Machine Learning ,(Submitted on 20 Oct 2019),Abstract:  Existing approaches to amortized inference in probabilistic programs withunbounded loops can produce estimators with infinite variance. An instance ofthis is importance sampling inference in programs that explicitly includerejection sampling as part of the user-programmed generative procedure. In thispaper we develop a new and efficient amortized importance sampling estimator.We prove finite variance of our estimator and empirically demonstrate ourmethod's correctness and efficiency compared to existing alternatives ongenerative programs containing rejection sampling loops and discuss how toimplement our method in a generic probabilistic programming framework.,https://arxiv.org/abs/1910.09056
['Title:Leveraging inductive bias of neural networks for learning without explicit human annotations'],Machine Learning ,(Submitted on 20 Oct 2019),"Abstract:  Classification problems today are typically solved by first collectingexamples along with candidate labels, second obtaining clean labels fromworkers, and third training a large, overparameterized deep neural network onthe clean examples. The second, labeling step is often the most expensive oneas it requires manually going through all examples. In this paper we skip thelabeling step entirely and propose to directly train the deep neural network onthe noisy candidate labels and early stop the training to avoid overfitting.With this procedure we exploit an intriguing property of largeoverparameterized neural networks: While they are capable of perfectly fittingthe noisy data, gradient descent fits clean labels much faster than the noisyones, thus early stopping resembles training on the clean labels. Our resultsshow that early stopping the training of standard deep networks such asResNet-18 on part of the Tiny Images dataset, which does not involve any humanlabeled data, and of which only about half of the labels are correct, gives asignificantly higher test performance than when trained on the clean CIFAR-10training dataset, which is a labeled version of the Tiny Images dataset, forthe same classification problem. In addition, our results show that the noisegenerated through the label collection process is not nearly as adversarial forlearning as the noise generated by randomly flipping labels, which is the noisemost prevalent in works demonstrating noise robustness of neural networks.",https://arxiv.org/abs/1910.09055
['Title:Learning from both experts and data'],Machine Learning ,(Submitted on 20 Oct 2019),"Abstract:  In this work we study the problem of inferring a discrete probabilitydistribution using both expert knowledge and empirical data. This is animportant issue for many applications where the scarcity of data prevents apurely empirical approach. In this context, it is common to rely first on aninitial domain knowledge a priori before proceeding to an online dataacquisition. We are particularly interested in the intermediate regime where wedo not have enough data to do without the initial expert a priori of theexperts, but enough to correct it if necessary. We present here a novel way totackle this issue with a method providing an objective way to choose the weightto be given to experts compared to data. We show, both empirically andtheoretically, that our proposed estimator is always more efficient than thebest of the two models (expert or data) within a constant.",https://arxiv.org/abs/1910.09043
['Title:Landing Probabilities of Random Walks for Seed-Set Expansion in Hypergraphs'],Machine Learning ,(Submitted on 20 Oct 2019),"Abstract:  We describe the first known mean-field study of landing probabilities forrandom walks on hypergraphs. In particular, we examine clique-expansion andtensor methods and evaluate their mean-field characteristics over a class ofrandom hypergraph models for the purpose of seed-set community expansion. Wedescribe parameter regimes in which the two methods outperform each other andpropose a hybrid expansion method that uses partial clique-expansion to reducethe projection distortion and low-complexity tensor methods applied directly onthe partially expanded hypergraphs.",https://arxiv.org/abs/1910.09040
['Title:Differentiable Deep Clustering with Cluster Size Constraints'],Machine Learning ,(Submitted on 20 Oct 2019),"Abstract:  Clustering is a fundamental unsupervised learning approach. Many clusteringalgorithms -- such as $k$-means -- rely on the euclidean distance as asimilarity measure, which is often not the most relevant metric for highdimensional data such as images. Learning a lower-dimensional embedding thatcan better reflect the geometry of the dataset is therefore instrumental forperformance. We propose a new approach for this task where the embedding isperformed by a differentiable model such as a deep neural network. By rewritingthe $k$-means clustering algorithm as an optimal transport task, and adding anentropic regularization, we derive a fully differentiable loss function thatcan be minimized with respect to both the embedding parameters and the clusterparameters via stochastic gradient descent. We show that this new formulationgeneralizes a recently proposed state-of-the-art method based on soft-$k$-meansby adding constraints on the cluster sizes. Empirical evaluations on imageclassification benchmarks suggest that compared to state-of-the-art methods,our optimal transport-based approach provide better unsupervised accuracy anddoes not require a pre-training phase.",https://arxiv.org/abs/1910.09036
['Title:Boosting Network Weight Separability via Feed-Backward Reconstruction'],Machine Learning ,(Submitted on 20 Oct 2019),"Abstract:  This paper proposes a new evaluation metric and boosting method for weightseparability in neural network design. In contrast to general visualrecognition methods designed to encourage both intra-class compactness andinter-class separability of latent features, we focus on estimating linearindependence of column vectors in weight matrix and improving the separabilityof weight vectors. To this end, we propose an evaluation metric for weightseparability based on semi-orthogonality of a matrix and Frobenius distance,and the feed-backward reconstruction loss which explicitly encourages weightseparability between the column vectors in the weight matrix. The experimentalresults on image classification and face recognition demonstrate that theweight separability boosting via minimization of feed-backward reconstructionloss can improve the visual recognition performance, hence universally boostingthe performance on various visual recognition tasks.",https://arxiv.org/abs/1910.09024
['Title:Diverse Behavior Is What Game AI Needs: Generating Varied Human-Like Playing Styles Using Evolutionary Multi-Objective Deep Reinforcement Learning'],Machine Learning ,(Submitted on 20 Oct 2019),"Abstract:  Designing artificial intelligence for games (Game AI) has been longrecognized as a notoriously challenging task in the game industry, as it mainlyrelies on manual design, requiring plenty of domain knowledge. Morefrustratingly, even spending a lot of effort, a satisfying Game AI is stillhard to achieve by manual design due to the almost infinite search space. Therecent success of deep reinforcement learning (DRL) sheds light on advancingautomated game designing, significantly relaxing human competitive intelligentsupport. However, existing DRL algorithms mostly focus on training a Game AI towin the game rather than the way it wins (style). To bridge the gap, weintroduce EMO-DRL, an end-to-end game design framework, leveraging evolutionaryalgorithm, DRL and multi-objective optimization (MOO) to perform intelligentand automatic game design. Firstly, EMO-DRL proposes style-oriented learning tobypass manual reward shaping in DRL and directly learns a Game AI with anexpected style in an end-to-end fashion. On this basis, the prioritizedmulti-objective optimization is introduced to achieve more diverse, nature andhuman-like Game AI. Large-scale evaluations on an Atari game and a commercialmassively multiplayer online game are conducted. The results demonstrate thatEMO-DRL, compared to existing algorithms, achieve better game designs in anintelligent and automatic way.",https://arxiv.org/abs/1910.09022
['Title:Mitigating Overfitting in Supervised Classification from Two Unlabeled Datasets: A Consistent Risk Correction Approach'],Machine Learning ,(Submitted on 20 Oct 2019),"Abstract:  From two unlabeled (U) datasets with different class priors, we can train abinary classifier by empirical risk minimization, which is called UUclassification. It is promising since UU methods are compatible with any neuralnetwork (NN) architecture and optimizer as if it is standard supervisedclassification. In this paper, however, we find that UU methods may suffersevere overfitting, and there is a high co-occurrence between the overfittingand the negative empirical risk regardless of datasets, NN architectures, andoptimizers. Hence, to mitigate the overfitting problem of UU methods, wepropose to keep two parts of the empirical risk (i.e., false positive and falsenegative) non-negative by wrapping them in a family of correction functions. Wetheoretically show that the corrected risk estimator is still asymptoticallyunbiased and consistent; furthermore we establish an estimation error bound forthe corrected risk minimizer. Experiments with feedforward/residual NNs onstandard benchmarks demonstrate that our proposed correction can successfullymitigate the overfitting of UU methods and significantly improve theclassification accuracy.",https://arxiv.org/abs/1910.08974
['Title:Image Difficulty Curriculum for Generative Adversarial Networks (CuGAN)'],Machine Learning ,(Submitted on 20 Oct 2019),"Abstract:  Despite the significant advances in recent years, Generative AdversarialNetworks (GANs) are still notoriously hard to train. In this paper, we proposethree novel curriculum learning strategies for training GANs. All strategiesare first based on ranking the training images by their difficulty scores,which are estimated by a state-of-the-art image difficulty predictor. Our firststrategy is to divide images into gradually more difficult batches. Our secondstrategy introduces a novel curriculum loss function for the discriminator thattakes into account the difficulty scores of the real images. Our third strategyis based on sampling from an evolving distribution, which favors the easierimages during the initial training stages and gradually converges to a uniformdistribution, in which samples are equally likely, regardless of difficulty. Wecompare our curriculum learning strategies with the classic training procedureon two tasks: image generation and image translation. Our experiments indicatethat all strategies provide faster convergence and superior results. Forexample, our best curriculum learning strategy applied on spectrally normalizedGANs (SNGANs) fooled human annotators in thinking that generated CIFAR-likeimages are real in 25.0% of the presented cases, while the SNGANs trained usingthe classic procedure fooled the annotators in only 18.4% cases. Similarly, inimage translation, the human annotators preferred the images produced by theCycle-consistent GAN (CycleGAN) trained using curriculum learning in 40.5%cases and those produced by CycleGAN based on classic training in only 19.8%cases, $39.7\%$ cases being labeled as ties.",https://arxiv.org/abs/1910.08967
['Title:Learning GANs and Ensembles Using Discrepancy'],Machine Learning ,(Submitted on 20 Oct 2019),"Abstract:  Generative adversarial networks (GANs) generate data based on minimizing adivergence between two distributions. The choice of that divergence istherefore critical. We argue that the divergence must take into account thehypothesis set and the loss function used in a subsequent learning task, wherethe data generated by a GAN serves for training. Taking that structuralinformation into account is also important to derive generalization guarantees.Thus, we propose to use the discrepancy measure, which was originallyintroduced for the closely related problem of domain adaptation and whichprecisely takes into account the hypothesis set and the loss function. We showthat discrepancy admits favorable properties for training GANs and proveexplicit generalization guarantees. We present efficient algorithms usingdiscrepancy for two tasks: training a GAN directly, namely DGAN, and mixingpreviously trained generative models, namely EDGAN. Our experiments on toyexamples and several benchmark datasets show that DGAN is competitive withother GANs and that EDGAN outperforms existing GAN ensembles, such as AdaGAN.",https://arxiv.org/abs/1910.08965
['Title:Towards Further Understanding of Sparse Filtering via Information Bottleneck'],Machine Learning ,(Submitted on 20 Oct 2019),"Abstract:  In this paper we examine a formalization of feature distribution learning(FDL) in information-theoretic terms relying on the analytical approach and onthe tools already used in the study of the information bottleneck (IB). It hasbeen conjectured that the behavior of FDL algorithms could be expressed as anoptimization problem over two information-theoretic quantities: the mutualinformation of the data with the learned representations and the entropy of thelearned distribution. In particular, such a formulation was offered in order toexplain the success of the most prominent FDL algorithm, sparse filtering (SF).This conjecture was, however, left unproven. In this work, we aim at providingpreliminary empirical support to this conjecture by performing experimentsreminiscent of the work done on deep neural networks in the context of the IBresearch. Specifically, we borrow the idea of using information planes toanalyze the behavior of the SF algorithm and gain insights on its dynamics. Aconfirmation of the conjecture about the dynamics of FDL may provide solidground to develop information-theoretic tools to assess the quality of thelearning process in FDL, and it may be extended to other unsupervised learningalgorithms.",https://arxiv.org/abs/1910.08964
['Title:Online Bagging for Anytime Transfer Learning'],Machine Learning ,(Submitted on 20 Oct 2019),"Abstract:  Transfer learning techniques have been widely used in the reality that it isdifficult to obtain sufficient labeled data in the target domain, but a largeamount of auxiliary data can be obtained in the relevant source domain. Butmost of the existing methods are based on offline data. In practicalapplications, it is often necessary to face online learning problems in whichthe data samples are achieved sequentially. In this paper, We are committed toapplying the ensemble approach to solving the problem of online transferlearning so that it can be used in anytime setting. More specifically, wepropose a novel online transfer learning framework, which applies the idea ofonline bagging methods to anytime transfer learning problems, and constructsstrong classifiers through online iterations of the usefulness of multiple weakclassifiers. Further, our algorithm also provides two extension schemes toreduce the impact of negative transfer. Experiments on three real data setsshow that the effectiveness of our proposed algorithms.",https://arxiv.org/abs/1910.08945
['Title:Policy Learning for Malaria Control'],Machine Learning ,(Submitted on 20 Oct 2019),"Abstract:  Sequential decision making is a typical problem in reinforcement learningwith plenty of algorithms to solve it. However, only a few of them can workeffectively with a very small number of observations. In this report, weintroduce the progress to learn the policy for Malaria Control as aReinforcement Learning problem in the KDD Cup Challenge 2019 and proposediverse solutions to deal with the limited observations problem. We apply theGenetic Algorithm, Bayesian Optimization, Q-learning with sequence breaking tofind the optimal policy for five years in a row with only 20 episodes/100evaluations. We evaluate those algorithms and compare their performance withRandom Search as a baseline. Among these algorithms, Q-Learning with sequencebreaking has been submitted to the challenge and got ranked 7th in KDD Cup.",https://arxiv.org/abs/1910.08926
['Title:Predicting ice flow using machine learning'],Machine Learning ,(Submitted on 20 Oct 2019),"Abstract:  Though machine learning has achieved notable success in modeling sequentialand spatial data for speech recognition and in computer vision, applications toremote sensing and climate science problems are seldom considered. In thispaper, we demonstrate techniques from unsupervised learning of future videoframe prediction, to increase the accuracy of ice flow tracking inmulti-spectral satellite images. As the volume of cryosphere data increases incoming years, this is an interesting and important opportunity for machinelearning to address a global challenge for climate change, risk management fromfloods, and conserving freshwater resources. Future frame prediction of icemelt and tracking the optical flow of ice dynamics presents modelingdifficulties, due to uncertainties in global temperature increase, changingprecipitation patterns, occlusion from cloud cover, rapid melting and glacierretreat due to black carbon aerosol deposition, from wildfires or human fossilemissions. We show the adversarial learning method helps improve the accuracyof tracking the optical flow of ice dynamics compared to existing methods inclimate science. We present a dataset, IceNet, to encourage machine learningresearch and to help facilitate further applications in the areas ofcryospheric science and climate change.",https://arxiv.org/abs/1910.08922
['Title:Leveraging Hierarchical Representations for Preserving Privacy and Utility in Text'],Machine Learning ,(Submitted on 20 Oct 2019),"Abstract:  Guaranteeing a certain level of user privacy in an arbitrary piece of text isa challenging issue. However, with this challenge comes the potential ofunlocking access to vast data stores for training machine learning models andsupporting data driven decisions. We address this problem through the lens ofdx-privacy, a generalization of Differential Privacy to non Hamming distancemetrics. In this work, we explore word representations in Hyperbolic space as ameans of preserving privacy in text. We provide a proof satisfying dx-privacy,then we define a probability distribution in Hyperbolic space and describe away to sample from it in high dimensions. Privacy is provided by perturbingvector representations of words in high dimensional Hyperbolic space to obtaina semantic generalization. We conduct a series of experiments to demonstratethe tradeoff between privacy and utility. Our privacy experiments illustrateprotections against an authorship attribution algorithm while our utilityexperiments highlight the minimal impact of our perturbations on severaldownstream machine learning models. Compared to the Euclidean baseline, weobserve > 20x greater guarantees on expected privacy against comparable worstcase statistics.",https://arxiv.org/abs/1910.08917
['Title:Sparse-Dense Subspace Clustering'],Machine Learning ,(Submitted on 20 Oct 2019),"Abstract:  Subspace clustering refers to the problem of clustering high-dimensional datainto a union of low-dimensional subspaces. Current subspace clusteringapproaches are usually based on a two-stage framework. In the first stage, anaffinity matrix is generated from data. In the second one, spectral clusteringis applied on the affinity matrix. However, the affinity matrix produced bytwo-stage methods cannot fully reveal the similarity between data points fromthe same subspace (intra-subspace similarity), resulting in inaccurateclustering. Besides, most approaches fail to solve large-scale clusteringproblems due to poor efficiency. In this paper, we first propose a new scalablesparse method called Iterative Maximum Correlation (IMC) to learn the affinitymatrix from data. Then we develop Piecewise Correlation Estimation (PCE) todensify the intra-subspace similarity produced by IMC. Finally we extend ourwork into a Sparse-Dense Subspace Clustering (SDSC) framework with a densestage to optimize the affinity matrix for two-stage methods. We show that IMCis efficient when clustering large-scale data, and PCE ensures betterperformance for IMC. We show the universality of our SDSC framework as well.Experiments on several data sets demonstrate the effectiveness of ourapproaches. Moreover, we are the first one to apply densification on affinitymatrix before spectral clustering, and SDSC constitutes the first attempt tobuild a universal three-stage subspace clustering framework.",https://arxiv.org/abs/1910.08909
['Title:Self-Adaptive Network Pruning'],Machine Learning ,(Submitted on 20 Oct 2019),"Abstract:  Deep convolutional neural networks have been proved successful on a widerange of tasks, yet they are still hindered by their large computation cost inmany industrial scenarios. In this paper, we propose to reduce such cost forCNNs through a self-adaptive network pruning method (SANP). Our methodintroduces a general Saliency-and-Pruning Module (SPM) for each convolutionallayer, which learns to predict saliency scores and applies pruning for eachchannel. Given a total computation budget, SANP adaptively determines thepruning strategy with respect to each layer and each sample, such that theaverage computation cost meets the budget. This design allows SANP to be moreefficient in computation, as well as more robust to datasets and backbones.Extensive experiments on 2 datasets and 3 backbones show that SANP surpassesstate-of-the-art methods in both classification accuracy and pruning rate.",https://arxiv.org/abs/1910.08906
['Title:Privacy- and Utility-Preserving Textual Analysis via Calibrated Multivariate Perturbations'],Machine Learning ,(Submitted on 20 Oct 2019),Abstract:  Accurately learning from user data while providing quantifiable privacyguarantees provides an opportunity to build better ML models while maintaininguser trust. This paper presents a formal approach to carrying out privacypreserving text perturbation using the notion of dx-privacy designed to achievegeo-indistinguishability in location data. Our approach applies carefullycalibrated noise to vector representation of words in a high dimension space asdefined by word embedding models. We present a privacy proof that satisfiesdx-privacy where the privacy parameter epsilon provides guarantees with respectto a distance metric defined by the word embedding space. We demonstrate howepsilon can be selected by analyzing plausible deniability statistics backed upby large scale analysis on GloVe and fastText embeddings. We conduct privacyaudit experiments against 2 baseline models and utility experiments on 3datasets to demonstrate the tradeoff between privacy and utility for varyingvalues of epsilon on different task types. Our results demonstrate practicalutility (< 2% utility loss for training binary classifiers) while providingbetter privacy guarantees than baseline models.,https://arxiv.org/abs/1910.08902
['Title:Identification of Interaction Clusters Using a Semi-supervised Hierarchical Clustering Method'],Machine Learning ,(Submitted on 20 Oct 2019),"Abstract:  Motivation: Identifying interaction clusters of large gene regulatorynetworks (GRNs) is critical for its further investigation, while this task isvery challenging, attributed to data noise in experiment data, large scale ofGRNs, and inconsistency between gene expression profiles and function modules,etc. It is promising to semi-supervise this process by prior information, butshortage of prior information sometimes make it very challenging. Meanwhile, itis also annoying, and sometimes impossible to discovery gold standard forevaluation of clustering results.\\ Results: With assistance of an onlineenrichment tool, this research proposes a semi-supervised hierarchicalclustering method via deconvolved correlation matrix~(SHC-DC) to discoverinteraction clusters of large-scale GRNs. Three benchmark networks including a\emph{Ecoli} network and two \emph{Yeast} networks are employed to testsemi-supervision scheme of the proposed method. Then, SHC-DC is utilized tocluster genes in sleep study. Results demonstrates it can find interactionmodules that are generally enriched in various signal pathways. Besides thesignificant influence on blood level of interleukins, impact of sleep onimportant pathways mediated by them is also validated by the discoveredinteraction modules.",https://arxiv.org/abs/1910.08864
['Title:LSTM-Assisted Evolutionary Self-Expressive Subspace Clustering'],Machine Learning ,(Submitted on 19 Oct 2019),"Abstract:  Massive volumes of high-dimensional data that evolves over time iscontinuously collected by contemporary information processing systems, whichbrings up the problem of organizing this data into clusters, i.e. achieve thepurpose of dimensional deduction, and meanwhile learning its temporal evolutionpatterns. In this paper, a framework for evolutionary subspace clustering,referred to as LSTM-ESCM, is introduced, which aims at clustering a set ofevolving high-dimensional data points that lie in a union of low-dimensionalevolving subspaces. In order to obtain the parsimonious data representation ateach time step, we propose to exploit the so-called self-expressive trait ofthe data at each time point. At the same time, LSTM networks are implemented toextract the inherited temporal patterns behind data in an overall time frame.An efficient algorithm has been proposed based on MATLAB. Next, experiments arecarried out on real-world datasets to demonstrate the effectiveness of ourproposed approach. And the results show that the suggested algorithmdramatically outperforms other known similar approaches in terms of both runtime and accuracy.",https://arxiv.org/abs/1910.08862
['Title:Machine Learning for AC Optimal Power Flow'],Machine Learning ,(Submitted on 19 Oct 2019),"Abstract:  We explore machine learning methods for AC Optimal Powerflow (ACOPF) - thetask of optimizing power generation in a transmission network according whilerespecting physical and engineering constraints. We present two formulations ofACOPF as a machine learning problem: 1) an end-to-end prediction task where wedirectly predict the optimal generator settings, and 2) a constraint predictiontask where we predict the set of active constraints in the optimal solution. Wevalidate these approaches on two benchmark grids.",https://arxiv.org/abs/1910.08842
['Title:Dictionary Learning with Almost Sure Error Constraints'],Machine Learning ,(Submitted on 19 Oct 2019),"Abstract:  A dictionary is a database of standard vectors, so that other vectors /signals are expressed as linear combinations of dictionary vectors, and thetask of learning a dictionary for a given data is to find a good dictionary sothat the representation of data points has desirable features. Dictionarylearning and the related matrix factorization methods have gained significantprominence recently due to their applications in Wide variety of fields likemachine learning, signal processing, statistics etc. In this article we studythe dictionary learning problem for achieving desirable features in therepresentation of a given data with almost sure recovery constraints. We imposethe constraint that every sample is reconstructed properly to within apredefined threshold. This problem formulation is more challenging than theconventional dictionary learning, which is done by minimizing a regularisedcost function. We make use of the duality results for linear inverse problemsto obtain an equivalent reformulation in the form of a convex-concave min-maxproblem. The resulting min-max problem is then solved using gradientdescent-ascent like algorithms.",https://arxiv.org/abs/1910.08828
['Title:A Structured Prediction Approach for Generalization in Cooperative Multi-Agent Reinforcement Learning'],Machine Learning ,(Submitted on 19 Oct 2019),"Abstract:  Effective coordination is crucial to solve multi-agent collaborative (MAC)problems. While centralized reinforcement learning methods can optimally solvesmall MAC instances, they do not scale to large problems and they fail togeneralize to scenarios different from those seen during training. In thispaper, we consider MAC problems with some intrinsic notion of locality (e.g.,geographic proximity) such that interactions between agents and tasks arelocally limited. By leveraging this property, we introduce a novel structuredprediction approach to assign agents to tasks. At each step, the assignment isobtained by solving a centralized optimization problem (the inferenceprocedure) whose objective function is parameterized by a learned scoringmodel. We propose different combinations of inference procedures and scoringmodels able to represent coordination patterns of increasing complexity. Theresulting assignment policy can be efficiently learned on small probleminstances and readily reused in problems with more agents and tasks (i.e.,zero-shot generalization). We report experimental results on a toy search andrescue problem and on several target selection scenarios in StarCraft: BroodWar, in which our model significantly outperforms strong rule-based baselineson instances with 5 times more agents and tasks than those seen duringtraining.",https://arxiv.org/abs/1910.08809
['Title:On Adaptivity in Information-constrained Online Learning'],Machine Learning ,(Submitted on 19 Oct 2019),"Abstract:  We study how to adapt to smoothly-varying (`easy') environments in well-knownonline learning problems where acquiring information is expensive. For theproblem of label efficient prediction, which is a budgeted version ofprediction with expert advice, we present an online algorithm whose regretdepends optimally on the number of labels allowed and $Q^*$ (the quadraticvariation of the losses of the best action in hindsight), along with aparameter-free counterpart whose regret depends optimally on $Q$ (the quadraticvariation of the losses of all the actions). These quantities can besignificantly smaller than $T$ (the total time horizon), yielding animprovement over existing, variation-independent results for the problem. Wethen extend our analysis to handle label efficient prediction with banditfeedback, i.e., label efficient bandits. Our work builds upon the framework ofoptimistic online mirror descent, and leverages second order corrections alongwith a carefully designed hybrid regularizer that encodes the constrainedinformation structure of the problem. We then consider revealing action-partialmonitoring games -- a version of label efficient prediction with additiveinformation costs, which in general are known to lie in the \textit{hard} classof games having minimax regret of order $T^{\frac{2}{3}}$. We provide astrategy with an $\mathcal{O}((Q^*T)^{\frac{1}{3}})$ bound for revealing actiongames, along with an one with a $\mathcal{O}((QT)^{\frac{1}{3}})$ bound for thefull class of hard partial monitoring games, both being strict improvementsover current bounds.",https://arxiv.org/abs/1910.08805
['Title:Reverse Experience Replay'],Machine Learning ,(Submitted on 19 Oct 2019),"Abstract:  This paper describes an improvement in Deep Q-learning called ReverseExperience Replay (also RER) that solves the problem of sparse rewards andhelps to deal with reward maximizing tasks by sampling transitions successivelyin reverse order. On tasks with enough experience for training and enoughExperience Replay memory capacity, Deep Q-learning Network with ReverseExperience Replay shows competitive results against both Double DQN, with astandard Experience Replay, and vanilla DQN. Also, RER achieves significantlyincreased results in tasks with a lack of experience and Replay memorycapacity.",https://arxiv.org/abs/1910.08780
['Title:Neural Spectrum Alignment'],Machine Learning ,(Submitted on 19 Oct 2019),"Abstract:  Expressiveness of deep models was recently addressed via the connectionbetween neural networks (NNs) and kernel learning, where first-order dynamicsof NN during a gradient-descent (GD) optimization were related to gradientsimilarity kernel, also known as Neural Tangent Kernel (NTK). In the majorityof works this kernel is considered to be time-invariant, with its propertiesbeing defined entirely by NN architecture and independent of the learning taskat hand. In contrast, in this paper we empirically explore these propertiesalong the optimization and show that in practical applications the NN kernelchanges in a very dramatic and meaningful way, with its top eigenfunctionsaligning toward the target function learned by NN. Moreover, these topeigenfunctions serve sort of basis functions for NN output - a functionrepresented by NN is spanned almost completely by them for the entireoptimization process. Further, since the learning along top eigenfunctions istypically fast, their alignment with the target function improves the overalloptimization performance. In addition, we study how the neural spectrum isaffected by learning rate, typically done by practitioners, showing varioustrends in the kernel behavior. We argue that the presented phenomena may leadto a more complete theoretical understanding behind NN learning.",https://arxiv.org/abs/1910.08720
['Title:Explainable AI: Deep Reinforcement Learning Agents for Residential Demand Side Cost Savings in Smart Grids'],Machine Learning ,(Submitted on 19 Oct 2019),"Abstract:  Motivated by the recent advancements in deep Reinforcement Learning (RL), wedevelop an RL agent to manage the operation of storage devices in a householddesigned to maximize demand-side cost savings. The proposed technique isdata-driven, and the RL agent learns from scratch on how to efficiently use theenergy storage device under variable tariff-structures Contracting the conceptof the ""black box"" where the techniques learned by the agent are ignored. Weexplain the learning progression of the RL agent, and the strategies it followsbased on the capacity of the storage device.",https://arxiv.org/abs/1910.08719
['Title:Introduction to Coresets: Accurate Coresets'],Machine Learning ,(Submitted on 19 Oct 2019),"Abstract:  A coreset (or core-set) of an input set is its small summation, such thatsolving a problem on the coreset as its input, provably yields the same resultas solving the same problem on the original (full) set, for a given family ofproblems (models, classifiers, loss functions). Over the past decade, coresetconstruction algorithms have been suggested for many fundamental problems ine.g. machine/deep learning, computer vision, graphics, databases, andtheoretical computer science. This introductory paper was written followingrequests from (usually non-expert, but also colleagues) regarding the manyinconsistent coreset definitions, lack of available source code, the requireddeep theoretical background from different fields, and the dense papers thatmake it hard for beginners to apply coresets and develop new ones.The paper provides folklore, classic and simple results includingstep-by-step proofs and figures, for the simplest (accurate) coresets of verybasic problems, such as: sum of vectors, minimum enclosing ball, SVD/ PCA andlinear regression. Nevertheless, we did not find most of their constructions inthe literature. Moreover, we expect that putting them together in aretrospective context would help the reader to grasp modern results thatusually extend and generalize these fundamental observations. Experts mightappreciate the unified notation and comparison table that links betweenexisting results.Open source code with example scripts are provided for all the presentedalgorithms, to demonstrate their practical usage, and to support the readerswho are more familiar with programming than math.",https://arxiv.org/abs/1910.08707
['Title:Online Pricing with Offline Data: Phase Transition and Inverse Square Law'],Machine Learning ,(Submitted on 19 Oct 2019),"Abstract:  This paper investigates the impact of pre-existing offline data on onlinelearning, in the context of dynamic pricing. We study a single-product dynamicpricing problem over a selling horizon of $T$ periods. The demand in eachperiod is determined by the price of the product according to a linear demandmodel with unknown parameters. We assume that an incumbent price has beentested for $n$ periods in the offline stage before the start of the sellinghorizon, and the seller has collected $n$ demand observations under theincumbent price from the market. The seller wants to utilize both thepre-existing offline data and the sequential online data to minimize the regretof the online learning process.In the well-separated case where the absolute difference between theincumbent price and the optimal price $\delta$ is lower bounded by a knownconstant, we prove that the best achievable regret is$\tilde{\Theta}\left(\sqrt{T}\wedge (\frac{T}{n}\vee \log T)\right)$, and showthat certain variants of the greedy policy achieve this bound. In the generalcase where $\delta$ is not necessarily lower bounded by a known constant, weprove that the best achievable regret is $\tilde{\Theta}\left(\sqrt{T}\wedge(\frac{T}{n\delta^2} \vee \frac{\log T}{\delta^2})\right)$, and construct alearning algorithm based on the ""optimism in the face of uncertainty""principle, whose regret is optimal up to a logarithm factor. In both cases, ourresults reveal surprising transformations of the optimal regret rate withrespect to the size of offline data, which we refer to as phase transitions. Inaddition, our result demonstrates that the shape of offline data, measured by$\delta$, also has an intrinsic effect on the optimal regret, and we quantifythis effect via the inverse-square law.",https://arxiv.org/abs/1910.08693
['Title:ELSA: A Throughput-Optimized Design of an LSTM Accelerator for Energy-Constrained Devices'],Machine Learning ,(Submitted on 19 Oct 2019),"Abstract:  The next significant step in the evolution and proliferation of artificialintelligence technology will be the integration of neural network (NN) modelswithin embedded and mobile systems. This calls for the design of compact,energy efficient NN models in silicon. In this paper, we present a scalableASIC design of an LSTM accelerator named ELSA, that is suitable forenergy-constrained devices. It includes several architectural innovations toachieve small area and high energy efficiency. To reduce the area and powerconsumption of the overall design, the compute-intensive units of ELSA employapproximate multiplications and still achieve high performance and accuracy.The performance is further improved through efficient synchronization of theelastic pipeline stages to maximize the utilization. The paper also includes aperformance model of ELSA, as a function of the hidden nodes and time steps,permitting its use for the evaluation of any LSTM application. ELSA wasimplemented in RTL and was synthesized and placed and routed in 65nmtechnology. Its functionality is demonstrated for language modeling-a commonapplication of LSTM. ELSA is compared against a baseline implementation of anLSTM accelerator with standard functional units and without any of thearchitectural innovations of ELSA. The paper demonstrates that ELSA can achievesignificant improvements in power, area and energy-efficiency when compared tothe baseline design and several ASIC implementations reported in theliterature, making it suitable for use in embedded systems and real-timeapplications.",https://arxiv.org/abs/1910.08683
['Title:Context-Driven Data Mining through Bias Removal and Data Incompleteness Mitigation'],Machine Learning ,(Submitted on 19 Oct 2019),"Abstract:  The results of data mining endeavors are majorly driven by data quality.Throughout these deployments, serious show-stopper problems are stillunresolved, such as: data collection ambiguities, data imbalance, hidden biasesin data, the lack of domain information, and data incompleteness. This paper isbased on the premise that context can aid in mitigating these issues. In atraditional data science lifecycle, context is not considered. Context-drivenData Science Lifecycle (C-DSL); the main contribution of this paper, isdeveloped to address these challenges. Two case studies (using data-sets fromsports events) are developed to test C-DSL. Results from both case studies areevaluated using common data mining metrics such as: coefficient ofdetermination (R2 value) and confusion matrices. The work presented in thispaper aims to re-define the lifecycle and introduce tangible improvements toits outcomes.",https://arxiv.org/abs/1910.08670
['Title:NASIB: Neural Architecture Search withIn Budget'],Machine Learning ,(Submitted on 19 Oct 2019),"Abstract:  Neural Architecture Search (NAS) represents a class of methods to generatethe optimal neural network architecture and typically iterate over candidatearchitectures till convergence over some particular metric like validationloss. They are constrained by the available computation resources, especiallyin enterprise environments. In this paper, we propose a new approach for NAS,called NASIB, which adapts and attunes to the computation resources (budget)available by varying the exploration vs. exploitation trade-off. We reduce theexpert bias by searching over an augmented search space induced bySuperkernels. The proposed method can provide the architecture search usefulfor different computation resources and different domains beyond imageclassification of natural images where we lack bespoke architecture motifs anddomain expertise. We show, on CIFAR10, that itis possible to search over aspace that comprises of 12x more candidate operations than the traditionalprior art in just 1.5 GPU days, while reaching close to state of the artaccuracy. While our method searches over an exponentially larger search space,it could lead to novel architectures that require lesser domain expertise,compared to the majority of the existing methods.",https://arxiv.org/abs/1910.08665
['Title:Machine Learning Systems for Highly-Distributed and Rapidly-Growing Data'],Machine Learning ,(Submitted on 18 Oct 2019),"Abstract:  The usability and practicality of any machine learning (ML) applications arelargely influenced by two critical but hard-to-attain factors: low latency andlow cost. Unfortunately, achieving low latency and low cost is very challengingwhen ML depends on real-world data that are highly distributed and rapidlygrowing (e.g., data collected by mobile phones and video cameras all over theworld). Such real-world data pose many challenges in communication andcomputation. For example, when training data are distributed across datacenters that span multiple continents, communication among data centers caneasily overwhelm the limited wide-area network bandwidth, leading toprohibitively high latency and high cost.In this dissertation, we demonstrate that the latency and cost of ML onhighly-distributed and rapidly-growing data can be improved by one to twoorders of magnitude by designing ML systems that exploit the characteristics ofML algorithms, ML model structures, and ML training/serving data. We supportthis thesis statement with three contributions. First, we design a system thatprovides both low-latency and low-cost ML serving (inferencing) overlarge-scale and continuously-growing datasets, such as videos. Second, we builda system that makes ML training over geo-distributed datasets as fast astraining within a single data center. Third, we present a first detailed studyand a system-level solution on a fundamental and largely overlooked problem: MLtraining over non-IID (i.e., not independent and identically distributed) datapartitions (e.g., facial images collected by cameras varies according to thedemographics of each camera's location).",https://arxiv.org/abs/1910.08663
['Title:PyTorchPipe: a framework for rapid prototyping of pipelines combining language and vision'],Machine Learning ,(Submitted on 18 Oct 2019),"Abstract:  Access to vast amounts of data along with affordable computational powerstimulated the reincarnation of neural networks. The progress could not beachieved without adequate software tools, lowering the entry bar for the nextgenerations of researchers and developers. The paper introduces PyTorchPipe(PTP), a framework built on top of PyTorch. Answering the recent needs andtrends in machine learning, PTP facilitates building and training of complex,multi-modal models combining language and vision (but is not limited to thosetwo modalities). At its core, PTP employs a component-oriented approach andrelies on the concept of a pipeline, defined as a directed acyclic graph ofloosely coupled components. A user defines a pipeline using yaml-based (thushuman-readable) configuration files, whereas PTP provides generic workers fortheir loading, training, and testing using all the computational power (CPUsand GPUs) that is available to the user. The paper covers the main concepts ofPyTorchPipe, discusses its key features and briefly presents the currentlyimplemented tasks, models and components.",https://arxiv.org/abs/1910.08654
['Title:Toward Metrics for Differentiating Out-of-Distribution Sets'],Machine Learning ,(Submitted on 18 Oct 2019),"Abstract:  Vanilla CNNs, as uncalibrated classifiers, suffer from classifyingout-of-distribution (OOD) samples nearly as confidently as in-distributionsamples, making them indistinguishable from each other. To tackle thischallenge, some recent works have demonstrated the gains of leveraging readilyaccessible OOD sets for training end-to-end calibrated CNNs. However, acritical question remains unanswered in these works: how to select an OOD set,among the available OOD sets, for training such CNNs that induces highdetection rates on unseen OOD sets? We address this pivotal question throughthe use of Augmented-CNN (A-CNN) involving an explicit rejection option. Wefirst provide a formal definition to precisely differentiate OOD sets for thepurpose of selection. As using this definition incurs a huge computationalcost, we propose novel metrics, as a computationally efficient tool, forcharacterizing OOD sets in order to select the proper one. In a series ofexperiments on several image and audio benchmarks, we show that training anA-CNN with an OOD set identified by our metrics (called A-CNN$^{\star}$) leadsto remarkable detection rate of unseen OOD sets while maintainingin-distribution generalization performance, thus demonstrating the viability ofour metrics for identifying the proper OOD set. Furthermore, we show thatA-CNN$^{\star}$ outperforms state-of-the-art OOD detectors across differentbenchmarks.",https://arxiv.org/abs/1910.08650
['Title:Are Perceptually-Aligned Gradients a General Property of Robust Classifiers?'],Machine Learning ,(Submitted on 18 Oct 2019),"Abstract:  For a standard convolutional neural network, optimizing over the input pixelsto maximize the score of some target class will generally produce agrainy-looking version of the original image. However, researchers havedemonstrated that for adversarially-trained neural networks, this optimizationproduces images that uncannily resemble the target class. In this paper, weshow that these ""perceptually-aligned gradients"" also occur under randomizedsmoothing, an alternative means of constructing adversarially-robustclassifiers. Our finding suggests that perceptually-aligned gradients may be ageneral property of robust classifiers, rather than a specific property ofadversarially-trained neural networks. We hope that our results will inspireresearch aimed at explaining this link between perceptually-aligned gradientsand adversarial robustness.",https://arxiv.org/abs/1910.08640
['Title:OffWorld Gym: open-access physical robotics environment for real-world reinforcement learning benchmark and research'],Machine Learning ,(Submitted on 18 Oct 2019),"Abstract:  Success stories of applied machine learning can be traced back to thedatasets and environments that were put forward as challenges for thecommunity. The challenge that the community sets as a benchmark is usually thechallenge that the community eventually solves. The ultimate challenge ofreinforcement learning research is to train real agents to operate in the realenvironment, but until now there has not been a common real-world RL benchmark.In this work, we present a prototype real-world environment from OffWorld Gym-- a collection of real-world environments for reinforcement learning inrobotics with free public remote access. Close integration into existingecosystem allows the community to start using OffWorld Gym without any priorexperience in robotics and takes away the burden of managing a physicalrobotics system, abstracting it under a familiar API. We introduce a navigationtask, where a robot has to reach a visual beacon on an uneven terrain usingonly the camera input and provide baseline results in both the real environmentand the simulated replica. To start training, visit this https URL.",https://arxiv.org/abs/1910.08639
['Title:The TCGA Meta-Dataset Clinical Benchmark'],Machine Learning ,(Submitted on 18 Oct 2019),"Abstract:  Machine learning is bringing a paradigm shift to healthcare by changing theprocess of disease diagnosis and prognosis in clinics and hospitals. Thisdevelopment equips doctors and medical staff with tools to evaluate theirhypotheses and hence make more precise decisions. Although most currentresearch in the literature seeks to develop techniques and methods forpredicting one particular clinical outcome, this approach is far from thereality of clinical decision making in which you have to consider severalfactors simultaneously. In addition, it is difficult to follow the recentprogress concretely as there is a lack of consistency in benchmark datasets andtask definitions in the field of Genomics. To address the aforementionedissues, we provide a clinical Meta-Dataset derived from the publicly availabledata hub called The Cancer Genome Atlas Program (TCGA) that contains 174 tasks.We believe those tasks could be good proxy tasks to develop methods which canwork on a few samples of gene expression data. Also, learning to predictmultiple clinical variables using gene-expression data is an important task dueto the variety of phenotypes in clinical problems and lack of samples for someof the rare variables. The defined tasks cover a wide range of clinicalproblems including predicting tumor tissue site, white cell count, histologicaltype, family history of cancer, gender, and many others which we explain laterin the paper. Each task represents an independent dataset. We use regressionand neural network baselines for all the tasks using only 150 samples andcompare their performance.",https://arxiv.org/abs/1910.08636
['Title:Tree-based Intelligent Intrusion Detection System in Internet of Vehicles'],Machine Learning ,(Submitted on 18 Oct 2019),"Abstract:  The use of autonomous vehicles (AVs) is a promising technology in IntelligentTransportation Systems (ITSs) to improve safety and driving efficiency.Vehicle-to-everything (V2X) technology enables communication among vehicles andother infrastructures. However, AVs and Internet of Vehicles (IoV) arevulnerable to different types of cyber-attacks such as denial of service,spoofing, and sniffing attacks. In this paper, an intelligent intrusiondetection system (IDS) is proposed based on tree-structure machine learningmodels. The results from the implementation of the proposed intrusion detectionsystem on standard data sets indicate that the system has the ability toidentify various cyber-attacks in the AV networks. Furthermore, the proposedensemble learning and feature selection approaches enable the proposed systemto achieve high detection rate and low computational cost simultaneously.",https://arxiv.org/abs/1910.08635
['Title:A Saddle-Point Dynamical System Approach for Robust Deep Learning'],Machine Learning ,(Submitted on 18 Oct 2019),"Abstract:  We propose a novel discrete-time dynamical system-based framework forachieving adversarial robustness in machine learning models. Our algorithm isoriginated from robust optimization, which aims to find the saddle point of amin-max optimization problem in the presence of uncertainties. The robustlearning problem is formulated as a robust optimization problem, and weintroduce a discrete-time algorithm based on a saddle-point dynamical system(SDS) to solve this problem. Under the assumptions that the cost function isconvex and uncertainties enter concavely in the robust learning problem, weanalytically show that using a diminishing step-size, the stochastic version ofour algorithm, SSDS converges asymptotically to the robust optimal solution.The algorithm is deployed for the training of adversarially robust deep neuralnetworks. Although such training involves highly non-convex non-concave robustoptimization problems, empirical results show that the algorithm can achievesignificant robustness for deep learning. We compare the performance of ourSSDS model to other state-of-the-art robust models, e.g., trained using theprojected gradient descent (PGD)-training approach. From the empirical results,we find that SSDS training is computationally inexpensive (compared toPGD-training) while achieving comparable performances. SSDS training also helpsrobust models to maintain a relatively high level of performance for clean dataas well as under black-box attacks.",https://arxiv.org/abs/1910.08623
['Title:Identifying the Most Explainable Classifier'],Machine Learning ,(Submitted on 18 Oct 2019),"Abstract:  We introduce the notion of pointwise coverage to measure the explainabilityproperties of machine learning classifiers. An explanation for a prediction isa definably simple region of the feature space sharing the same label as theprediction, and the coverage of an explanation measures its size orgeneralizability. With this notion of explanation, we investigate whether ornot there is a natural characterization of the most explainable classifier.According with our intuitions, we prove that the binary linear classifier isuniquely the most explainable classifier up to negligible sets.",https://arxiv.org/abs/1910.08595
['Title:Decoupling feature propagation from the design of graph auto-encoders'],Machine Learning ,(Submitted on 18 Oct 2019),"Abstract:  We present two instances, L-GAE and L-VGAE, of the variational graphauto-encoding family (VGAE) based on separating feature propagation operationsfrom graph convolution layers typically found in graph learning methods to asingle linear matrix computation made prior to input in standard auto-encoderarchitectures. This decoupling enables the independent and fixed design of theauto-encoder without requiring additional GCN layers for every desired increasein the size of a node's local receptive field. Fixing the auto-encoder enablesa fairer assessment on the size of a nodes receptive field in buildingrepresentations. Furthermore a by-product of fixing the auto-encoder designoften results in substantially smaller networks than their VGAE counterpartsespecially as we increase the number of feature propagations. A comparativedownstream evaluation on link prediction tasks show comparable state of the artperformance to similar VGAE arrangements despite considerable simplification.We also show the simple application of our methodology to more challengingrepresentation learning scenarios such as spatio-temporal graph representationlearning.",https://arxiv.org/abs/1910.08589
['Title:Towards Quantifying Intrinsic Generalization of Deep ReLU Networks'],Machine Learning ,(Submitted on 18 Oct 2019),"Abstract:  Understanding the underlying mechanisms that enable the empirical successesof deep neural networks is essential for further improving their performanceand explaining such networks. Towards this goal, a specific question is how toexplain the ""surprising"" behavior of the same over-parametrized deep neuralnetworks that can generalize well on real datasets and at the same time""memorize"" training samples when the labels are randomized. In this paper, wedemonstrate that deep ReLU networks generalize from training samples to newpoints via piece-wise linear interpolation. We provide a quantified analysis onthe generalization ability of a deep ReLU network: Given a fixed point$\mathbf{x}$ and a fixed direction in the input space $\mathcal{S}$, there isalways a segment such that any point on the segment will be classified the sameas the fixed point $\mathbf{x}$. We call this segment the $generalization \interval$. We show that the generalization intervals of a ReLU network behavesimilarly along pairwise directions between samples of the same label in bothreal and random cases on the MNIST and CIFAR-10 datasets. This result suggeststhat the same interpolation mechanism is used in both cases. Additionally, fordatasets using real labels, such networks provide a good approximation of theunderlying manifold in the data, where the changes are much smaller alongtangent directions than along normal directions. On the other hand, however,for datasets with random labels, generalization intervals along mid-lines oftriangles with the same label are much smaller than those on the datasets withreal labels, suggesting different behaviors along other directions. Oursystematic experiments demonstrate for the first time that such deep neuralnetworks generalize through the same interpolation and explain the differencesbetween their performance on datasets with real and random labels.",https://arxiv.org/abs/1910.08581
['Title:Semi-supervised Learning using Adversarial Training with Good and Bad Samples'],Machine Learning ,(Submitted on 18 Oct 2019),"Abstract:  In this work, we investigate semi-supervised learning (SSL) for imageclassification using adversarial training. Previous results have illustratedthat generative adversarial networks (GANs) can be used for multiple purposes.Triple-GAN, which aims to jointly optimize model components by incorporatingthree players, generates suitable image-label pairs to compensate for the lackof labeled data in SSL with improved benchmark performance. Conversely, Bad (orcomplementary) GAN, optimizes generation to produce complementary data-labelpairs and force a classifier's decision boundary to lie between data manifolds.Although it generally outperforms Triple-GAN, Bad GAN is highly sensitive tothe amount of labeled data used for training. Unifying these two approaches, wepresent unified-GAN (UGAN), a novel framework that enables a classifier tosimultaneously learn from both good and bad samples through adversarialtraining. We perform extensive experiments on various datasets and demonstratethat UGAN: 1) achieves state-of-the-art performance among other deep generativemodels, and 2) is robust to variations in the amount of labeled data used fortraining.",https://arxiv.org/abs/1910.08540
['Title:Building Dynamic Knowledge Graphs from Text-based Games'],Machine Learning ,"(Submitted on 21 Oct 2019 (v1), last revised 22 Oct 2019 (this version, v2))","Abstract:  We are interested in learning how to update Knowledge Graphs (KG) from text.In this preliminary work, we propose a novel Sequence-to-Sequence (Seq2Seq)architecture to generate elementary KG operations. Furthermore, we introduce anew dataset for KG extraction built upon text-based game transitions (over 300kdata points). We conduct experiments and discuss the results.",https://arxiv.org/abs/1910.09532
['Title:Adaptive gradient descent without descent'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  We present a strikingly simple proof that two rules are sufficient toautomate gradient descent: 1) don't increase the stepsize too fast and 2) don'toverstep the local curvature. No need for functional values, no line search, noinformation about the function except for the gradients. By following theserules, you get a method adaptive to the local geometry, with convergenceguarantees depending only on smoothness in a neighborhood of a solution. Giventhat the problem is convex, our method will converge even if the globalsmoothness constant is infinity. As an illustration, it can minimize arbitrarycontinuously twice-differentiable convex function. We examine its performanceon a range of convex and nonconvex problems, including matrix factorization andtraining of ResNet-18.",https://arxiv.org/abs/1910.09529
['Title:Comparative Study between Adversarial Networks and Classical Techniques for Speech Enhancement'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Speech enhancement is a crucial task for several applications. Among the mostexplored techniques are the Wiener filter and the LogMMSE, but approachesexploring deep learning adapted to this task, such as SEGAN, have presentedrelevant results. This study compared the performance of the mentionedtechniques in 85 noise conditions regarding quality, intelligibility, anddistortion; and concluded that classical techniques continue to exhibitsuperior results for most scenarios, but, in severe noise scenarios, SEGANperformed better and with lower variance.",https://arxiv.org/abs/1910.09522
['Title:Multi-agent Hierarchical Reinforcement Learning with Dynamic Termination'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  In a multi-agent system, an agent's optimal policy will typically depend onthe policies chosen by others. Therefore, a key issue in multi-agent systemsresearch is that of predicting the behaviours of others, and respondingpromptly to changes in such behaviours. One obvious possibility is for eachagent to broadcast their current intention, for example, the currently executedoption in a hierarchical reinforcement learning framework. However, thisapproach results in inflexibility of agents if options have an extendedduration and are dynamic. While adjusting the executed option at each stepimproves flexibility from a single-agent perspective, frequent changes inoptions can induce inconsistency between an agent's actual behaviour and itsbroadcast intention. In order to balance flexibility and predictability, wepropose a dynamic termination Bellman equation that allows the agents toflexibly terminate their options. We evaluate our model empirically on a set ofmulti-agent pursuit and taxi tasks, and show that our agents learn to adaptflexibly across scenarios that require different termination behaviours.",https://arxiv.org/abs/1910.09508
['Title:Multi-Resolution Weak Supervision for Sequential Data'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Since manually labeling training data is slow and expensive, recentindustrial and scientific research efforts have turned to weaker or noisierforms of supervision sources. However, existing weak supervision approachesfail to model multi-resolution sources for sequential data, like video, thatcan assign labels to individual elements or collections of elements in asequence. A key challenge in weak supervision is estimating the unknownaccuracies and correlations of these sources without using labeled data.Multi-resolution sources exacerbate this challenge due to complex correlationsand sample complexity that scales in the length of the sequence. We proposeDugong, the first framework to model multi-resolution weak supervision sourceswith complex correlations to assign probabilistic labels to training data.Theoretically, we prove that Dugong, under mild conditions, can uniquelyrecover the unobserved accuracy and correlation parameters and use parametersharing to improve sample complexity. Our method assigns clinician-validatedlabels to population-scale biomedical video repositories, helping outperformtraditional supervision by 36.8 F1 points and addressing a key use case wheremachine learning has been severely limited by the lack of expert labeled data.On average, Dugong improves over traditional supervision by 16.0 F1 points andexisting weak supervision approaches by 24.2 F1 points across several video andsensor classification tasks.",https://arxiv.org/abs/1910.09505
['Title:Generalized tensor regression with covariates on multiple modes'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  We consider the problem of tensor-response regression given covariates onmultiple modes. Such data problems arise frequently in applications such asneuroimaging, network analysis, and spatial-temporal modeling. We propose a newfamily of tensor response regression models that incorporate covariates, andestablish the theoretical accuracy guarantees. Unlike earlier methods, ourestimation allows high-dimensionality in both the tensor response and thecovariate matrices on multiple modes. An efficient alternating updatingalgorithm is further developed. Our proposal handles a broad range of datatypes, including continuous, count, and binary observations. Through simulationand applications to two real datasets, we demonstrate the outperformance of ourapproach over the state-of-art.",https://arxiv.org/abs/1910.09499
['Title:Policy Optimization for $\\mathcal{H}_2$ Linear Control with $\\mathcal{H}_\\infty$ Robustness Guarantee: Implicit Regularization and Global Convergence'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Policy optimization (PO) is a key ingredient for reinforcement learning (RL).For control design, certain constraints are usually enforced on the policies tooptimize, accounting for either the stability, robustness, or safety concernson the system. Hence, PO is by nature a constrained (nonconvex) optimization inmost cases, whose global convergence is challenging to analyze in general. Moreimportantly, some constraints that are safety-critical, e.g., the$\mathcal{H}_\infty$-norm constraint that guarantees the system robustness, aredifficult to enforce as the PO methods proceed. Recently, policy gradientmethods have been shown to converge to the global optimum of linear quadraticregulator (LQR), a classical optimal control problem, withoutregularizing/projecting the control iterates onto the stabilizing set (Fazel etal., 2018), its (implicit) feasible set. This striking result is built upon thecoercive property of the cost, ensuring that the iterates remain feasible asthe cost decreases. In this paper, we study the convergence theory of PO for$\mathcal{H}_2$ linear control with $\mathcal{H}_\infty$-norm robustnessguarantee. One significant new feature of this problem is the lack ofcoercivity, i.e., the cost may have finite value around the feasible setboundary, breaking the existing analyses for LQR. Interestingly, we show thattwo PO methods enjoy the implicit regularization property, i.e., the iteratespreserve the $\mathcal{H}_\infty$ robustness constraint as if they areregularized by the algorithms. Furthermore, convergence to the globally optimalpolicies with globally sublinear and locally (super-)linear rates are providedunder certain conditions, despite the nonconvexity of the problem. To the bestof our knowledge, our work offers the first results on the implicitregularization property and global convergence of PO methods forrobust/risk-sensitive control.",https://arxiv.org/abs/1910.09496
['Title:Sampling random graph homomorphisms and applications to network data analysis'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  A graph homomorphism is a map between two graphs that preserves adjacencyrelations. We consider the problem of sampling a random graph homomorphism froma graph $F$ into a large network $\mathcal{G}$. When $\mathcal{G}$ is thecomplete graph with $q$ nodes, this becomes the well-known problem of samplinguniform $q$-colorings of $F$. We propose two complementary MCMC algorithms forsampling a random graph homomorphisms and establish bounds on their mixingtimes and concentration of their time averages. Based on our samplingalgorithms, we propose a novel framework for network data analysis thatcircumvents some of the drawbacks in methods based on independent andneigborhood sampling. Various time averages of the MCMC trajectory give usreal-, function-, and network-valued computable observables, includingwell-known ones such as homomorphism density and average clusteringcoefficient. One of the main observable we propose is called the conditionalhomomorphism density profile, which reveals hierarchical structure of thenetwork. Furthermore, we show that these network observables are stable withrespect to a suitably renormalized cut distance between networks. We alsoprovide various examples and simulations demonstrating our framework throughsynthetic and real-world networks. For instance, we apply our framework toanalyze Word Adjacency Networks of a 45 novels data set and propose anauthorship attribution scheme using motif sampling and conditional homomorphismdensity profiles.",https://arxiv.org/abs/1910.09483
['Title:Toward automatic comparison of visualization techniques: Application to graph visualization'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Many end-user evaluations of data visualization techniques have been runduring the last decades. Their results are cornerstones to build efficientvisualization systems. However, designing an evaluation is always complex andtime-consuming and may end in a lack of statistical evidence. The raising ofmodern efficient computer vision techniques may help visualization researchersto adjust their evaluation hypothesis and thus reduces the risk of failure. Inthis paper, we present a methodology that uses such computer vision techniquesto automatically compare the efficiency of several visualization techniques.The basis of our methodology is to generate a set of images for each comparedvisualization technique from a common dataset and to train machine learningmodels (one for each set and visualization technique) to solve a given task.Our assumption is that the performance of each model allows to compare theefficiencies of the corresponding visualization techniques; as current machinelearning models are not capable enough to reflect human capabilities, includingtheir imperfections, such results should be interpreted with caution. However,we argue that using machine learning-based evaluation as a pre-process ofstandard user evaluations should help researchers to perform a more exhaustivestudy of the design space and thus should improve the final user evaluation byproviding better test cases. To show that our methodology can reproduce, up toa certain level, results of user evaluations, we applied it to compare twomainstream graph visualization techniques: node-link (NL) and adjacency-matrix(MD) diagrams. We partially reproduced a user evaluation from Ghoniem et al.using two well-known deep convolutional neural networks as machinelearning-based systems. Our results show up that Ghoniem et al. results can bereproduced automatically at a larger scale with our system.",https://arxiv.org/abs/1910.09477
['Title:A Logic-Based Framework Leveraging Neural Networks for Studying the Evolution of Neurological Disorders'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Deductive formalisms have been strongly developed in recent years; amongthem, Answer Set Programming (ASP) gained some momentum, and has been latelyfruitfully employed in many real-world scenarios. Nonetheless, in spite of alarge number of success stories in relevant application areas, and even inindustrial contexts, deductive reasoning cannot be considered the ultimate,comprehensive solution to AI; indeed, in several contexts, other approachesresult to be more useful. Typical Bioinformatics tasks, for instanceclassification, are currently carried out mostly by Machine Learning (ML) basedsolutions. In this paper, we focus on the relatively new problem of analyzingthe evolution of neurological disorders. In this context, ML approaches alreadydemonstrated to be a viable solution for classification tasks; here, we showhow ASP can play a relevant role in the brain evolution simulation task. Inparticular, we propose a general and extensible framework to support physiciansand researchers at understanding the complex mechanisms underlying neurologicaldisorders. The framework relies on a combined use of ML and ASP, and is generalenough to be applied in several other application scenarios, which are outlinedin the paper.",https://arxiv.org/abs/1910.09472
['Title:Modelling Generalized Forces with Reinforcement Learning for Sim-to-Real Transfer'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Learning robotic control policies in the real world gives rise to challengesin data efficiency, safety, and controlling the initial condition of thesystem. On the other hand, simulations are a useful alternative as they providean abundant source of data without the restrictions of the real world.Unfortunately, simulations often fail to accurately model complex real-worldphenomena. Traditional system identification techniques are limited inexpressiveness by the analytical model parameters, and usually are notsufficient to capture such phenomena. In this paper we propose a generalframework for improving the analytical model by optimizing state dependentgeneralized forces. State dependent generalized forces are expressive enough tomodel constraints in the equations of motion, while maintaining a clearphysical meaning and intuition. We use reinforcement learning to efficientlyoptimize the mapping from states to generalized forces over a discountedinfinite horizon. We show that using only minutes of real world data improvesthe sim-to-real control policy transfer. We demonstrate the feasibility of ourapproach by validating it on a nonprehensile manipulation task on the Sawyerrobot.",https://arxiv.org/abs/1910.09471
['Title:Object landmark discovery through unsupervised adaptation'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  This paper proposes a method to ease the unsupervised learning of objectlandmark detectors. Similarly to previous methods, our approach is fullyunsupervised in a sense that it does not require or make any use of annotatedlandmarks for the target object category. Contrary to previous works, we dohowever assume that a landmark detector, which has already learned a structuredrepresentation for a given object category in a fully supervised manner, isavailable. Under this setting, our main idea boils down to adapting the givenpre-trained network to the target object categories in a fully unsupervisedmanner. To this end, our method uses the pre-trained network as a core whichremains frozen and does not get updated during training, and learns, in anunsupervised manner, only a projection matrix to perform the adaptation to thetarget categories. By building upon an existing structured representationlearned in a supervised manner, the optimization problem solved by our methodis much more constrained with significantly less parameters to learn whichseems to be important for the case of unsupervised learning. We show that ourmethod surpasses fully unsupervised techniques trained from scratch as well asa strong baseline based on fine-tuning, and produces state-of-the-art resultson several datasets. Code can be found atthis https URL .",https://arxiv.org/abs/1910.09469
['Title:Improving Vehicle Re-Identification using CNN Latent Spaces: Metrics Comparison and Track-to-track Extension'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  This paper addresses the problem of vehicle re-identification using distancecomparison of images in CNN latent spaces. First, we study the impact of thedistance metrics, comparing performances obtained with different metrics: theminimal Euclidean distance (MED), the minimal cosine distance (MCD), and theresidue of the sparse coding reconstruction (RSCR). These metrics are appliedusing features extracted through five different CNN architectures, namelyResNet18, AlexNet, VGG16, InceptionV3 and DenseNet201. We use the specificvehicle re-identification dataset VeRI to fine-tune these CNNs and evaluateresults.In overall, independently from the CNN used, MCD outperforms MED, commonlyused in the literature. Secondly, the state-of-the-art image-to-track process(I2TP) is extended to a track-to-track process (T2TP) without usingcomplementary metadata. Metrics are extended to measure distance betweentracks, enabling the evaluation of T2TP and comparison with I2TP using the sameCNN models. Results show that T2TP outperforms I2TP for MCD and RSCR. T2TPcombining DenseNet201 and MCD-based metrics exhibits the best performances,outperforming the state-of-the-art I2TP models that use complementary metadata.Finally, our experiments highlight two main results: i) the importance of themetric choice for vehicle re-identification, and ii) T2TP improves theperformances compared to I2TP, especially when coupled with MCD-based metrics.",https://arxiv.org/abs/1910.09458
['Title:Towards better healthcare: What could and should be automated?'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  While artificial intelligence (AI) and other automation technologies mightlead to enormous progress in healthcare, they may also have undesiredconsequences for people working in the field. In this interdisciplinary study,we capture empirical evidence of not only what healthcare work could beautomated, but also what should be automated. We quantitatively investigatethese research questions by utilizing probabilistic machine learning modelstrained on thousands of ratings, provided by both healthcare practitioners andautomation experts. Based on our findings, we present an analytical tool(Automatability-Desirability Matrix) to support policymakers and organizationalleaders in developing practical strategies on how to harness the positive powerof automation technologies, while accompanying change and empoweringstakeholders in a participatory fashion.",https://arxiv.org/abs/1910.09444
['Title:Data assimilation in Agent-based models using creation and annihilation operators'],Machine Learning ,(Submitted on 8 Oct 2019),"Abstract:  Agent-based models are a powerful tool for studying the behaviour of complexsystems that can be described in terms of multiple, interacting ``agents''.However, because of their inherently discrete and often highly non-linearnature, it is very difficult to reason about the relationship between the stateof the model, on the one hand, and our observations of the real world on theother. In this paper we consider agents that have a discrete set of statesthat, at any instant, act with a probability that may depend on the environmentor the state of other agents. Given this, we show how the mathematicalapparatus of quantum field theory can be used to reason probabilistically aboutthe state and dynamics the model, and describe an algorithm to update ourbelief in the state of the model in the light of new, real-world observations.Using a simple predator-prey model on a 2-dimensional spatial grid as anexample, we demonstrate the assimilation of incomplete, noisy observations andshow that this leads to an increase in the mutual information between theactual state of the observed system and the posterior distribution given theobservations, when compared to a null model.",https://arxiv.org/abs/1910.09442
['Title:Towards a Reinforcement Learning Environment Toolbox for Intelligent Electric Motor Control'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Electric motors are used in many applications and their efficiency isstrongly dependent on their control. Among others, PI approaches or modelpredictive control methods are well-known in the scientific literature andindustrial practice. A novel approach is to use reinforcement learning (RL) tohave an agent learn electric drive control from scratch merely by interactingwith a suitable control environment. RL achieved remarkable results withsuper-human performance in many games (e.g. Atari classics or Go) and alsobecomes more popular in control tasks like cartpole or swinging pendulumbenchmarks. In this work, the open-source Python package gym-electric-motor(GEM) is developed for ease of training of RL-agents for electric motorcontrol. Furthermore, this package can be used to compare the trained agentswith other state-of-the-art control approaches. It is based on the OpenAI Gymframework that provides a widely used interface for the evaluation ofRL-agents. The initial package version covers different DC motor variants andthe prevalent permanent magnet synchronous motor as well as different powerelectronic converters and a mechanical load model. Due to the modular setup ofthe proposed toolbox, additional motor, load, and power electronic devices canbe easily extended in the future. Furthermore, different secondary effects likecontroller interlocking time or noise are considered. An intelligent controllerexample based on the deep deterministic policy gradient algorithm whichcontrols a series DC motor is presented and compared to a cascadedPI-controller as a baseline for future research. Fellow researchers areencouraged to use the framework in their RL investigations or to contribute tothe functional scope (e.g. further motor types) of the package.",https://arxiv.org/abs/1910.09434
['Title:KuroNet: Pre-Modern Japanese Kuzushiji Character Recognition with Deep Learning'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Kuzushiji, a cursive writing style, had been used in Japan for over athousand years starting from the 8th century. Over 3 millions books on adiverse array of topics, such as literature, science, mathematics and evencooking are preserved. However, following a change to the Japanese writingsystem in 1900, Kuzushiji has not been included in regular school curricula.Therefore, most Japanese natives nowadays cannot read books written or printedjust 150 years ago. Museums and libraries have invested a great deal of effortinto creating digital copies of these historical documents as a safeguardagainst fires, earthquakes and tsunamis. The result has been datasets withhundreds of millions of photographs of historical documents which can only beread by a small number of specially trained experts. Thus there has been agreat deal of interest in using Machine Learning to automatically recognizethese historical texts and transcribe them into modern Japanese characters.Nevertheless, several challenges in Kuzushiji recognition have made theperformance of existing systems extremely poor. To tackle these challenges, wepropose KuroNet, a new end-to-end model which jointly recognizes an entire pageof text by using a residual U-Net architecture which predicts the location andidentity of all characters given a page of text (without any pre-processing).This allows the model to handle long range context, large vocabularies, andnon-standardized character layouts. We demonstrate that our system is able tosuccessfully recognize a large fraction of pre-modern Japanese documents, butalso explore areas where our system is limited and suggest directions forfuture work.",https://arxiv.org/abs/1910.09433
['Title:Adversarial Skill Networks: Unsupervised Robot Skill Learning from Video'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Key challenges for the deployment of reinforcement learning (RL) agents inthe real world are the discovery, representation and reuse of skills in theabsence of a reward function. To this end, we propose a novel approach to learna task-agnostic skill embedding space from unlabeled multi-view videos. Ourmethod learns a general skill embedding independently from the task context byusing an adversarial loss. We combine a metric learning loss, which utilizestemporal video coherence to learn a state representation, with an entropyregularized adversarial skill-transfer loss. The metric learning loss learns adisentangled representation by attracting simultaneous viewpoints of the sameobservations and repelling visually similar frames from temporal neighbors. Theadversarial skill-transfer loss enhances re-usability of learned skillembeddings over multiple task domains. We show that the learned embeddingenables training of continuous control policies to solve novel tasks thatrequire the interpolation of previously seen skills. Our extensive evaluationwith both simulation and real world data demonstrates the effectiveness of ourmethod in learning transferable skills from unlabeled interaction videos andcomposing them for new tasks.",https://arxiv.org/abs/1910.09430
['Title:A Survey and Taxonomy of Adversarial Neural Networks for Text-to-Image Synthesis'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Text-to-image synthesis refers to computational methods which translate humanwritten textual descriptions, in the form of keywords or sentences, into imageswith similar semantic meaning to the text. In earlier research, image synthesisrelied mainly on word to image correlation analysis combined with supervisedmethods to find best alignment of the visual content matching to the text.Recent progress in deep learning (DL) has brought a new set of unsuperviseddeep learning methods, particularly deep generative models which are able togenerate realistic visual images using suitably trained neural network models.In this paper, we review the most recent development in the text-to-imagesynthesis research domain. Our survey first introduces image synthesis and itschallenges, and then reviews key concepts such as generative adversarialnetworks (GANs) and deep convolutional encoder-decoder neural networks (DCNN).After that, we propose a taxonomy to summarize GAN based text-to-imagesynthesis into four major categories: Semantic Enhancement GANs, ResolutionEnhancement GANs, Diversity Enhancement GANS, and Motion Enhancement GANs. Weelaborate the main objective of each group, and further review typical GANarchitectures in each group. The taxonomy and the review outline the techniquesand the evolution of different approaches, and eventually provide a clearroadmap to summarize the list of contemporaneous solutions that utilize GANsand DCNNs to generate enthralling results in categories such as human faces,birds, flowers, room interiors, object reconstruction from edge maps (games)etc. The survey will conclude with a comparison of the proposed solutions,challenges that remain unresolved, and future developments in the text-to-imagesynthesis domain.",https://arxiv.org/abs/1910.09399
['Title:Generalised learning of time-series: Ornstein-Uhlenbeck processes'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  In machine learning, statistics, econometrics and statistical physics,$k$-fold cross-validation (CV) is used as a standard approach in quantifyingthe generalization performance of a statistical model. Applying this approachdirectly to time series models is avoided by practitioners due to intrinsicnature of serial correlations in the ordered data due to implications likeabsurdity of using future data to predict past and non-stationarity issues. Inthis work, we propose a technique called {\it reconstructive cross validation}($rCV$) that avoids all these issues enabling generalized learning intime-series as a meta-algorithm. In $rCV$, data points in the test fold,randomly selected points from the time series, are first removed. Then, asecondary time series model or a technique is used in reconstructing theremoved points from the test fold, i.e., imputation or smoothing. Thereafter,the primary model is build using new dataset coming from the secondary model ora technique. The performance of the primary model on the test set by computingthe deviations from the originally removed and out-of-sample (OSS) data areevaluated simultaneously. This amounts to reconstruction and prediction errors.By this procedure serial correlations and data order is retained and $k$-foldcross-validation is reached generically. If reconstruction model uses atechnique whereby the existing data points retained exactly, such as Gaussianprocess regression, the reconstruction itself will not result in informationloss from non-reconstructed portion of the original data points. We haveapplied $rCV$ to estimate the general performance of the model build onsimulated Ornstein-Uhlenbeck process. We have shown an approach to build atime-series learning curves utilizing $rCV$.",https://arxiv.org/abs/1910.09394
['Title:Clotho: An Audio Captioning Dataset'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Audio captioning is the novel task of general audio content description usingfree text. It is an intermodal translation task (not speech-to-text), where asystem accepts as an input an audio signal and outputs the textual description(i.e. the caption) of that signal. In this paper we present Clotho, a datasetfor audio captioning consisting of 4981 audio samples of 15 to 30 secondsduration and 24 905 captions of eight to 20 words length, and a baseline methodto provide initial results. Clotho is built with focus on audio content andcaption diversity, and the splits of the data are not hampering the training orevaluation of methods. All sounds are from the Freesound platform, and captionsare crowdsourced using Amazon Mechanical Turk and annotators from Englishspeaking countries. Unique words, named entities, and speech transcription areremoved with post-processing. Clotho is freely available online(this https URL).",https://arxiv.org/abs/1910.09387
['Title:Variational Integrator Networks for Physically Meaningful Embeddings'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Learning workable representations of dynamical systems is becoming anincreasingly important problem in a number of application areas. By leveragingrecent work connecting deep neural networks to systems of differentialequations, we propose variational integrator networks, a class of neuralnetwork architectures designed to ensure faithful representations of thedynamics under study. This class of network architectures facilitates accuratelong-term prediction, interpretability, and data-efficient learning, whilestill remaining highly flexible and capable of modeling complex behavior. Wedemonstrate that they can accurately learn dynamical systems from both noisyobservations in phase space and from image pixels within which the unknowndynamics are embedded.",https://arxiv.org/abs/1910.09349
['Title:A Causal Perspective to Unbiased Conversion Rate Estimation on Data Missing Not at Random'],Machine Learning ,(Submitted on 16 Oct 2019),"Abstract:  In modern e-commerce and advertising recommender systems, ongoing researchworks attempt to optimize conversion rate (CVR) estimation, and increase thegross merchandise volume. Even though the state-of-the-art CVR estimators adoptdeep learning methods, their model performances are still subject to sampleselection bias and data sparsity issues. Conversion labels of exposed items intraining dataset are typically missing not at random due to selection bias.Empirically, data sparsity issue causes the performance degradation of modelwith large parameter space. In this paper, we proposed two causal estimatorscombined with multi-task learning, and aim to solve sample selection bias (SSB)and data sparsity (DS) issues in conversion rate estimation. The proposedestimators adjust for the MNAR mechanism as if they are trained on a ""dodataset"" where users are forced to click on all exposed items. We evaluate thecausal estimators with billion data samples. Experiment results demonstratethat the proposed CVR estimators outperform other state-of-the-art CVRestimators. In addition, empirical study shows that our methods arecost-effective with large scale dataset.",https://arxiv.org/abs/1910.09337
['Title:Multi-dimensional Features for Prediction with Tweets'],Machine Learning ,(Submitted on 15 Oct 2019),"Abstract:  With the rise of opioid abuse in the US, there has been a growth ofoverlapping hotspots for overdose-related and HIV-related deaths inSpringfield, Boston, Fall River, New Bedford, and parts of Cape Cod. With alarge part of population, including rural communities, active on social media,it is crucial that we leverage the predictive power of social media as apreventive measure. We explore the predictive power of micro-blogging socialmedia website Twitter with respect to HIV new diagnosis rates per county. Whiletrending work in Twitter NLP has focused on primarily text-based features, weshow that multi-dimensional feature construction can significantly improve thepredictive power of topic features alone with respect STI's (sexuallytransmitted infections). By multi-dimensional features, we mean leveraging notonly the topical features (text) of a corpus, but also location-basedinformation (counties) about the tweets in feature-construction. We developnovel text-location-based smoothing features to predict new diagnoses of HIV.",https://arxiv.org/abs/1910.09324
['Title:Using Supervised Learning to Classify Metadata of Research Data by Discipline of Research'],Machine Learning ,(Submitted on 16 Oct 2019),"Abstract:  Automated classification of metadata of research data by their discipline(s)of research can be used in scientometric research, by repository serviceproviders, and in the context of research data aggregation services. Openlyavailable metadata of the DataCite index for research data were used to compilea large training and evaluation set comprised of 609,524 records, which ispublished alongside this paper. These data allow to reproducibly assessclassification approaches, such as tree-based models and neural networks.According to our experiments with 20 base classes (multi-label classification),multi-layer perceptron models perform best with a f1-macro score of 0.760closely followed by Long Short-Term Memory models (f1-macro score of 0.755). Apossible application of the trained classification models is the quantitativeanalysis of trends towards interdisciplinarity of digital scholarly output orthe characterization of growth patterns of research data, stratified bydiscipline of research. Both applications perform at scale with the proposedmodels which are available for re-use.",https://arxiv.org/abs/1910.09313
['Title:MIScnn: A Framework for Medical Image Segmentation with Convolutional Neural Networks and Deep Learning'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  The increased availability and usage of modern medical imaging induced astrong need for automatic medical image segmentation. Still, current imagesegmentation platforms do not provide the required functionalities for plainsetup of medical image segmentation pipelines. Already implemented pipelinesare commonly standalone software, optimized on a specific public data set.Therefore, this paper introduces the open-source Python library MIScnn. The aimof MIScnn is to provide an intuitive API allowing fast building of medicalimage segmentation pipelines including data I/O, preprocessing, dataaugmentation, patch-wise analysis, metrics, a library with state-of-the-artdeep learning models and model utilization like training, prediction, as wellas fully automatic evaluation (e.g. cross-validation). Similarly, highconfigurability and multiple open interfaces allow full pipeline customization.Running a cross-validation with MIScnn on the Kidney Tumor SegmentationChallenge 2019 data set (multi-class semantic segmentation with 300 CT scans)resulted into a powerful predictor based on the standard 3D U-Net model. Withthis experiment, we could show that the MIScnn framework enables researchers torapidly set up a complete medical image segmentation pipeline by using just afew lines of code. The source code for MIScnn is available in the Gitrepository: this https URL.",https://arxiv.org/abs/1910.09308
['Title:Model Order Selection in DoA Scenarios via Cross-Entropy based Machine Learning Techniques'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  In this paper, we present a machine learning approach for estimating thenumber of incident wavefronts in a direction of arrival scenario. In contrastto previous works, a multilayer neural network with a cross-entropy objectiveis trained. Furthermore, we investigate an online training procedure thatallows an adaption of the neural network to imperfections of an antenna arraywithout explicitly calibrating the array manifold. We show via simulations thatthe proposed method outperforms classical model order selection schemes basedon information criteria in terms of accuracy, especially for a small number ofsnapshots and at low signal-to-noise-ratios. Also, the online trainingprocedure enables the neural network to adapt with only a few online trainingsamples, if initialized by offline training on artificial data.",https://arxiv.org/abs/1910.09284
['Title:Robust Online Learning for Resource Allocation -- Beyond Euclidean Projection and Dynamic Fit'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Online-learning literature has focused on designing algorithms that ensuresub-linear growth of the cumulative long-term constraint violations. Thedrawback of this guarantee is that strictly feasible actions may cancel outconstraint violations on other time slots. For this reason, we introduce a newperformance measure called $\hCFit$, whose particular instance is thecumulative positive part of the constraint violations. We propose a class ofnon-causal algorithms for online-decision making, which guarantees, in slowlychanging environments, sub-linear growth of this quantity despite noisyfirst-order feedback. Furthermore, we demonstrate by numerical experiments theperformance gain of our method relative to the state of art.",https://arxiv.org/abs/1910.09282
['Title:Extracting local switching fields in permanent magnets using machine learning'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Microstructural features play an important role for the quality of permanentmagnets. The coercivity is greatly influenced by crystallographic defects,which is well known for MnAl-C, for example. In this work we show a direct linkof microstructural features to the local coercivity of MnAl-C grains by machinelearning. A large number of micromagnetic simulations is performed directlyfrom Electron Backscatter Diffraction (EBSD) data using an automated meshing,modeling and simulation procedure. Decision trees are trained with thesimulation results and predict local switching fields from new microscopic datawithin seconds.",https://arxiv.org/abs/1910.09279
['Title:Semi-Decentralized Coordinated Online Learning for Continuous Games with Coupled Constraints via Augmented Lagrangian'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  We consider a class of concave continuous games in which the correspondingadmissible strategy profile of each player underlies affine couplingconstraints. We propose a novel algorithm that leads the relevant populationdynamic toward Nash equilibrium. This algorithm is based on a mirror ascentalgorithm, which suits with the framework of no-regret online learning, and onthe augmented Lagrangian method. The decentralization aspect of the algorithmcorresponds to the aspects that the iterate of each player requires the localinformation about how she contributes to the coupling constraints and the pricevector broadcasted by a central coordinator. So each player needs not knowabout the population action. Moreover, no specific control by the centralprimary coordinator is required. We give a condition on the step sizes and thedegree of the augmentation of the Lagrangian, such that the proposed algorithmconverges to a generalized Nash equilibrium.",https://arxiv.org/abs/1910.09276
['Title:Multi-Band Multi-Resolution Fully Convolutional Neural Networks for Singing Voice Separation'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Deep neural networks with convolutional layers usually process the entirespectrogram of an audio signal with the same time-frequency resolutions, numberof filters, and dimensionality reduction scale. According to the constant-Qtransform, good features can be extracted from audio signals if the lowfrequency bands are processed with high frequency resolution filters and thehigh frequency bands with high time resolution filters. In the spectrogram of amixture of singing voices and music signals, there is usually more informationabout the voice in the low frequency bands than the high frequency bands. Theseraise the need for processing each part of the spectrogram differently. In thispaper, we propose a multi-band multi-resolution fully convolutional neuralnetwork (MBR-FCN) for singing voice separation. The MBR-FCN processes thefrequency bands that have more information about the target signals with morefilters and smaller dimentionality reduction scale than the bands with lessinformation. Furthermore, the MBR-FCN processes the low frequency bands withhigh frequency resolution filters and the high frequency bands with high timeresolution filters. Our experimental results show that the proposed MBR-FCNwith very few parameters achieves better singing voice separation performancethan other deep neural networks.",https://arxiv.org/abs/1910.09266
['Title:Bayesian Optimization Allowing for Common Random Numbers'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Bayesian optimization is a powerful tool for expensive stochastic black-boxoptimization problems such as simulation-based optimization or machine learninghyperparameter tuning. Many stochastic objective functions implicitly require arandom number seed as input. By explicitly reusing a seed a user can exploitcommon random numbers, comparing two or more inputs under the same randomlygenerated scenario, such as a common customer stream in a job shop problem, orthe same random partition of training data into training and validation set fora machine learning algorithm. With the aim of finding an input with the bestaverage performance over infinitely many seeds, we propose a novel Gaussianprocess model that jointly models both the output for each seed and theaverage. We then introduce the Knowledge gradient for Common Random Numbersthat iteratively determines a combination of input and random seed to evaluatethe objective and automatically trades off reusing old seeds and querying newseeds, thus overcoming the need to evaluate inputs in batches or measuringdifferences of pairs as suggested in previous methods. We investigate theKnowledge Gradient for Common Random Numbers both theoretically andempirically, finding it achieves significant performance improvements with onlymoderate added computational cost.",https://arxiv.org/abs/1910.09259
['Title:Learning a Generic Adaptive Wavelet Shrinkage Function for Denoising'],Machine Learning ,"(Submitted on 21 Oct 2019 (v1), last revised 22 Oct 2019 (this version, v2))","Abstract:  The rise of machine learning in image processing has created a gap betweentrainable data-driven and classical model-driven approaches: Whilelearning-based models often show superior performance, classical ones are oftenmore transparent. To reduce this gap, we introduce a generic wavelet shrinkagefunction for denoising which is adaptive to both the wavelet scales as well asthe noise standard deviation. It is inferred from trained results of a tightlyparametrised function which is inherited from nonlinear diffusion. Our proposedshrinkage function is smooth and compact while only using two parameters. Incontrast to many existing shrinkage functions, it is able to enhance imagestructures by amplifying wavelet coefficients. Experiments show that itoutperforms classical shrinkage functions by a significant margin.",https://arxiv.org/abs/1910.09234
['Title:Safe-Bayesian Generalized Linear Regression'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  We study generalized Bayesian inference under misspecification, i.e. when themodel is `wrong but useful'. Generalized Bayes equips the likelihood with alearning rate $\eta$. We show that for generalized linear models (GLMs),$\eta$-generalized Bayes concentrates around the best approximation of thetruth within the model for specific $\eta \neq 1$, even under severelymisspecified noise, as long as the tails of the true distribution areexponential. We then derive MCMC samplers for generalized Bayesian lasso andlogistic regression, and give examples of both simulated and real-world data inwhich generalized Bayes outperforms standard Bayes by a vast margin.",https://arxiv.org/abs/1910.09227
['Title:Deep Reinforcement Learning Control of Quantum Cartpoles'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  We generalize a standard benchmark of reinforcement learning, the classicalcartpole balancing problem, to the quantum regime by stabilizing a particle inan unstable potential through measurement and feedback. We use thestate-of-the-art deep reinforcement learning to stabilize the quantum cartpoleand find that our deep learning approach performs comparably to or better thanother strategies in standard control theory. Our approach also applies tomeasurement-feedback cooling of quantum oscillators, showing the applicabilityof deep learning to general continuous-space quantum control.",https://arxiv.org/abs/1910.09200
['Title:Transferable Recognition-Aware Image Processing'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Recent progress in image recognition has stimulated the deployment of visionsystems (e.g. image search engines) at an unprecedented scale. As a result,visual data are now often consumed not only by humans but also by machines.Meanwhile, existing image processing methods only optimize for better humanperception, whereas the resulting images may not be accurately recognized bymachines. This can be undesirable, e.g., the images can be improperly handledby search engines or recommendation systems. In this work, we propose simpleapproaches to improve machine interpretability of processed images: optimizingthe recognition loss directly on the image processing network or through anintermediate transforming model, a process which we show can also be done in anunsupervised manner. Interestingly, the processing model's ability to enhancethe recognition performance can transfer when evaluated on differentrecognition models, even if they are of different architectures, trained ondifferent object categories or even different recognition tasks. This makes thesolutions applicable even when we do not have the knowledge about futuredownstream recognition models, e.g., if we are to upload the processed imagesto the Internet. We conduct comprehensive experiments on three image processingtasks with two downstream recognition tasks, and confirm our method bringssubstantial accuracy improvement on both the same recognition model and whentransferring to a different one, with minimal or no loss in the imageprocessing quality.",https://arxiv.org/abs/1910.09185
['Title:MeteorNet: Deep Learning on Dynamic 3D Point Cloud Sequences'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Understanding dynamic 3D environment is crucial for robotic agents and manyother applications. We propose a novel neural network architecture called$MeteorNet$ for learning representations for dynamic 3D point cloud sequences.Different from previous work that adopts a grid-based representation andapplies 3D or 4D convolutions, our network directly processes point clouds. Wepropose two ways to construct spatiotemporal neighborhoods for each point inthe point cloud sequence. Information from these neighborhoods is aggregated tolearn features per point. We benchmark our network on a variety of 3Drecognition tasks including action recognition, semantic segmentation and sceneflow estimation. MeteorNet shows stronger performance than previous grid-basedmethods while achieving state-of-the-art performance on Synthia. MeteorNet alsooutperforms previous baseline methods that are able to process at most twoconsecutive point clouds. To the best of our knowledge, this is the first workon deep learning for dynamic raw point cloud sequences.",https://arxiv.org/abs/1910.09165
['Title:Adversarial Anomaly Detection for Marked Spatio-Temporal Streaming Data'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Spatio-temporal event data are becoming increasingly available in a widevariety of applications, such as electronic transaction records, social networkdata, and crime data. How to efficiently detect anomalies in these dynamicsystems using these streaming event data? We propose a novel anomaly detectionframework for such event data combining the Long short-term memory (LSTM) andmarked spatio-temporal point processes. The detection procedure can be computedin an online and distributed fashion via feeding the streaming data through anLSTM and a neural network-based discriminator. We study the false-alarm-rateand detection delay using theory and simulation and show that it can achieveweak signal detection by aggregating local statistics over time and networks.Finally, we demonstrate the good performance using real-world datasets.",https://arxiv.org/abs/1910.09161
['Title:Entropic Dynamic Time Warping Kernels for Co-evolving Financial Time Series Analysis'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  In this work, we develop a novel framework to measure the similarity betweendynamic financial networks, i.e., time-varying financial networks.Particularly, we explore whether the proposed similarity measure can beemployed to understand the structural evolution of the financial networks withtime. For a set of time-varying financial networks with each vertexrepresenting the individual time series of a different stock and each edgebetween a pair of time series representing the absolute value of their Pearsoncorrelation, our start point is to compute the commute time matrix associatedwith the weighted adjacency matrix of the network structures, where eachelement of the matrix can be seen as the enhanced correlation value betweenpairwise stocks. For each network, we show how the commute time matrix allowsus to identify a reliable set of dominant correlated time series as well as anassociated dominant probability distribution of the stock belonging to thisset. Furthermore, we represent each original network as a discrete dominantShannon entropy time series computed from the dominant probabilitydistribution. With the dominant entropy time series for each pair of financialnetworks to hand, we develop a similarity measure based on the classicaldynamic time warping framework, for analyzing the financial time-varyingnetworks. We show that the proposed similarity measure is positive definite andthus corresponds to a kernel measure on graphs. The proposed kernel bridges thegap between graph kernels and the classical dynamic time warping framework formultiple financial time series analysis. Experiments on time-varying networksextracted through New York Stock Exchange (NYSE) database demonstrate theeffectiveness of the proposed approach.",https://arxiv.org/abs/1910.09153
['Title:Exploration via Sample-Efficient Subgoal Design'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  The problem of exploration in unknown environments continues to pose achallenge for reinforcement learning algorithms, as interactions with theenvironment are usually expensive or limited. The technique of setting subgoalswith an intrinsic shaped reward allows for the use of supplemental feedback toaid an agent in environment with sparse and delayed rewards. In fact, it can bean effective tool in directing the exploration behavior of the agent towarduseful parts of the state space. In this paper, we consider problems where anagent faces an unknown task in the future and is given prior opportunities to""practice"" on related tasks where the interactions are still expensive. Wepropose a one-step Bayes-optimal algorithm for selecting subgoal designs, alongwith the number of episodes and the episode length, to efficiently maximize theexpected performance of an agent. We demonstrate its excellent performance on avariety of tasks and also prove an asymptotic optimality guarantee.",https://arxiv.org/abs/1910.09143
['Title:DwNet: Dense warp-based network for pose-guided human video generation'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Generation of realistic high-resolution videos of human subjects is achallenging and important task in computer vision. In this paper, we focus onhuman motion transfer - generation of a video depicting a particular subject,observed in a single image, performing a series of motions exemplified by anauxiliary (driving) video. Our GAN-based architecture, DwNet, leverages denseintermediate pose-guided representation and refinement process to warp therequired subject appearance, in the form of the texture, from a source imageinto a desired pose. Temporal consistency is maintained by further conditioningthe decoding process within a GAN on the previously generated frame. In thisway a video is generated in an iterative and recurrent fashion. We illustratethe efficacy of our approach by showing state-of-the-art quantitative andqualitative performance on two benchmark datasets: TaiChi and Fashion Modeling.The latter is collected by us and will be made publicly available to thecommunity.",https://arxiv.org/abs/1910.09139
['Title:Adaptive Learned Bloom Filter (Ada-BF): Efficient Utilization of the Classifier'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Recent work suggests improving the performance of Bloom filter byincorporating a machine learning model as a binary classifier. However, suchlearned Bloom filter does not take full advantage of the predicted probabilityscores. We proposed new algorithms that generalize the learned Bloom filter byusing the complete spectrum of the scores regions. We proved our algorithmshave lower False Positive Rate (FPR) and memory usage compared with theexisting approaches to learned Bloom filter. We also demonstrated the improvedperformance of our algorithms on real-world datasets.",https://arxiv.org/abs/1910.09131
['Title:A Comparison of Semantic Similarity Methods for Maximum Human Interpretability'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  The inclusion of semantic information in any similarity measures improves theefficiency of the similarity measure and provides human interpretable result.This paper presents three different methods to compute semantic similaritiesbetween short news texts. These methods are based on corpus-based andknowledge-based methods: cosine similarity using tf-idf vectors, cosinesimilarity using word embedding and soft cosine similarity using wordembedding. As a result, cosine similarity using tf-idf vectors performed bestamong three in finding similarities between short news texts.",https://arxiv.org/abs/1910.09129
['Title:Communication Efficient Decentralized Training with Multiple Local Updates'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Communication efficiency plays a significant role in decentralizedoptimization, especially when the data is highly non-identically distributed.In this paper, we propose a novel algorithm that we call Periodic DecentralizedSGD (PD-SGD), to reduce the communication cost in a decentralized heterogeneousnetwork. PD-SGD alternates between multiple local updates and multipledecentralized communications, making communication more flexible andcontrollable. We theoretically prove PD-SGD convergence at speed$O(\frac{1}{\sqrt{nT}})$ under the setting of stochastic non-convexoptimization and non-i.i.d. data where $n$ is the number of worker nodes. Wealso propose a novel decay strategy which periodically shrinks the length oflocal updates. PD-SGD equipped with this strategy can better balance thecommunication-convergence trade-off both theoretically and empirically.",https://arxiv.org/abs/1910.09126
['Title:Self-Supervised Physics-Based Deep Learning MRI Reconstruction Without Fully-Sampled Data'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Deep learning (DL) has emerged as a tool for improving accelerated MRIreconstruction. A common strategy among DL methods is the physics-basedapproach, where a regularized iterative algorithm alternating between dataconsistency and a regularizer is unrolled for a finite number of iterations.This unrolled network is then trained end-to-end in a supervised manner, usingfully-sampled data as ground truth for the network output. However, in a numberof scenarios, it is difficult to obtain fully-sampled datasets, due tophysiological constraints such as organ motion or physical constraints such assignal decay. In this work, we tackle this issue and propose a self-supervisedlearning strategy that enables physics-based DL reconstruction withoutfully-sampled data. Our approach is to divide the acquired sub-sampled pointsfor each scan into training and validation subsets. During training, dataconsistency is enforced over the training subset, while the validation subsetis used to define the loss function. Results show that the proposedself-supervised learning method successfully reconstructs images withoutfully-sampled data, performing similarly to the supervised approach that istrained with fully-sampled references. This has implications for physics-basedinverse problem approaches for other settings, where fully-sampled data is notavailable or possible to acquire.",https://arxiv.org/abs/1910.09116
['Title:Adversarial Regression. Generative Adversarial Networks for Non-Linear Regression: Theory and Assessment'],Machine Learning ,(Submitted on 18 Oct 2019),"Abstract:  Adversarial Regression is a proposition to perform high dimensionalnon-linear regression with uncertainty estimation. We used ConditionalGenerative Adversarial Network to obtain an estimate of the full predictivedistribution for a new observation. Generative Adversarial Networks (GAN) areimplicit generative models which produce samples from a distributionapproximating the distribution of the data. The conditional version of it(CGAN) takes the following expression: $\min\limits_G \max\limits_D V(D, G) =\mathbb{E}_{x\sim p_{r}(x)} [log(D(x, y))] + \mathbb{E}_{z\sim p_{z}(z)} [log(1-D(G(z, y)))]$. An approximate solution can be found by trainingsimultaneously two neural networks to model D and G and feeding G with a randomnoise vector $z$. After training, we have that $G(z, y)\mathrel{\dot\sim}p_{data}(x, y)$. By fixing $y$, we have $G(z|y) \mathrel{\dot\sim}p{data}(x|y)$. By sampling $z$, we can therefore obtain samples followingapproximately $p(x|y)$, which is the predictive distribution of $x$ for a new$y$. We ran experiments to test various loss functions, data distributions,sample size, size of the noise vector, etc. Even if we observed differences, noexperiment outperformed consistently the others. The quality of CGAN forregression relies on fine-tuning a range of hyperparameters. In a broader view,the results show that CGANs are very promising methods to perform uncertaintyestimation for high dimensional non-linear regression.",https://arxiv.org/abs/1910.09106
['Title:Predicting origin-destination ride-sourcing demand with a spatio-temporal encoder-decoder residual multi-graph convolutional network'],Machine Learning ,(Submitted on 17 Oct 2019),"Abstract:  With the rapid development of mobile-internet technologies, on-demandride-sourcing services have become increasingly popular and largely reshapedthe way people travel. Demand prediction is one of the most fundamentalcomponents in supply-demand management systems of ride-sourcing platforms. Withaccurate short-term prediction for origin-destination (OD) demand, theplatforms make precise and timely decisions on real-time matching, idle vehiclereallocations and ride-sharing vehicle routing, etc. Compared to zone-baseddemand prediction that has been examined by many previous studies, OD-baseddemand prediction is more challenging. This is mainly due to the complicatedspatial and temporal dependencies among demand of different OD pairs. Toovercome this challenge, we propose the Spatio-Temporal Encoder-DecoderResidual Multi-Graph Convolutional network (ST-ED-RMGC), a novel deep learningmodel for predicting ride-sourcing demand of various OD pairs. Firstly, themodel constructs OD graphs, which utilize adjacent matrices to characterize thenon-Euclidean pair-wise geographical and semantic correlations among differentOD pairs. Secondly, based on the constructed graphs, a residual multi-graphconvolutional (RMGC) network is designed to encode the contextual-aware spatialdependencies, and a long-short term memory (LSTM) network is used to encode thetemporal dependencies, into a dense vector space. Finally, we reuse the RMGCnetworks to decode the compressed vector back to OD graphs and predict thefuture OD demand. Through extensive experiments on the for-hire-vehiclesdatasets in Manhattan, New York City, we show that our proposed deep learningframework outperforms the state-of-arts by a significant margin.",https://arxiv.org/abs/1910.09103
['Title:Biologic and Prognostic Feature Scores from Whole-Slide Histology Images Using Deep Learning'],Machine Learning ,"(Submitted on 21 Oct 2019 (v1), last revised 22 Oct 2019 (this version, v2))","Abstract:  Histopathology is a reflection of the molecular changes and providesprognostic phenotypes representing the disease progression. In this study, weintroduced feature scores generated from hematoxylin and eosin histology imagesbased on deep learning (DL) models developed for prostate pathology. Wedemonstrated that these feature scores were significantly prognostic for timeto event endpoints (biochemical recurrence and cancer-specific survival) andhad simultaneously molecular biologic associations to relevant genomicalterations and molecular subtypes using already trained DL models that werenot previously exposed to the datasets of the current study. Further, wediscussed the potential of such feature scores to improve the current tumorgrading system and the challenges that are associated with tumor heterogeneityand the development of prognostic models from histology images. Our findingsuncover the potential of feature scores from histology images as digitalbiomarkers in precision medicine and as an expanding utility for digitalpathology.",https://arxiv.org/abs/1910.09100
['Title:Self-supervised classification of dynamic obstacles using the temporal information provided by videos'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Nowadays, autonomous driving systems can detect, segment, and classify thesurrounding obstacles using a monocular camera. However, state-of-the-artmethods solving these tasks generally perform a fully supervised learningprocess and require a large amount of training labeled data. On another note,some self-supervised learning approaches can deal with detection andsegmentation of dynamic obstacles using the temporal information available invideo sequences. In this work, we propose in addition to classifiy the detectedobstacles depending on their motion pattern. We present a novel self-supervisedframework consisting of learning offline clusters from temporal patch sequencesand using these clusters as pseudo labels to train a real-time imageclassifier. The presented model outperforms state-of-the-art unsupervised imageclassification methods on BDD100K dataset.",https://arxiv.org/abs/1910.09094
['Title:Multi-User MABs with User Dependent Rewards for Uncoordinated Spectrum Access'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Multi-user multi-armed bandits have emerged as a good model for uncoordinatedspectrum access problems. In this paper we consider the scenario where userscannot communicate with each other. In addition, the environment may appeardifferently to different users, ${i.e.}$, the mean rewards as observed bydifferent users for the same channel may be different. With this setup, wepresent a policy that achieves a regret of $O (\log{T})$. This paper has beenaccepted at Asilomar Conference on Signals, Systems, and Computers 2019.",https://arxiv.org/abs/1910.09091
['Title:Semantics for Global and Local Interpretation of Deep Neural Networks'],Machine Learning ,(Submitted on 21 Oct 2019),"Abstract:  Deep neural networks (DNNs) with high expressiveness have achievedstate-of-the-art performance in many tasks. However, their distributed featurerepresentations are difficult to interpret semantically. In this work,human-interpretable semantic concepts are associated with vectors in featurespace. The association process is mathematically formulated as an optimizationproblem. The semantic vectors obtained from the optimal solution are applied tointerpret deep neural networks globally and locally. The global interpretationsare useful to understand the knowledge learned by DNNs. The interpretation oflocal behaviors can help to understand individual decisions made by DNNsbetter. The empirical experiments demonstrate how to use identified semanticsto interpret the existing DNNs.",https://arxiv.org/abs/1910.09085
['Title:Online Community Detection by Spectral CUSUM'],Machine Learning ,(Submitted on 20 Oct 2019),"Abstract:  We present an online community change detection algorithm called spectralCUSUM to detect the emergence of a community using a subspace projectionprocedure based on a Gaussian model setting. Theoretical analysis is providedto characterize the average run length (ARL) and expected detection delay(EDD), as well as the asymptotic optimality. Simulation and real data examplesdemonstrate the good performance of the proposed method.",https://arxiv.org/abs/1910.09083
['Title:Looking Ahead: Anticipating Pedestrians Crossing with Future Frames Prediction'],Machine Learning ,(Submitted on 20 Oct 2019),"Abstract:  In this paper, we present an end-to-end future-prediction model that focuseson pedestrian safety. Specifically, our model uses previous video frames,recorded from the perspective of the vehicle, to predict if a pedestrian willcross in front of the vehicle. The long term goal of this work is to design afully autonomous system that acts and reacts as a defensive human driver would--- predicting future events and reacting to mitigate risk. We focus onpedestrian-vehicle interactions because of the high risk of harm to thepedestrian if their actions are miss-predicted. Our end-to-end model consistsof two stages: the first stage is an encoder/decoder network that learns topredict future video frames. The second stage is a deep spatio-temporal networkthat utilizes the predicted frames of the first stage to predict thepedestrian's future action. Our system achieves state-of-the-art accuracy onpedestrian behavior prediction and future frames prediction on the JointAttention for Autonomous Driving (JAAD) dataset.",https://arxiv.org/abs/1910.09077
['Title:Detecting muscle activation using ultrasound speed of sound inversion with deep learning'],Machine Learning ,"(Submitted on 20 Oct 2019 (v1), last revised 22 Oct 2019 (this version, v2))","Abstract:  Functional muscle imaging is essential for diagnostics of a multitude ofmusculoskeletal afflictions such as degenerative muscle diseases, muscleinjuries, muscle atrophy, and neurological related issues such as spasticity.However, there is currently no solution, imaging or otherwise, capable ofproviding a map of active muscles over a large field of view in dynamicscenarios. In this work, we look at the feasibility of longitudinal sound speedmeasurements to the task of dynamic muscle imaging of contraction oractivation. We perform the assessment using a deep learning network applied topre-beamformed ultrasound channel data for sound speed inversion. Preliminaryresults show that dynamic muscle contraction can be detected in the calf andthat this contraction can be positively assigned to the operating muscles.Potential frame rates in the hundreds to thousands of frames per second arenecessary to accomplish this.",https://arxiv.org/abs/1910.09046
['Title:Ordering-Based Causal Structure Learning in the Presence of Latent Variables'],Machine Learning ,(Submitted on 20 Oct 2019),"Abstract:  We consider the task of learning a causal graph in the presence of latentconfounders given i.i.d.~samples from the model. While current algorithms forcausal structure discovery in the presence of latent confounders areconstraint-based, we here propose a score-based approach. We prove that underassumptions weaker than faithfulness, any sparsest independence map (IMAP) ofthe distribution belongs to the Markov equivalence class of the true model.This motivates the \emph{Sparsest Poset} formulation - that posets can bemapped to minimal IMAPs of the true model such that the sparsest of these IMAPsis Markov equivalent to the true model. Motivated by this result, we propose agreedy algorithm over the space of posets for causal structure discovery in thepresence of latent confounders and compare its performance to the currentstate-of-the-art algorithms FCI and FCI+ on synthetic data.",https://arxiv.org/abs/1910.09014
['Title:Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images'],Machine Learning ,(Submitted on 20 Oct 2019),"Abstract:  Incorporating human expertise and domain knowledge is particularly importantfor medical image processing applications, marked with small datasets, andobjects of interests in the form of organs or lesions not typically seen intraditional datasets. However, the incorporation of prior knowledge for breasttumor detection is challenging, since shape, boundary, curvature, intensity, orother common medical priors vary significantly across patients and cannot beemployed. This work proposes an approach for integrating visual saliency into adeep learning model for breast tumor segmentation in ultrasound images. Visualsaliency emphasizes regions that are more likely to attract radiologists'visual attention and stand out from its surrounding. Our approach is based on aU-Net model and employs attention blocks to introduce visual saliency. Suchmodel forces learning feature representations that prioritize spatial regionswith high levels of saliency. The approach is validated using a dataset of 510breast ultrasound images.",https://arxiv.org/abs/1910.08978
['Title:i-RIM applied to the fastMRI challenge'],Machine Learning ,(Submitted on 20 Oct 2019),"Abstract:  We, team AImsterdam, summarize our submission to the fastMRI challenge(Zbontar et al., 2018). Our approach builds on recent advances in invertiblelearning to infer models as presented in Putzky and Welling (2019). Both, oursingle-coil and our multi-coil model share the same basic architecture.",https://arxiv.org/abs/1910.08952
['Title:Sketch2Code: Transformation of Sketches to UI in Real-time Using Deep Neural Network'],Machine Learning ,(Submitted on 20 Oct 2019),"Abstract:  User Interface (UI) prototyping is a necessary step in the early stages ofapplication development. Transforming sketches of a Graphical User Interface(UI) into a coded UI application is an uninspired but time-consuming taskperformed by a UI designer. An automated system that can replace human effortsfor straightforward implementation of UI designs will greatly speed up thisprocedure. The works that propose such a system primarily focus on using UIwireframes as input rather than hand-drawn sketches. In this paper, we putforward a novel approach wherein we employ a Deep Neural Network that istrained on our custom database of such sketches to detect UI elements in theinput sketch. Detection of objects in sketches is a peculiar visual recognitiontask that requires a specific solution that our deep neural network modelattempts to provide. The output from the network is a platform-independent UIrepresentation object. The UI representation object is a dictionary ofkey-value pairs to represent the UI elements recognized along with theirproperties. This is further consumed by our UI parser which creates code fordifferent platforms. The intrinsic platform-independence allows the model tocreate a UI prototype for multiple platforms with single training. Thistwo-step approach without the need for two trained models improves over othermethods giving time-efficient results (average time: 129 ms) with goodaccuracy.",https://arxiv.org/abs/1910.08930
['Title:Enhancing Recurrent Neural Networks with Sememes'],Machine Learning ,(Submitted on 20 Oct 2019),"Abstract:  Sememes, the minimum semantic units of human languages, have beensuccessfully utilized in various natural language processing applications.However, most existing studies exploit sememes in specific tasks and fewefforts are made to utilize sememes more fundamentally. In this paper, wepropose to incorporate sememes into recurrent neural networks (RNNs) to improvetheir sequence modeling ability, which is beneficial to all kinds of downstreamtasks. We design three different sememe incorporation methods and employ themin typical RNNs including LSTM, GRU and their bidirectional variants. Forevaluation, we use several benchmark datasets involving PTB and WikiText-2 forlanguage modeling, SNLI for natural language inference. Experimental resultsshow evident and consistent improvement of our sememe-incorporated modelscompared with vanilla RNNs, which proves the effectiveness of our sememeincorporation methods. Moreover, we find the sememe-incorporated models havegreat robustness and outperform adversarial training in defending adversarialattack. All the code and data of this work will be made available to thepublic.",https://arxiv.org/abs/1910.08910
['Title:$hv$-Block Cross Validation is not a BIBD: a Note on the Paper by Jeff Racine (2000)'],Machine Learning ,(Submitted on 20 Oct 2019),"Abstract:  This note corrects a mistake in the paper ""consistent cross-validatorymodel-selection for dependent data: $hv$-block cross-validation"" by Racine(2000). In his paper, he implied that the therein proposed $hv$-blockcross-validation is consistent in the sense of Shao (1993). To get thisintuition, he relied on the speculation that $hv$-block is a balancedincomplete block design (BIBD). This note demonstrates that this is not thecase, and thus the theoretical consistency of $hv$-block remains an openquestion. In addition, I also provide a Python program counting the number ofoccurrences of each sample and each pair of samples.",https://arxiv.org/abs/1910.08904
['Title:Benchmark Dataset for Timetable Optimization of Bus Routes in the City of New Delhi'],Machine Learning ,(Submitted on 20 Oct 2019),"Abstract:  Public transport is one of the major forms of transportation in the world.This makes it vital to ensure that public transport is efficient. This researchpresents a novel real-time GPS bus transit data for over 500 routes of busesoperating in New Delhi. The data can be used for modeling various timetableoptimization tasks as well as in other domains such as traffic management,travel time estimation, etc. The paper also presents an approach to reduce thewaiting time of Delhi buses by analyzing the traffic behavior and proposing atimetable. This algorithm serves as a benchmark for the dataset. The algorithmuses a constrained clustering algorithm for classification of trips. It furtheranalyses the data statistically to provide a timetable which is efficient inlearning the inter- and intra-month variations.",https://arxiv.org/abs/1910.08903
['Title:Personalizing Graph Neural Networks with Attention Mechanism for Session-based Recommendation'],Machine Learning ,(Submitted on 20 Oct 2019),"Abstract:  The problem of personalized session-based recommendation aims to predictusers' next click based on their sequential behaviors. Existing session-basedrecommendation methods only consider all sessions of user as a single sequence,ignoring the relationship of among sessions. Other than that, most of themneglect complex transitions of items and the collaborative relationship betweenusers and items. To this end, we propose a novel method, named PersonalizingGraph Neural Networks with Attention Mechanism, A-PGNN for brevity. A-PGNNmainly consists of two components: One is Personalizing Graph Neural Network(PGNN), which is used to capture complex transitions in user session sequence.Compared with the traditional Graph Neural Network (GNN) model, it alsoconsiders the role of users in the sequence. The other is Dot-Product Attentionmechanism, which draws on the attention mechanism in machine translation toexplicitly model the effect of historical sessions on the current session.These two parts make it possible to learn the multi-level transitionrelationships between items and sessions in user-specific fashion. Extensiveexperiments conducted on two real-world data sets show that A-PGNNsignificantly outperforms the state-of-the-art personalizing session-basedrecommendation methods consistently.",https://arxiv.org/abs/1910.08887
['Title:The Exact Equivalence of Independence Testing and Two-Sample Testing'],Machine Learning ,(Submitted on 20 Oct 2019),"Abstract:  Testing independence and testing equality of distributions are two tightlyrelated statistical hypotheses. Several distance and kernel-based statisticsare recently proposed to achieve universally consistent testing for eitherhypothesis. On the distance side, the distance correlation is proposed forindependence testing, and the energy statistic is proposed for two-sampletesting. On the kernel side, the Hilbert-Schmidt independence criterion isproposed for independence testing and the maximum mean discrepancy is proposedfor two-sample testing. In this paper, we show that two-sample testing arespecial cases of independence testing via an auxiliary label vector, and provethat distance correlation is exactly equivalent to the energy statistic interms of the population statistic, the sample statistic, and the testingp-value via permutation test. The equivalence can be further generalized toK-sample testing and extended to the kernel regime. As a consequence, itsuffices to always use an independence statistic to test equality ofdistributions, which enables better interpretability of the test statistic andmore efficient testing.",https://arxiv.org/abs/1910.08883
['Title:Sparse (group) learning with Lipschitz loss functions: a unified analysis'],Machine Learning ,(Submitted on 20 Oct 2019),"Abstract:  We study a family of sparse estimators defined as minimizers of someempirical Lipschitz loss function---which include hinge, logistic and quantileregression losses---with a convex, sparse or group-sparse regularization. Inparticular, we consider the L1-norm on the coefficients, its sorted Slopeversion, and the Group L1-L2 extension. First, we propose a theoreticalframework which simultaneously derives new L2 estimation upper bounds for allthree regularization schemes. For L1 and Slope regularizations, our boundsscale as $(k^*/n) \log(p/k^*)$---$n\times p$ is the size of the design matrixand $k^*$ the dimension of the theoretical loss minimizer $\beta^*$---matchingthe optimal minimax rate achieved for the least-squares case. For Group L1-L2regularization, our bounds scale as $(s^*/n) \log\left( G / s^* \right) + m^* /n$---$G$ is the total number of groups and $m^*$ the number of coefficients inthe $s^*$ groups which contain $\beta^*$---and improve over the least-squarescase. We additionally show that Group L1-L2 is superior to L1 and Slope whenthe signal is strongly group-sparse. Our bounds are achieved both inprobability and in expectation, under common assumptions in the literature.Second, we propose an accelerated proximal algorithm which computes the convexestimators studied when the number of variables is of the order of $100,000$.We compare the statistical performance of our estimators against standardbaselines for settings where the signal is either sparse or group-sparse. Ourexperiments findings reveal (i) the good empirical performance of L1 and Slopefor sparse binary classification problems, (ii) the superiority of Group L1-L2regularization for group-sparse classification problems and (iii) the appealingproperties of sparse quantile regression estimators for sparse regressionproblems with heteroscedastic noise.",https://arxiv.org/abs/1910.08880
['Title:Probabilistic Radiomics: Ambiguous Diagnosis with Controllable Shape Analysis'],Machine Learning ,(Submitted on 20 Oct 2019),"Abstract:  Radiomics analysis has achieved great success in recent years. However,conventional Radiomics analysis suffers from insufficiently expressivehand-crafted features. Recently, emerging deep learning techniques, e.g.,convolutional neural networks (CNNs), dominate recent research inComputer-Aided Diagnosis (CADx). Unfortunately, as black-box predictors, weargue that CNNs are ""diagnosing"" voxels (or pixels), rather than lesions; inother words, visual saliency from a trained CNN is not necessarily concentratedon the lesions. On the other hand, classification in clinical applicationssuffers from inherent ambiguities: radiologists may produce diverse diagnosison challenging cases. To this end, we propose a controllable and explainable{\em Probabilistic Radiomics} framework, by combining the Radiomics analysisand probabilistic deep learning. In our framework, 3D CNN feature is extractedupon lesion region only, then encoded into lesion representation, by acontrollable Non-local Shape Analysis Module (NSAM) based on self-attention.Inspired from variational auto-encoders (VAEs), an Ambiguity PriorNet is usedto approximate the ambiguity distribution over human experts. The finaldiagnosis is obtained by combining the ambiguity prior sample and lesionrepresentation, and the whole network named $DenseSharp^{+}$ is end-to-endtrainable. We apply the proposed method on lung nodule diagnosis on LIDC-IDRIdatabase to validate its effectiveness.",https://arxiv.org/abs/1910.08878
['Title:Speech Emotion Recognition with Dual-Sequence LSTM Architecture'],Machine Learning ,(Submitted on 20 Oct 2019),"Abstract:  Speech Emotion Recognition (SER) has emerged as a critical component of thenext generation of human-machine interfacing technologies. In this work, wepropose a new dual-level model that combines handcrafted and raw features foraudio signals. Each utterance is preprocessed into a handcrafted input and twomel-spectrograms at different time-frequency resolutions. An LSTM processes thehandcrafted input, while a novel LSTM architecture, denoted as Dual-SequenceLSTM (DS-LSTM), processes the two mel-spectrograms simultaneously. The outputsare later averaged to produce a final classification of the utterance. Ourproposed model achieves, on average, a weighted accuracy of 72.7% and anunweighted accuracy of 73.3% --- a 6% improvement over current state-of-the-artmodels --- and is comparable with multimodal SER models that leverage textualinformation.",https://arxiv.org/abs/1910.08874
['Title:KRNET: Image Denoising with Kernel Regulation Network'],Machine Learning ,(Submitted on 20 Oct 2019),"Abstract:  One popular strategy for image denoising is to design a generalizedregularization term that is capable of exploring the implicit prior underlyingdata observation. Convolutional neural networks (CNN) have shown the powerfulcapability to learn image prior information through a stack of layers definedby a combination of kernels (filters) on the input. However, existing CNN-basedmethods mainly focus on synthetic gray-scale images. These methods stillexhibit low performance when tackling multi-channel color image denoising. Inthis paper, we optimize CNN regularization capability by developing a kernelregulation module. In particular, we propose a kernel regulation network-block,referred to as KR-block, by integrating the merits of both large and smallkernels, that can effectively estimate features in solving image denoising. Webuild a deep CNN-based denoiser, referred to as KRNET, via concatenatingmultiple KR-blocks. We evaluate KRNET on additive white Gaussian noise (AWGN),multi-channel (MC) noise, and realistic noise, where KRNET obtains significantperformance gains over state-of-the-art methods across a wide spectrum of noiselevels.",https://arxiv.org/abs/1910.08867
['Title:Image Restoration Using Deep Regulated Convolutional Networks'],Machine Learning ,(Submitted on 19 Oct 2019),"Abstract:  While the depth of convolutional neural networks has attracted substantialattention in the deep learning research, the width of these networks hasrecently received greater interest. The width of networks, defined as the sizeof the receptive fields and the density of the channels, has demonstratedcrucial importance in low-level vision tasks such as image denoising andrestoration. However, the limited generalization ability, due to the increasedwidth of networks, creates a bottleneck in designing wider networks. In thispaper, we propose the Deep Regulated Convolutional Network (RC-Net), a deepnetwork composed of regulated sub-network blocks cascaded by skip-connections,to overcome this bottleneck. Specifically, the Regulated Convolution block(RC-block), featured by a combination of large and small convolution filters,balances the effectiveness of prominent feature extraction and thegeneralization ability of the network. RC-Nets have several compellingadvantages: they embrace diversified features through large-small filtercombinations, alleviate the hazy boundary and blurred details in imagedenoising and super-resolution problems, and stabilize the learning process.Our proposed RC-Nets outperform state-of-the-art approaches with significantperformance gains in various image restoration tasks while demonstratingpromising generalization ability. The code is available atthis https URL.",https://arxiv.org/abs/1910.08853
['Title:ProxIQA: A Proxy Approach to Perceptual Optimization of Learned Image Compression'],Machine Learning ,(Submitted on 19 Oct 2019),"Abstract:  The use of $\ell_p$ $(p=1,2)$ norms has largely dominated the measurement ofloss in neural networks due to their simplicity and analytical properties.However, when used to assess the loss of visual information, these simple normsare not very consistent with human perception. Here, we describe a different""proximal"" approach to optimize image analysis networks against quantitativeperceptual models. Specifically, we construct a proxy network, broadly termedProxIQA, which mimics the perceptual model while serving as a loss layer of thenetwork. We experimentally demonstrate how this optimization framework can beapplied to train an end-to-end optimized image compression network. By buildingon top of an existing deep image compression model, we are able to demonstratea bitrate reduction of as much as $31\%$ over MSE optimization, given aspecified perceptual quality (VMAF) level.",https://arxiv.org/abs/1910.08845
['Title:NormGrad: Finding the Pixels that Matter for Training'],Machine Learning ,(Submitted on 19 Oct 2019),"Abstract:  The different families of saliency methods, either based on contrastivesignals, closed-form formulas mixing gradients with activations or onperturbation masks, all focus on which parts of an image are responsible forthe model's inference. In this paper, we are rather interested by the locationsof an image that contribute to the model's training. First, we propose aprincipled attribution method that we extract from the summation formula usedto compute the gradient of the weights for a 1x1 convolutional layer. Theresulting formula is fast to compute and can used throughout the network,allowing us to efficiently produce fined-grained importance maps. We will showhow to extend it in order to compute saliency maps at any targeted point withinthe network. Secondly, to make the attribution really specific to the trainingof the model, we introduce a meta-learning approach for saliency methods byconsidering an inner optimisation step within the loss. This way, we do not aimat identifying the parts of an image that contribute to the model's output butrather the locations that are responsible for the good training of the model onthis image. Conversely, we also show that a similar meta-learning approach canbe used to extract the adversarial locations which can lead to the degradationof the model.",https://arxiv.org/abs/1910.08823
['Title:Opinion shaping in social networks using reinforcement learning'],Machine Learning ,(Submitted on 19 Oct 2019),"Abstract:  In this paper, we study how to shape opinions in social networks when thematrix of interactions is unknown. We consider classical opinion dynamics withsome stubborn agents and the possibility of continuously influencing theopinions of a few selected agents, albeit under resource constraints. We mapthe opinion dynamics to a value iteration scheme for policy evaluation for aspecific stochastic shortest path problem. This leads to a representation ofthe opinion vector as an approximate value function for a stochastic shortestpath problem with some non-classical constraints. We suggest two possible waysof influencing agents. One leads to a convex optimization problem and the otherto a non-convex one. Firstly, for both problems, we propose two differentonline two-time scale reinforcement learning schemes that converge to theoptimal solution of each problem. Secondly, we suggest stochastic gradientdescent schemes and compare these classes of algorithms with the two-time scalereinforcement learning schemes. Thirdly, we also derive another algorithmdesigned to tackle the curse of dimensionality one faces when all agents areobserved. Numerical studies are provided to illustrate the convergence andefficiency of our algorithms.",https://arxiv.org/abs/1910.08802
['Title:Kernels of Mallows Models under the Hamming Distance for solving the Quadratic Assignment Problem'],Machine Learning ,(Submitted on 19 Oct 2019),"Abstract:  The Quadratic Assignment Problem (QAP) is a well-known permutation-basedcombinatorial optimization problem with real applications in industrial andlogistics environments. Motivated by the challenge that this NP-hard problemrepresents, it has captured the attention of the evolutionary computationcommunity for decades. As a result, a large number of algorithms have beenproposed to optimize this algorithm. Among these, exact methods are only ableto solve instances of size $n<40$, and thus, many heuristic and metaheuristicmethods have been applied to the QAP. In this work, we follow this direction byapproaching the QAP through Estimation of Distribution Algorithms (EDAs).Particularly, a non-parametric distance-based exponential probabilistic modelis used. Based on the analysis of the characteristics of the QAP, and previouswork in the area, we introduce Kernels of Mallows Model under the Hammingdistance to the context of EDAs. Conducted experiments point out that theperformance of the proposed algorithm in the QAP is superior to (i) theclassical EDAs adapted to deal with the QAP, and also (ii) to the specific EDAsproposed in the literature to deal with permutation problems.",https://arxiv.org/abs/1910.08800
['Title:A simple approach to design quantum neural networks and its applications to kernel-learning methods'],Machine Learning ,(Submitted on 19 Oct 2019),"Abstract:  We give an explicit simple method to build quantum neural networks (QNNs) tosolve classification problems. Besides the input (state preparation) and output(amplitude estimation), it has one hidden layer which uses a tensor product of$\log M$ two-dimensional rotations to introduce $\log M$ weights. Here $M$ isthe number of training samples. We also have an efficient method to prepare thequantum states of the training samples. By the quantum-classical hybrid methodor the variational method, the training algorithm of this QNN is easy toaccomplish in a quantum computer. The idea is inspired by the kernel methodsand the radial basis function (RBF) networks. In turn, the construction of QNNprovides new findings in the design of RBF networks. As an application, weintroduce a quantum-inspired RBF network, in which the number of weightparameters is $\log M$. Numerical tests indicate that the performance of thisneural network in solving classification problems improves when $M$ increases.Since using exponentially fewer parameters, more advanced optimization methods(e.g. Newton's method) can be used to train this network. Finally, about theconvex optimization problem to train support vector machines, we use a similaridea to reduce the number of variables, which equals $M$, to $\log M$.",https://arxiv.org/abs/1910.08798
['Title:Online Ranking with Concept Drifts in Streaming Data'],Machine Learning ,(Submitted on 19 Oct 2019),"Abstract:  Two important problems in preference elicitation are rank aggregation andlabel ranking. Rank aggregation consists of finding a ranking that bestsummarizes a collection of preferences of some agents. The latter, labelranking, aims at learning a mapping between data instances and rankings definedover a finite set of categories or labels. This problem can effectively modelmany real application scenarios such as recommender systems. However, even whenthe preferences of populations usually change over time, related literature hasso far addressed both problems over non-evolving preferences.This work deals with the problems of rank aggregation and label ranking overnon-stationary data streams. In this context, there is a set of $n$ items and$m$ agents which provide their votes by casting a ranking of the $n$ items. Therankings are noisy realizations of an unknown probability distribution thatchanges over time. Our goal is to learn, in an online manner, the currentground truth distribution of rankings.We begin by proposing an aggregation function called Forgetful Borda (FBorda)that, using a forgetting mechanism, gives more importance to recently observedpreferences. We prove that FBorda is a consistent estimator of the Kemenyranking and lower bound the number of samples needed to learn the distributionwhile guaranteeing a certain level of confidence. Then, we develop a$k$-nearest neighbor classifier based on the proposed FBorda aggregationalgorithm for the label ranking problem and demonstrate its accuracy in severalscenarios of label ranking problem over evolving preferences.",https://arxiv.org/abs/1910.08795
['Title:LEt-SNE: A Hybrid Approach To Data Embedding and Visualization of Hyperspectral Imagery'],Machine Learning ,(Submitted on 19 Oct 2019),"Abstract:  Hyperspectral Imagery (and Remote Sensing in general) captured from UAVs orsatellites are highly voluminous in nature due to the large spatial extent andwavelengths captured by them. Since analyzing these images requires a hugeamount of computational time and power, various dimensionality reductiontechniques have been used for feature reduction. Some popular techniques amongthese falter when applied to Hyperspectral Imagery due to the famed curse ofdimensionality. In this paper, we propose a novel approach, LEt-SNE, whichcombines graph based algorithms like t-SNE and Laplacian Eigenmaps into a modelparameterized by a shallow feed forward network. We introduce a new term,Compression Factor, that enables our method to combat the curse ofdimensionality. The proposed algorithm is suitable for manifold visualizationand sample clustering with labelled or unlabelled data. We demonstrate that ourmethod is competitive with current state-of-the-art methods on hyperspectralremote sensing datasets in public domain.",https://arxiv.org/abs/1910.08790
['Title:Measurement Dependence Inducing Latent Causal Models'],Machine Learning ,(Submitted on 19 Oct 2019),"Abstract:  We consider the task of causal structure learning over measurement dependenceinducing latent (MeDIL) causal models. We show that this task can be framed interms of the graph theoretical problem of finding edge clique covers, resultingin a simple algorithm for returning minimal MeDIL causal models (minMCMs). Thisalgorithm is non-parametric, requiring no assumptions about linearity orGaussianity. Furthermore, despite rather weak assumptions about the class ofMeDIL causal models, we show that minimality in minMCMs implies three ratherspecific and interesting properties: first, minMCMs provide lower bounds on (i)the number of latent causal variables and (ii) the number of functional causalrelations that are required to model a complex system at any level ofgranularity; second, a minMCM contains no causal links between the latentvariables; and third, in contrast to factor analysis, a minMCM may require morelatent than measurement variables.",https://arxiv.org/abs/1910.08778
['Title:Convex Reconstruction of Structured Matrix Signals from Random Linear Measurements (I): Theoretical Results'],Machine Learning ,(Submitted on 19 Oct 2019),"Abstract:  We investigate the problem of reconstructing n-by-n column-wise sparse matrixsignal X=(x1,...,xn) via convex programming, where each column xj is a vectorof s-sparsity. The regularizer is matrix norm |||X|||1:=maxj|xj|1 where |.|1 isthe l1-norm in vector space. We take the convex geometric approach in randommeasurement setting and establish sufficient conditions on dimensions ofmeasurement spaces for robust reconstruction in noise and some necessaryconditions for accurate reconstruction. For example, for the m-by-m measurementY=AXB+E where E is bounded noise and A, B are m-by-n random matrices, one ofthe established sufficient conditions for X to be reconstructed robustly withrespect to Frobenius norm is m2 > C(n2-r(n-slog2(C1n2r))+C2n) when A, B areboth sub-Gaussian matrices, where r and s are signal's structural parameters,i.e., s is the maximum number of nonzero entries in each column and r is thenumber of columns which l1-norms are maximum among all columns. In particular,when r = n the sufficient condition reduces to m2 > Cnslog2(Cn3). This bound isrelatively tight because a provable necessary condition is m2 > C1nslog(C2n/s)",https://arxiv.org/abs/1910.08771
['Title:Microservices based Framework to Support Interoperable IoT Applications for Enhanced Data Analytics'],Machine Learning ,(Submitted on 19 Oct 2019),"Abstract:  Internet of things is growing with a large number of diverse objects whichgenerate billions of data streams by sensing, actuating and communicating.Management of heterogeneous IoT objects with existing approaches and processingof myriads of data from these objects using monolithic services have becomemajor challenges in developing effective IoT applications. The heterogeneitycan be resolved by providing interoperability with semantic virtualization ofobjects. Moreover, monolithic services can be substituted with modularmicroservices. This article presents an architecture that enables thedevelopment of IoT applications using semantically interoperable microservicesand virtual objects. The proposed framework supports analytic features withknowledge-driven and data-driven techniques to provision intelligent serviceson top of interoperable microservices in Web Objects enabled IoT environment.The knowledge-driven aspects are supported with reasoning on semantic ontologymodels and the data-driven aspects are realized with machine learning pipeline.The development of service functionalities is supported with microservices toenhance modularity and reusability. To evaluate the proposed framework a proofof concept implementation with a use case is discussed.",https://arxiv.org/abs/1910.08713
['Title:Gastroscopic Panoramic View: Application to Automatic Polyps Detection under Gastroscopy'],Machine Learning ,(Submitted on 19 Oct 2019),"Abstract:  Endoscopic diagnosis is an important means for gastric polyp detection. Inthis paper, a panoramic image of gastroscopy is developed, which can displaythe inner surface of the stomach intuitively and comprehensively. Moreover, theproposed automatic detection solution can help doctors locate the polypsautomatically, and reduce missed diagnosis. The main contributions of thispaper are: firstly, a gastroscopic panorama reconstruction method is developed.The reconstruction does not require additional hardware devices, and can solvethe problem of texture dislocation and illumination imbalance properly;secondly, an end-to-end multi-object detection for gastroscopic panorama istrained based on deep learning framework. Compared with traditional solutions,the automatic polyp detection system can locate all polyps in the inner wall ofstomach in real time and assist doctors to find the lesions. Thirdly, thesystem was evaluated in the Affiliated Hospital of Zhejiang University. Theresults show that the average error of the panorama is less than 2 mm, theaccuracy of the polyp detection is 95%, and the recall rate is 99%. Inaddition, the research roadmap of this paper has guiding significance forendoscopy-assisted detection of other human soft cavities.",https://arxiv.org/abs/1910.08697
['Title:Real-Time Lip Sync for Live 2D Animation'],Machine Learning ,(Submitted on 19 Oct 2019),"Abstract:  The emergence of commercial tools for real-time performance-based 2Danimation has enabled 2D characters to appear on live broadcasts and streamingplatforms. A key requirement for live animation is fast and accurate lip syncthat allows characters to respond naturally to other actors or the audiencethrough the voice of a human performer. In this work, we present a deeplearning based interactive system that automatically generates live lip syncfor layered 2D characters using a Long Short Term Memory (LSTM) model. Oursystem takes streaming audio as input and produces viseme sequences with lessthan 200ms of latency (including processing time). Our contributions includespecific design decisions for our feature definition and LSTM configurationthat provide a small but useful amount of lookahead to produce accurate lipsync. We also describe a data augmentation procedure that allows us to achievegood results with a very small amount of hand-animated training data (13-20minutes). Extensive human judgement experiments show that our results arepreferred over several competing methods, including those that only supportoffline (non-live) processing. Video summary and supplementary results atGitHub link: this https URL",https://arxiv.org/abs/1910.08685
['Title:Temporal Network Sampling'],Machine Learning ,(Submitted on 18 Oct 2019),"Abstract:  Temporal networks representing a stream of timestamped edges are seeminglyubiquitous in the real-world. However, the massive size and continuous natureof these networks make them fundamentally challenging to analyze and leveragefor descriptive and predictive modeling tasks. In this work, we propose ageneral framework for temporal network sampling with unbiased estimation. Wedevelop online, single-pass sampling algorithms and unbiased estimators fortemporal network sampling. The proposed algorithms enable fast, accurate, andmemory-efficient statistical estimation of temporal network patterns andproperties. In addition, we propose a temporally decaying sampling algorithmwith unbiased estimators for studying networks that evolve in continuous time,where the strength of links is a function of time, and the motif patterns aretemporally-weighted. In contrast to the prior notion of a $\bigtriangleupt$-temporal motif, the proposed formulation and algorithms for countingtemporally weighted motifs are useful for forecasting tasks in networks such aspredicting future links, or a future time-series variable of nodes and links.Finally, extensive experiments on a variety of temporal networks from differentdomains demonstrate the effectiveness of the proposed algorithms.",https://arxiv.org/abs/1910.08657
['Title:Ensemble learning based linear power flow'],Machine Learning ,(Submitted on 18 Oct 2019),"Abstract:  This paper develops an ensemble learning-based linearization approach forpower flow, which differs from the network-parameter based direct current (DC)power flow or other extended versions of linearization. As a novel data-drivenlinearization through data mining, it firstly applies the polynomial regression(PR) as a basic learner to capture the linear relationships between the busvoltage as the independent variable and the active or reactive power as thedependent variable in rectangular coordinates. Then, gradient boosting (GB) andbagging as ensemble learning methods are introduced to combine all basiclearners to boost the model performance. The fitted linear power flow model isalso relaxed to compute the optimal power flow (OPF). The simulating results ofstandard IEEE cases indicate that (1) ensemble learning methods outperform PRand GB works better than bagging; (2) as for solving OPF, the data-driven modelexcels the DC model and the SDP relaxation in the computational accuracy, andworks faster than ACOPF and SDPOPF.",https://arxiv.org/abs/1910.08655
['Title:Clustering by Optimizing the Average Silhouette Width'],Machine Learning ,(Submitted on 18 Oct 2019),"Abstract:  In this paper, we propose a unified clustering approach that can estimatenumber of clusters and produce clustering against this number simultaneously.Average silhouette width (ASW) is a widely used standard cluster quality index.We define a distance based objective function that optimizes ASW forclustering. The proposed algorithm named as OSil, only, needs data observationsas an input without any prior knowledge of the number of clusters. This work isabout thorough investigation of the proposed methodology, its usefulness andlimitations. A vast spectrum of clustering structures were generated, andseveral well-known clustering methods including partitioning, hierarchical,density based, and spatial methods were consider as the competitor of theproposed methodology. Simulation reveals that OSil algorithm has shown superiorperform in terms of clustering quality than all clustering methods included inthe study. OSil can find well separated, compact clusters and have shown betterperformance for the estimation of number of clusters than several methods.Apart from the proposal of the new methodology and it's investigation thispapers offer a systematic analysis on the estimation of cluster indices, someof which never appeared together in comparative simulation setup before. Thestudy offers many insightful findings useful for the selection of theclustering methods and indices.",https://arxiv.org/abs/1910.08644
['Title:Neural Logic Networks'],Machine Learning ,(Submitted on 17 Oct 2019),"Abstract:  Recent years have witnessed the great success of deep neural networks in manyresearch areas. The fundamental idea behind the design of most neural networksis to learn similarity patterns from data for prediction and inference, whichlacks the ability of logical reasoning. However, the concrete ability oflogical reasoning is critical to many theoretical and practical problems. Inthis paper, we propose Neural Logic Network (NLN), which is a dynamic neuralarchitecture that builds the computational graph according to input logicalexpressions. It learns basic logical operations as neural modules, and conductspropositional logical reasoning through the network for inference. Experimentson simulated data show that NLN achieves significant performance on solvinglogical equations. Further experiments on real-world data show that NLNsignificantly outperforms state-of-the-art models on collaborative filteringand personalized recommendation tasks.",https://arxiv.org/abs/1910.08629
['Title:Poisson CNN: Convolutional Neural Networks for the Solution of the Poisson Equation with Varying Meshes and Dirichlet Boundary Conditions'],Machine Learning ,(Submitted on 18 Oct 2019),"Abstract:  The Poisson equation is commonly encountered in engineering, including incomputational fluid dynamics where it is needed to compute corrections to thepressure field. We propose a novel fully convolutional neural network (CNN)architecture to infer the solution of the Poisson equation on a 2D Cartesiangrid of varying size and spacing given the right hand side term, arbitraryDirichlet boundary conditions and grid parameters which provides unprecendentedversatility in this application. The boundary conditions are handled using anovel approach by decomposing the original Poisson problem into a homogeneousPoisson problem plus four inhomogeneous Laplace sub-problems. The model istrained using a novel loss function approximating the continuous $L^p$ normbetween the prediction and the target. Analytical test cases indicate that ourCNN architecture is capable of predicting the correct solution of a Poissonproblem with mean percentage errors of 15% and promises improvements inwall-clock runtimes for large problems. Furthermore, even when predicting onmeshes denser than previously encountered, our model demonstrates encouragingcapacity to reproduce the correct solution profile.",https://arxiv.org/abs/1910.08613
['Title:AI Safety for High Energy Physics'],Machine Learning ,(Submitted on 18 Oct 2019),"Abstract:  The field of high-energy physics (HEP), along with many scientificdisciplines, is currently experiencing a dramatic influx of new methodologiespowered by modern machine learning techniques. Over the last few years, agrowing body of HEP literature has focused on identifying promisingapplications of deep learning in particular, and more recently these techniquesare starting to be realized in an increasing number of experimentalmeasurements. The overall conclusion from this impressive and extensive set ofstudies is that rarer and more complex physics signatures can be identifiedwith the new set of powerful tools from deep learning. However, there is anunstudied systematic risk associated with combining the traditional HEPworkflow and deep learning with high-dimensional data. In particular,calibrating and validating the response of deep neural networks is in generalnot experimentally feasible, and therefore current methods may be biased inways that are not covered by current uncertainty estimates. By borrowing ideasfrom AI safety, we illustrate these potential issues and propose a method tobound the size of unaccounted for uncertainty. In addition to providing apragmatic diagnostic, this work will hopefully begin a dialogue within thecommunity about the robust application of deep learning to experimentalanalyses.",https://arxiv.org/abs/1910.08606
['Title:Robust Learning Rate Selection for Stochastic Optimization via Splitting Diagnostic'],Machine Learning ,(Submitted on 18 Oct 2019),"Abstract:  This paper proposes SplitSGD, a new stochastic optimization algorithm with adynamic learning rate selection rule. This procedure decreases the learningrate for better adaptation to the local geometry of the objective functionwhenever a stationary phase is detected, that is, the iterates are likely tobounce around a vicinity of a local minimum. The detection is performed bysplitting the single thread into two and using the inner products of thegradients from the two threads as a measure of stationarity. This learning rateselection is provably valid, robust to initial parameters, easy-to-implement,and essentially does not incur additional computational cost. Finally, weillustrate the robust convergence properties of SplitSGD through extensiveexperiments.",https://arxiv.org/abs/1910.08597
['Title:Anderson Acceleration of Proximal Gradient Methods'],Machine Learning ,(Submitted on 18 Oct 2019),"Abstract:  Anderson acceleration is a well-established and simple technique for speedingup fixed-point computations with countless applications. Previous studies ofAnderson acceleration in optimization have only been able to provideconvergence guarantees for unconstrained and smooth problems. This workintroduces novel methods for adapting Anderson acceleration to (non-smooth andconstrained) proximal gradient algorithms. Under some technical conditions, weextend the existing local convergence results of Anderson acceleration forsmooth fixed-point mappings to the proposed scheme. We also prove analyticallythat it is not, in general, possible to guarantee global convergence of nativeAnderson acceleration. We therefore propose a simple scheme for stabilizationthat combines the global worst-case guarantees of proximal gradient methodswith the local adaptation and practical speed-up of Anderson acceleration.",https://arxiv.org/abs/1910.08590
['Title:Coupling Oceanic Observation Systems to Study Mesoscale Ocean Dynamics'],Machine Learning ,(Submitted on 18 Oct 2019),"Abstract:  Understanding local currents in the North Atlantic region of the ocean is akey part of modelling heat transfer and global climate patterns. Satellitesprovide a surface signature of the temperature of the ocean with a highhorizontal resolution while in situ autonomous probes supply high verticalresolution, but horizontally sparse, knowledge of the ocean interior thermalstructure. The objective of this paper is to develop a methodology to combinethese complementary ocean observing systems measurements to obtain athree-dimensional time series of ocean temperatures with high horizontal andvertical resolution. Within an observation-driven framework, we investigate theextent to which mesoscale ocean dynamics in the North Atlantic region may bedecomposed into a mixture of dynamical modes, characterized by different localregressions between Sea Surface Temperature (SST), Sea Level Anomalies (SLA)and Vertical Temperature fields. Ultimately we propose a Latent-classregression method to improve prediction of vertical ocean temperature.",https://arxiv.org/abs/1910.08573
['Title:Towards Learning Cross-Modal Perception-Trace Models'],Machine Learning ,(Submitted on 18 Oct 2019),"Abstract:  Representation learning is a key element of state-of-the-art deep learningapproaches. It enables to transform raw data into structured vector spaceembeddings. Such embeddings are able to capture the distributional semantics oftheir context, e.g. by word windows on natural language sentences, graph walkson knowledge graphs or convolutions on images. So far, this context is manuallydefined, resulting in heuristics which are solely optimized for computationalperformance on certain tasks like link-prediction. However, such heuristicmodels of context are fundamentally different to how humans captureinformation. For instance, when reading a multi-modal webpage (i) humans do notperceive all parts of a document equally: Some words and parts of images areskipped, others are revisited several times which makes the perception tracehighly non-sequential; (ii) humans construct meaning from a document's contentby shifting their attention between text and image, among other things, guidedby layout and design elements. In this paper we empirically investigate thedifference between human perception and context heuristics of basic embeddingmodels. We conduct eye tracking experiments to capture the underlyingcharacteristics of human perception of media documents containing a mixture oftext and images. Based on that, we devise a prototypical computationalperception-trace model, called CMPM. We evaluate empirically how CMPM canimprove a basic skip-gram embedding approach. Our results suggest, that evenwith a basic human-inspired computational perception model, there is a hugepotential for improving embeddings since such a model does inherently capturemultiple modalities, as well as layout and design elements.",https://arxiv.org/abs/1910.08549
['Title:Beating humans in a penny-matching game by leveraging cognitive hierarchy theory and Bayesian learning'],Machine Learning ,"(Submitted on 27 Sep 2019 (v1), last revised 22 Oct 2019 (this version, v2))","Abstract:  It is a long-standing goal of artificial intelligence (AI) to be superior tohuman beings in decision making. Games are suitable for testing AI capabilitiesof making good decisions in non-numerical tasks. In this paper, we develop anew AI algorithm to play the penny-matching game considered in Shannon's""mind-reading machine"" (1953) against human players. In particular, we exploitcognitive hierarchy theory and Bayesian learning techniques to continuallyevolve a model for predicting human player decisions, and let the AI playermake decisions according to the model predictions to pursue the best chance ofwinning. Experimental results show that our AI algorithm beats 27 out of 30volunteer human players.",https://arxiv.org/abs/1909.12701
"['Title:Backtracking gradient descent method for general $C^1$ functions, with applications to Deep Learning']",Machine Learning ,"(Submitted on 15 Aug 2018 (v1), last revised 4 Apr 2019 (this version, v2))","Abstract:  While Standard gradient descent is one very popular optimisation method, itsconvergence cannot be proven beyond the class of functions whose gradient isglobally Lipschitz continuous. As such, it is not actually applicable torealistic applications such as Deep Neural Networks. In this paper, we provethat its backtracking variant behaves very nicely, in particular convergencecan be shown for all Morse functions. The main theoretical result of this paperis as follows.Theorem. Let $f:\mathbb{R}^k\rightarrow \mathbb{R}$ be a $C^1$ function, and$\{z_n\}$ a sequence constructed from the Backtracking gradient descentalgorithm. (1) Either $\lim _{n\rightarrow\infty}||z_n||=\infty$ or $\lim_{n\rightarrow\infty}||z_{n+1}-z_n||=0$. (2) Assume that $f$ has at mostcountably many critical points. Then either $\lim_{n\rightarrow\infty}||z_n||=\infty$ or $\{z_n\}$ converges to a critical pointof $f$. (3) More generally, assume that all connected components of the set ofcritical points of $f$ are compact. Then either $\lim_{n\rightarrow\infty}||z_n||=\infty$ or $\{z_n\}$ is bounded. Moreover, in thelatter case the set of cluster points of $\{z_n\}$ is connected.Some generalised versions of this result, including an inexact version, areincluded. Another result in this paper concerns the problem of saddle points.We then present a heuristic argument to explain why Standard gradient descentmethod works so well, and modifications of the backtracking versions of GD, MMTand NAG. Experiments with datasets CIFAR10 and CIFAR100 on various populararchitectures verify the heuristic argument also for the mini-batch practiceand show that our new algorithms, while automatically fine tuning learningrates, perform better than current state-of-the-art methods such as MMT, NAG,Adagrad, Adadelta, RMSProp, Adam and Adamax.",https://arxiv.org/abs/1808.05160
